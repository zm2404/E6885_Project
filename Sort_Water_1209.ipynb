{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlI8iHgEP1wB"
      },
      "outputs": [],
      "source": [
        "!pip install tf_agents\n",
        "!pip install tf-agents[reverb]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tf_agents\n",
        "import reverb\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import epsilon_greedy_policy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/E6885_Project')\n",
        "import SortWaterEnv"
      ],
      "metadata": {
        "id": "CLWWFFWwP-Fk"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_iterations = 20000        #\n",
        "\n",
        "initial_collect_steps = 100     #\n",
        "collect_steps_per_iteration = 1   #\n",
        "replay_buffer_max_length = 100000  #\n",
        "\n",
        "batch_size = 64            #\n",
        "learning_rate = 1e-3        #\n",
        "log_interval = 200          #\n",
        "\n",
        "num_eval_episodes = 10        #\n",
        "eval_interval = 1000        #"
      ],
      "metadata": {
        "id": "meuuIXmuQLEW"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############# create training and evaluation environment #############\n",
        "num_bottles = 5\n",
        "water_level = 4\n",
        "env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level)\n",
        "train_py_env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level)\n",
        "eval_py_env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level)\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Be0fVlSQM1w",
        "outputId": "c3ef8326-2d02-485c-d010-0b03495cb67b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "8\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############ create a DQN agent ############\n",
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1"
      ],
      "metadata": {
        "id": "KSsS5sENQQa2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Customized Q netword\n",
        "class MaskedQNetwork(q_network.QNetwork):\n",
        "  def __init__(self, input_tensor_spec, action_spec, fc_layer_params=(100,), **kwargs):\n",
        "    # 从 input_tensor_spec 元组中提取观察值规格\n",
        "    observation_spec = input_tensor_spec[0]\n",
        "\n",
        "    # 调用基类的构造函数以构建网络\n",
        "    super(MaskedQNetwork, self).__init__(observation_spec, action_spec, fc_layer_params=fc_layer_params, **kwargs)\n",
        "\n",
        "  def call(self, observation, step_type=None, network_state=(), training=False):\n",
        "    # 直接调用父类的 call 方法，处理观察值\n",
        "    return super(MaskedQNetwork, self).call(\n",
        "        observation, step_type, network_state, training)"
      ],
      "metadata": {
        "id": "jddUDIr2QSTO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observation_spec = train_env.observation_spec()\n",
        "action_spec = train_env.action_spec()\n",
        "\n",
        "# create Q-network\n",
        "q_net = MaskedQNetwork(\n",
        "    (observation_spec['observation'], observation_spec['action_mask']),\n",
        "    action_spec,\n",
        "    fc_layer_params=fc_layer_params\n",
        ")"
      ],
      "metadata": {
        "id": "rqi6-5F7RG-n"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def observation_and_action_constraint_splitter(obs):\n",
        "\treturn obs['observation'], obs['action_mask']"
      ],
      "metadata": {
        "id": "jMoFiViiC7NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter,\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter\n",
        ")"
      ],
      "metadata": {
        "id": "qI_nu75uRRqe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# an example, just to show what policies are used during evaluation and collecting\n",
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ],
      "metadata": {
        "id": "1AhbI3ZfbFpm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# an example, just show how to create epsilon_greedy_policy. Not to be used\n",
        "base_policy = agent.policy\n",
        "epsilon = 0.1  # 例如，使用 0.1 作为 epsilon 值\n",
        "epsilon_greedy_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(base_policy, epsilon=epsilon)"
      ],
      "metadata": {
        "id": "sH_fIsadbICb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of greedy policy choosing action\n",
        "example_py_env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level)\n",
        "example_env = tf_py_environment.TFPyEnvironment(example_py_env)\n",
        "time_step = example_env.reset()\n",
        "print(time_step)\n",
        "epsilon_greedy_policy.action(time_step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9kV-ZLvbJVC",
        "outputId": "78762946-1db0-4952-b3e0-4dc1cbc91d14"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'action_mask': <tf.Tensor: shape=(1, 20), dtype=bool, numpy=\n",
            "array([[False, False, False, False,  True, False, False,  True,  True,\n",
            "         True, False,  True,  True, False, False, False,  True,  True,\n",
            "        False, False]])>,\n",
            "                 'observation': <tf.Tensor: shape=(1, 5, 4), dtype=int32, numpy=\n",
            "array([[[0, 0, 0, 0],\n",
            "        [2, 0, 0, 0],\n",
            "        [3, 2, 1, 2],\n",
            "        [3, 3, 1, 3],\n",
            "        [1, 1, 2, 0]]], dtype=int32)>},\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ],
      "metadata": {
        "id": "_0MKdny2bK-Q"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# an example\n",
        "compute_avg_return(eval_env, epsilon_greedy_policy, num_eval_episodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuqXjbWrbMQY",
        "outputId": "8a1147e7-51b8-4328-decc-77cea6a43a78"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################# Replay buffer #################\n",
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)"
      ],
      "metadata": {
        "id": "CQzc5_e4bP53"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.collect_data_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XbLZjCtb__-",
        "outputId": "46363912-8a89-4d7e-95e1-9c8470518800"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_TupleWrapper(Trajectory(\n",
              "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(19, dtype=int32)),\n",
              " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
              " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'observation': DictWrapper({'observation': BoundedTensorSpec(shape=(5, 4), dtype=tf.int32, name='observation', minimum=array(0, dtype=int32), maximum=array(3, dtype=int32)), 'action_mask': TensorSpec(shape=(20,), dtype=tf.bool, name='action_mask')}),\n",
              " 'policy_info': (),\n",
              " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
              " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.collect_data_spec._fields"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeCbnlphcesk",
        "outputId": "c1d463d1-8994-4f8c-d9ca-8ebedc870774"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('step_type',\n",
              " 'observation',\n",
              " 'action',\n",
              " 'policy_info',\n",
              " 'next_step_type',\n",
              " 'reward',\n",
              " 'discount')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# an example.\n",
        "py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      epsilon_greedy_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(train_py_env.reset())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3Hv74IhckJK",
        "outputId": "8a474cab-93df-4c68-dcc2-85f0d27ac66c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TimeStep(\n",
              " {'discount': array(1., dtype=float32),\n",
              "  'observation': {'action_mask': array([False, False,  True, False, False,  True,  True, False, False,\n",
              "         True,  True, False, False, False, False, False, False, False,\n",
              "        False,  True]),\n",
              "                  'observation': array([[1, 1, 1, 1],\n",
              "        [3, 0, 0, 0],\n",
              "        [3, 3, 3, 0],\n",
              "        [0, 0, 0, 0],\n",
              "        [2, 2, 2, 2]], dtype=int32)},\n",
              "  'reward': array(0., dtype=float32),\n",
              "  'step_type': array(1, dtype=int32)}),\n",
              " ())"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh7SqXgkdYNu",
        "outputId": "95ebf91f-37b7-4d80-f5c1-23fb729c90bb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(Trajectory(\n",
              "{'action': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
              " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'observation': DictWrapper({'observation': TensorSpec(shape=(64, 2, 5, 4), dtype=tf.int32, name=None), 'action_mask': TensorSpec(shape=(64, 2, 20), dtype=tf.bool, name=None)}),\n",
              " 'policy_info': (),\n",
              " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
              " 'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)}), SampleInfo(key=TensorSpec(shape=(64, 2), dtype=tf.uint64, name=None), probability=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), table_size=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), priority=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), times_sampled=TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)))>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(dataset)"
      ],
      "metadata": {
        "id": "Mp0I59Vwd11M"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# demo: only 1 episode\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, 1)"
      ],
      "metadata": {
        "id": "pqBPd98nu52O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training process\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = train_py_env.reset()\n",
        "\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ],
      "metadata": {
        "id": "yn5SoNGjd6ij"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
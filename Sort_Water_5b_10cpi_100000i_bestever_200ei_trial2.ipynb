{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BlI8iHgEP1wB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0908b326-00dc-43ba-e923-6986a58eb8cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf_agents\n",
            "  Downloading tf_agents-0.19.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf_agents)\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf_agents) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (4.5.0)\n",
            "Collecting pygame==2.1.3 (from tf_agents)\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-probability~=0.23.0 (from tf_agents)\n",
            "  Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents) (0.0.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (0.5.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (0.1.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697628 sha256=0a4c230cb3d17cf5b8b5a63b7e5add04f47cec93bb7a0f38512b7df95e751000\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
            "Successfully built gym\n",
            "Installing collected packages: tensorflow-probability, pygame, gym, tf_agents\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.22.0\n",
            "    Uninstalling tensorflow-probability-0.22.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.22.0\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.23.0 pygame-2.1.3 tensorflow-probability-0.23.0 tf_agents-0.19.0\n",
            "Requirement already satisfied: tf-agents[reverb] in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.23.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
            "Collecting rlds (from tf-agents[reverb])\n",
            "  Downloading rlds-0.1.8-py3-none-manylinux2010_x86_64.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m839.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dm-reverb~=0.14.0 (from tf-agents[reverb])\n",
            "  Downloading dm_reverb-0.14.0-cp310-cp310-manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow~=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.15.0->tf-agents[reverb]) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb~=0.14.0->tf-agents[reverb]) (5.9.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.2.2)\n",
            "Installing collected packages: rlds, dm-reverb\n",
            "Successfully installed dm-reverb-0.14.0 rlds-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install tf_agents\n",
        "!pip install tf-agents[reverb]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CLWWFFWwP-Fk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tf_agents\n",
        "import reverb\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import epsilon_greedy_policy\n",
        "from tf_agents.policies import policy_saver\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/E6885_Project')\n",
        "import SortWaterEnv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.version.VERSION"
      ],
      "metadata": {
        "id": "S7-5FyITqOTK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "52c6a757-7219-4848-c3b4-fd229f1f7920"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "meuuIXmuQLEW"
      },
      "outputs": [],
      "source": [
        "num_iterations = 100000        #\n",
        "\n",
        "initial_collect_steps = 100     #\n",
        "collect_steps_per_iteration = 10#2#1   #\n",
        "replay_buffer_max_length = 100000  #\n",
        "\n",
        "batch_size = 64            #\n",
        "learning_rate = 1e-3        #\n",
        "log_interval = 200          #\n",
        "\n",
        "num_eval_episodes = 100        #\n",
        "eval_interval = 200#500#1000        #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_Be0fVlSQM1w"
      },
      "outputs": [],
      "source": [
        "############# create training and evaluation environment #############\n",
        "num_bottles = 5\n",
        "water_level = 4\n",
        "env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level, random_init=True)\n",
        "train_py_env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level, random_init=True)\n",
        "eval_py_env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level, random_init=True)\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KSsS5sENQQa2"
      },
      "outputs": [],
      "source": [
        "############ create a DQN agent ############\n",
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jddUDIr2QSTO"
      },
      "outputs": [],
      "source": [
        "# Customized Q netword\n",
        "class MaskedQNetwork(q_network.QNetwork):\n",
        "  def __init__(self, input_tensor_spec, action_spec, fc_layer_params=(100,), **kwargs):\n",
        "    # 从 input_tensor_spec 元组中提取观察值规格\n",
        "    observation_spec = input_tensor_spec[0]\n",
        "\n",
        "    # 调用基类的构造函数以构建网络\n",
        "    super(MaskedQNetwork, self).__init__(observation_spec, action_spec, fc_layer_params=fc_layer_params, **kwargs)\n",
        "\n",
        "  def call(self, observation, step_type=None, network_state=(), training=False):\n",
        "    # 直接调用父类的 call 方法，处理观察值\n",
        "    return super(MaskedQNetwork, self).call(\n",
        "        observation, step_type, network_state, training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rqi6-5F7RG-n"
      },
      "outputs": [],
      "source": [
        "observation_spec = train_env.observation_spec()\n",
        "action_spec = train_env.action_spec()\n",
        "\n",
        "# create Q-network\n",
        "q_net = MaskedQNetwork(\n",
        "    (observation_spec['observation'], observation_spec['action_mask']),\n",
        "    action_spec,\n",
        "    fc_layer_params=fc_layer_params\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jMoFiViiC7NV"
      },
      "outputs": [],
      "source": [
        "def observation_and_action_constraint_splitter(obs):\n",
        "\treturn obs['observation'], obs['action_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qI_nu75uRRqe"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter,\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1AhbI3ZfbFpm"
      },
      "outputs": [],
      "source": [
        "# an example, just to show what policies are used during evaluation and collecting\n",
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sH_fIsadbICb"
      },
      "outputs": [],
      "source": [
        "# an example, just show how to create epsilon_greedy_policy. Not to be used\n",
        "base_policy = agent.policy\n",
        "epsilon = 0.1  # 例如，使用 0.1 作为 epsilon 值\n",
        "epsilon_greedy_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(base_policy, epsilon=epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "z9kV-ZLvbJVC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "552c2a47-020a-403a-e707-b12e7163366f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TimeStep(\n",
            "{'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observation': <tf.Tensor: shape=(1, 5, 4), dtype=int32, numpy=\n",
            "array([[[1, 1, 1, 2],\n",
            "        [0, 0, 0, 0],\n",
            "        [0, 0, 0, 0],\n",
            "        [2, 3, 3, 2],\n",
            "        [3, 1, 2, 3]]], dtype=int32)>,\n",
            "                 'action_mask': <tf.Tensor: shape=(1, 20), dtype=bool, numpy=\n",
            "array([[ True,  True, False, False, False, False, False, False, False,\n",
            "        False, False, False, False,  True,  True, False, False,  True,\n",
            "         True, False]])>}})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=())"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# example of greedy policy choosing action\n",
        "example_py_env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level, random_init=True)\n",
        "example_env = tf_py_environment.TFPyEnvironment(example_py_env)\n",
        "time_step = example_env.reset()\n",
        "print(time_step)\n",
        "epsilon_greedy_policy.action(time_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_0MKdny2bK-Q"
      },
      "outputs": [],
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZuqXjbWrbMQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c52022-50ac-4002-d3b0-70c6e0c75582"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.6"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# an example\n",
        "compute_avg_return(eval_env, epsilon_greedy_policy, num_eval_episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CQzc5_e4bP53"
      },
      "outputs": [],
      "source": [
        "################# Replay buffer #################\n",
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0XbLZjCtb__-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b26e096a-4805-464a-ccc8-4fad5cd690b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_TupleWrapper(Trajectory(\n",
              "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'observation': DictWrapper({'observation': BoundedTensorSpec(shape=(5, 4), dtype=tf.int32, name='observation', minimum=array(0, dtype=int32), maximum=array(3, dtype=int32)), 'action_mask': TensorSpec(shape=(20,), dtype=tf.bool, name='action_mask')}),\n",
              " 'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(19, dtype=int32)),\n",
              " 'policy_info': (),\n",
              " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
              " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))}))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "agent.collect_data_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TeCbnlphcesk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28470f70-9c6e-4155-bded-cb338d33bf97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('step_type',\n",
              " 'observation',\n",
              " 'action',\n",
              " 'policy_info',\n",
              " 'next_step_type',\n",
              " 'reward',\n",
              " 'discount')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "agent.collect_data_spec._fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "X3Hv74IhckJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf7b061-3d3c-4cbb-a853-e41a6b30ba5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TimeStep(\n",
              " {'step_type': array(1, dtype=int32),\n",
              "  'reward': array(0., dtype=float32),\n",
              "  'discount': array(1., dtype=float32),\n",
              "  'observation': {'observation': array([[1, 1, 1, 0],\n",
              "        [2, 2, 2, 2],\n",
              "        [3, 3, 0, 0],\n",
              "        [3, 3, 0, 0],\n",
              "        [1, 0, 0, 0]], dtype=int32),\n",
              "                  'action_mask': array([False, False, False,  True, False, False, False, False, False,\n",
              "        False,  True, False, False, False,  True, False,  True, False,\n",
              "        False, False])}}),\n",
              " ())"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# an example.\n",
        "py_driver.PyDriver(\n",
        "    train_py_env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      epsilon_greedy_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(train_py_env.reset())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.get_state()"
      ],
      "metadata": {
        "id": "ohhyBZROzNh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e28d92-9563-4dd4-e382-1ac0972f5647"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'observation': array([[1, 1, 1, 3],\n",
              "        [2, 2, 3, 0],\n",
              "        [0, 0, 0, 0],\n",
              "        [1, 2, 2, 3],\n",
              "        [3, 0, 0, 0]], dtype=int32),\n",
              " 'action_mask': array([ True,  True, False,  True, False,  True, False,  True, False,\n",
              "        False, False, False, False,  True,  True,  True, False,  True,\n",
              "         True, False])}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nh7SqXgkdYNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "526a219e-907e-4c2f-ae39-3928fac83841"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(Trajectory(\n",
              "{'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'observation': DictWrapper({'observation': TensorSpec(shape=(64, 2, 5, 4), dtype=tf.int32, name=None), 'action_mask': TensorSpec(shape=(64, 2, 20), dtype=tf.bool, name=None)}),\n",
              " 'action': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'policy_info': (),\n",
              " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
              " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None)}), SampleInfo(key=TensorSpec(shape=(64, 2), dtype=tf.uint64, name=None), probability=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), table_size=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), priority=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), times_sampled=TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)))>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Mp0I59Vwd11M"
      },
      "outputs": [],
      "source": [
        "iterator = iter(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pqBPd98nu52O"
      },
      "outputs": [],
      "source": [
        "# demo: only 1 episode\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = train_py_env.reset()\n",
        "\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    train_py_env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "######### early stopping ##########\n",
        "# set save direction\n",
        "tempdir = os.getenv(\"TEST_TMPDIR\", tempfile.gettempdir())\n",
        "# checkpointer\n",
        "# checkpoint_dir = os.path.join(tempdir, 'checkpoint')\n",
        "# train_checkpointer = common.Checkpointer(\n",
        "#     ckpt_dir=checkpoint_dir,\n",
        "#     max_to_keep=1,\n",
        "#     agent=agent,\n",
        "#     policy=agent.policy,\n",
        "#     replay_buffer=replay_buffer,\n",
        "#     global_step=train_step_counter\n",
        "# )\n",
        "# policy saver\n",
        "policy_dir = os.path.join(tempdir, 'policy')\n",
        "tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
        "\n",
        "# some threshold\n",
        "#best_avg_return = -float('inf')\n",
        "best_avg_return = 0.7\n",
        "no_improvement_steps = 0\n",
        "early_stopping_threshold = 10\n",
        "earlystop = False\n",
        "iter_thres = 10000\n",
        "######### early stopping ##########\n",
        "\n",
        "\n",
        "iter_count = 0\n",
        "for _ in range(num_iterations):\n",
        "  iter_count += 1\n",
        "  #print('iter_count: ',iter_count)\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)\n",
        "\n",
        "  ######### early stopping ##########\n",
        "    if avg_return > best_avg_return:\n",
        "      earlystop = True\n",
        "    if (avg_return > best_avg_return) and earlystop and (iter_count > iter_thres):\n",
        "      best_avg_return = avg_return\n",
        "      no_improvement_steps = 0\n",
        "      tf_policy_saver.save(policy_dir)  # save policy\n",
        "    elif earlystop and (iter_count > iter_thres):\n",
        "      no_improvement_steps += 0\n",
        "    print(\"no_improvement_steps: \",no_improvement_steps)\n",
        "  if no_improvement_steps >= early_stopping_threshold:\n",
        "    print(\"No improvement for {0} steps. Early stopping...\".format(no_improvement_steps))\n",
        "    break\n",
        "  ######### early stopping ##########"
      ],
      "metadata": {
        "id": "35k0m3HtqQyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3028b4ce-1529-4681-df0d-4a6a8e20c3db"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 200: loss = 0.14442898333072662\n",
            "step = 200: Average Return = -0.25999999046325684\n",
            "no_improvement_steps:  0\n",
            "step = 400: loss = 0.07895389199256897\n",
            "step = 400: Average Return = 0.36000001430511475\n",
            "no_improvement_steps:  0\n",
            "step = 600: loss = 0.12948694825172424\n",
            "step = 600: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 800: loss = 0.12459661811590195\n",
            "step = 800: Average Return = 0.36000001430511475\n",
            "no_improvement_steps:  0\n",
            "step = 1000: loss = 0.1078200489282608\n",
            "step = 1000: Average Return = 0.30000001192092896\n",
            "no_improvement_steps:  0\n",
            "step = 1200: loss = 0.07622701674699783\n",
            "step = 1200: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 1400: loss = 0.1394142210483551\n",
            "step = 1400: Average Return = 0.41999998688697815\n",
            "no_improvement_steps:  0\n",
            "step = 1600: loss = 0.1104680672287941\n",
            "step = 1600: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 1800: loss = 0.08731383085250854\n",
            "step = 1800: Average Return = 0.4000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 2000: loss = 0.127986878156662\n",
            "step = 2000: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 2200: loss = 0.11141438782215118\n",
            "step = 2200: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 2400: loss = 0.07585395872592926\n",
            "step = 2400: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 2600: loss = 0.1654958426952362\n",
            "step = 2600: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 2800: loss = 0.11219894886016846\n",
            "step = 2800: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 3000: loss = 0.05132610350847244\n",
            "step = 3000: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 3200: loss = 0.14246009290218353\n",
            "step = 3200: Average Return = 0.41999998688697815\n",
            "no_improvement_steps:  0\n",
            "step = 3400: loss = 0.1727680265903473\n",
            "step = 3400: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 3600: loss = 0.18480682373046875\n",
            "step = 3600: Average Return = 0.25999999046325684\n",
            "no_improvement_steps:  0\n",
            "step = 3800: loss = 0.18687333166599274\n",
            "step = 3800: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 4000: loss = 0.1996178925037384\n",
            "step = 4000: Average Return = 0.4399999976158142\n",
            "no_improvement_steps:  0\n",
            "step = 4200: loss = 0.09825456142425537\n",
            "step = 4200: Average Return = 0.4399999976158142\n",
            "no_improvement_steps:  0\n",
            "step = 4400: loss = 0.09040345996618271\n",
            "step = 4400: Average Return = 0.41999998688697815\n",
            "no_improvement_steps:  0\n",
            "step = 4600: loss = 0.20882219076156616\n",
            "step = 4600: Average Return = 0.3400000035762787\n",
            "no_improvement_steps:  0\n",
            "step = 4800: loss = 0.15838786959648132\n",
            "step = 4800: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 5000: loss = 0.03218415752053261\n",
            "step = 5000: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 5200: loss = 0.1653941422700882\n",
            "step = 5200: Average Return = 0.41999998688697815\n",
            "no_improvement_steps:  0\n",
            "step = 5400: loss = 0.20086389780044556\n",
            "step = 5400: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 5600: loss = 0.13385851681232452\n",
            "step = 5600: Average Return = 0.3799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 5800: loss = 0.16560274362564087\n",
            "step = 5800: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 6000: loss = 0.10813970863819122\n",
            "step = 6000: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 6200: loss = 0.14849388599395752\n",
            "step = 6200: Average Return = 0.4399999976158142\n",
            "no_improvement_steps:  0\n",
            "step = 6400: loss = 0.11169908940792084\n",
            "step = 6400: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 6600: loss = 0.059078555554151535\n",
            "step = 6600: Average Return = 0.4000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 6800: loss = 0.13815993070602417\n",
            "step = 6800: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 7000: loss = 0.12521134316921234\n",
            "step = 7000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 7200: loss = 0.13978369534015656\n",
            "step = 7200: Average Return = 0.41999998688697815\n",
            "no_improvement_steps:  0\n",
            "step = 7400: loss = 0.16841423511505127\n",
            "step = 7400: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 7600: loss = 0.20125924050807953\n",
            "step = 7600: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 7800: loss = 0.12585517764091492\n",
            "step = 7800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 8000: loss = 0.09586402773857117\n",
            "step = 8000: Average Return = 0.4399999976158142\n",
            "no_improvement_steps:  0\n",
            "step = 8200: loss = 0.048838093876838684\n",
            "step = 8200: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 8400: loss = 0.06710976362228394\n",
            "step = 8400: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 8600: loss = 0.13594302535057068\n",
            "step = 8600: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 8800: loss = 0.19649681448936462\n",
            "step = 8800: Average Return = 0.23999999463558197\n",
            "no_improvement_steps:  0\n",
            "step = 9000: loss = 0.2249712347984314\n",
            "step = 9000: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 9200: loss = 0.13282141089439392\n",
            "step = 9200: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 9400: loss = 0.15281330049037933\n",
            "step = 9400: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 9600: loss = 0.12376011908054352\n",
            "step = 9600: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 9800: loss = 0.16178534924983978\n",
            "step = 9800: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 10000: loss = 0.12611818313598633\n",
            "step = 10000: Average Return = 0.2199999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 10200: loss = 0.18148130178451538\n",
            "step = 10200: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 10400: loss = 0.17823344469070435\n",
            "step = 10400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 10600: loss = 0.08679597079753876\n",
            "step = 10600: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 10800: loss = 0.15227827429771423\n",
            "step = 10800: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 11000: loss = 0.11553287506103516\n",
            "step = 11000: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 11200: loss = 0.2388439029455185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
            "WARNING:absl:`0/reward` is not a valid tf.function parameter name. Sanitizing to `arg_0_reward`.\n",
            "WARNING:absl:`0/discount` is not a valid tf.function parameter name. Sanitizing to `arg_0_discount`.\n",
            "WARNING:absl:`0/observation/action_mask` is not a valid tf.function parameter name. Sanitizing to `arg_0_observation_action_mask`.\n",
            "WARNING:absl:`0/observation/observation` is not a valid tf.function parameter name. Sanitizing to `arg_0_observation_observation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 11200: Average Return = 0.7200000286102295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no_improvement_steps:  0\n",
            "step = 11400: loss = 0.1717182993888855\n",
            "step = 11400: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 11600: loss = 0.125770702958107\n",
            "step = 11600: Average Return = 0.7599999904632568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no_improvement_steps:  0\n",
            "step = 11800: loss = 0.1737036556005478\n",
            "step = 11800: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 12000: loss = 0.059013351798057556\n",
            "step = 12000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 12200: loss = 0.31504562497138977\n",
            "step = 12200: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 12400: loss = 0.09267086535692215\n",
            "step = 12400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 12600: loss = 0.14817865192890167\n",
            "step = 12600: Average Return = 0.7799999713897705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no_improvement_steps:  0\n",
            "step = 12800: loss = 0.20211687684059143\n",
            "step = 12800: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 13000: loss = 0.09607937932014465\n",
            "step = 13000: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 13200: loss = 0.0792350172996521\n",
            "step = 13200: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 13400: loss = 0.07989799231290817\n",
            "step = 13400: Average Return = 0.8199999928474426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no_improvement_steps:  0\n",
            "step = 13600: loss = 0.09400962293148041\n",
            "step = 13600: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 13800: loss = 0.12302570044994354\n",
            "step = 13800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 14000: loss = 0.13044220209121704\n",
            "step = 14000: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 14200: loss = 0.09052856266498566\n",
            "step = 14200: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 14400: loss = 0.21944081783294678\n",
            "step = 14400: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 14600: loss = 0.15245145559310913\n",
            "step = 14600: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 14800: loss = 0.1268279254436493\n",
            "step = 14800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 15000: loss = 0.029036398977041245\n",
            "step = 15000: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 15200: loss = 0.2614029347896576\n",
            "step = 15200: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 15400: loss = 0.15019811689853668\n",
            "step = 15400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 15600: loss = 0.19065579771995544\n",
            "step = 15600: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 15800: loss = 0.11818827688694\n",
            "step = 15800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 16000: loss = 0.09566331654787064\n",
            "step = 16000: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 16200: loss = 0.12249112874269485\n",
            "step = 16200: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 16400: loss = 0.1914880871772766\n",
            "step = 16400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 16600: loss = 0.10711339861154556\n",
            "step = 16600: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 16800: loss = 0.138575941324234\n",
            "step = 16800: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 17000: loss = 0.16583895683288574\n",
            "step = 17000: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 17200: loss = 0.1103578507900238\n",
            "step = 17200: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 17400: loss = 0.12520338594913483\n",
            "step = 17400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 17600: loss = 0.0958138257265091\n",
            "step = 17600: Average Return = 0.25999999046325684\n",
            "no_improvement_steps:  0\n",
            "step = 17800: loss = 0.22466924786567688\n",
            "step = 17800: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 18000: loss = 0.25957781076431274\n",
            "step = 18000: Average Return = 0.4000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 18200: loss = 0.08916875720024109\n",
            "step = 18200: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 18400: loss = 0.2226717174053192\n",
            "step = 18400: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 18600: loss = 0.15710321068763733\n",
            "step = 18600: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 18800: loss = 0.11045287549495697\n",
            "step = 18800: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 19000: loss = 0.23648732900619507\n",
            "step = 19000: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 19200: loss = 0.20133501291275024\n",
            "step = 19200: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 19400: loss = 0.21463647484779358\n",
            "step = 19400: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 19600: loss = 0.11767607927322388\n",
            "step = 19600: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 19800: loss = 0.07419212907552719\n",
            "step = 19800: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 20000: loss = 0.16432520747184753\n",
            "step = 20000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 20200: loss = 0.25462818145751953\n",
            "step = 20200: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 20400: loss = 0.09547913074493408\n",
            "step = 20400: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 20600: loss = 0.141248419880867\n",
            "step = 20600: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 20800: loss = 0.14413373172283173\n",
            "step = 20800: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 21000: loss = 0.052719805389642715\n",
            "step = 21000: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 21200: loss = 0.16884338855743408\n",
            "step = 21200: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 21400: loss = 0.06730365008115768\n",
            "step = 21400: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 21600: loss = 0.12062179297208786\n",
            "step = 21600: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 21800: loss = 0.11122819781303406\n",
            "step = 21800: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 22000: loss = 0.11809879541397095\n",
            "step = 22000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 22200: loss = 0.09908194839954376\n",
            "step = 22200: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 22400: loss = 0.17270702123641968\n",
            "step = 22400: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 22600: loss = 0.15707743167877197\n",
            "step = 22600: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 22800: loss = 0.08821707963943481\n",
            "step = 22800: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 23000: loss = 0.22557765245437622\n",
            "step = 23000: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 23200: loss = 0.30237269401550293\n",
            "step = 23200: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 23400: loss = 0.11860913783311844\n",
            "step = 23400: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 23600: loss = 0.08592237532138824\n",
            "step = 23600: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 23800: loss = 0.04530918598175049\n",
            "step = 23800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 24000: loss = 0.10031279921531677\n",
            "step = 24000: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 24200: loss = 0.08903589844703674\n",
            "step = 24200: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 24400: loss = 0.04358907788991928\n",
            "step = 24400: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 24600: loss = 0.181057870388031\n",
            "step = 24600: Average Return = 0.3199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 24800: loss = 0.17798718810081482\n",
            "step = 24800: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 25000: loss = 0.05286320671439171\n",
            "step = 25000: Average Return = 0.8399999737739563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no_improvement_steps:  0\n",
            "step = 25200: loss = 0.13950055837631226\n",
            "step = 25200: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 25400: loss = 0.06249859184026718\n",
            "step = 25400: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 25600: loss = 0.04348534345626831\n",
            "step = 25600: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 25800: loss = 0.14602762460708618\n",
            "step = 25800: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 26000: loss = 0.11211922019720078\n",
            "step = 26000: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 26200: loss = 0.11006537079811096\n",
            "step = 26200: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 26400: loss = 0.08951111137866974\n",
            "step = 26400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 26600: loss = 0.07107362151145935\n",
            "step = 26600: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 26800: loss = 0.18063980340957642\n",
            "step = 26800: Average Return = 0.8999999761581421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no_improvement_steps:  0\n",
            "step = 27000: loss = 0.09525664150714874\n",
            "step = 27000: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 27200: loss = 0.17969569563865662\n",
            "step = 27200: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 27400: loss = 0.12656059861183167\n",
            "step = 27400: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 27600: loss = 0.020579464733600616\n",
            "step = 27600: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 27800: loss = 0.03856930881738663\n",
            "step = 27800: Average Return = 0.23999999463558197\n",
            "no_improvement_steps:  0\n",
            "step = 28000: loss = 0.10953100025653839\n",
            "step = 28000: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 28200: loss = 0.10649438947439194\n",
            "step = 28200: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 28400: loss = 0.1007978618144989\n",
            "step = 28400: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 28600: loss = 0.14433401823043823\n",
            "step = 28600: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 28800: loss = 0.12451722472906113\n",
            "step = 28800: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 29000: loss = 0.10968641191720963\n",
            "step = 29000: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 29200: loss = 0.1466379463672638\n",
            "step = 29200: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 29400: loss = 0.08081991970539093\n",
            "step = 29400: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 29600: loss = 0.10811410844326019\n",
            "step = 29600: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 29800: loss = 0.23077848553657532\n",
            "step = 29800: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 30000: loss = 0.055747535079717636\n",
            "step = 30000: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 30200: loss = 0.077887162566185\n",
            "step = 30200: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 30400: loss = 0.15858063101768494\n",
            "step = 30400: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 30600: loss = 0.10740257054567337\n",
            "step = 30600: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 30800: loss = 0.11498284339904785\n",
            "step = 30800: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 31000: loss = 0.07459817081689835\n",
            "step = 31000: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 31200: loss = 0.07827358692884445\n",
            "step = 31200: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 31400: loss = 0.044878698885440826\n",
            "step = 31400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 31600: loss = 0.1189585030078888\n",
            "step = 31600: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 31800: loss = 0.15023183822631836\n",
            "step = 31800: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 32000: loss = 0.09330755472183228\n",
            "step = 32000: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 32200: loss = 0.13624513149261475\n",
            "step = 32200: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 32400: loss = 0.1827884465456009\n",
            "step = 32400: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 32600: loss = 0.018298711627721786\n",
            "step = 32600: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 32800: loss = 0.1940329372882843\n",
            "step = 32800: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 33000: loss = 0.059599392116069794\n",
            "step = 33000: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 33200: loss = 0.11281381547451019\n",
            "step = 33200: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 33400: loss = 0.1416376531124115\n",
            "step = 33400: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 33600: loss = 0.03576212748885155\n",
            "step = 33600: Average Return = 0.9399999976158142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no_improvement_steps:  0\n",
            "step = 33800: loss = 0.1391964554786682\n",
            "step = 33800: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 34000: loss = 0.18854743242263794\n",
            "step = 34000: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 34200: loss = 0.1462891697883606\n",
            "step = 34200: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 34400: loss = 0.11488087475299835\n",
            "step = 34400: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 34600: loss = 0.14634279906749725\n",
            "step = 34600: Average Return = 0.8799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 34800: loss = 0.15714451670646667\n",
            "step = 34800: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 35000: loss = 0.049211882054805756\n",
            "step = 35000: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 35200: loss = 0.06222503259778023\n",
            "step = 35200: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 35400: loss = 0.07362662255764008\n",
            "step = 35400: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 35600: loss = 0.06945271044969559\n",
            "step = 35600: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 35800: loss = 0.34780365228652954\n",
            "step = 35800: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 36000: loss = 0.015376507304608822\n",
            "step = 36000: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 36200: loss = 0.15261992812156677\n",
            "step = 36200: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 36400: loss = 0.1288387030363083\n",
            "step = 36400: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 36600: loss = 0.07850761711597443\n",
            "step = 36600: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 36800: loss = 0.13412167131900787\n",
            "step = 36800: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 37000: loss = 0.15183338522911072\n",
            "step = 37000: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 37200: loss = 0.13909827172756195\n",
            "step = 37200: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 37400: loss = 0.17761096358299255\n",
            "step = 37400: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 37600: loss = 0.06545189768075943\n",
            "step = 37600: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 37800: loss = 0.05590467154979706\n",
            "step = 37800: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 38000: loss = 0.19833913445472717\n",
            "step = 38000: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 38200: loss = 0.2807418704032898\n",
            "step = 38200: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 38400: loss = 0.15872928500175476\n",
            "step = 38400: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 38600: loss = 0.031226862221956253\n",
            "step = 38600: Average Return = 0.8799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 38800: loss = 0.15081089735031128\n",
            "step = 38800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 39000: loss = 0.11858272552490234\n",
            "step = 39000: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 39200: loss = 0.13149471580982208\n",
            "step = 39200: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 39400: loss = 0.06643752753734589\n",
            "step = 39400: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 39600: loss = 0.12691649794578552\n",
            "step = 39600: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 39800: loss = 0.14909125864505768\n",
            "step = 39800: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 40000: loss = 0.06378095597028732\n",
            "step = 40000: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 40200: loss = 0.06880372762680054\n",
            "step = 40200: Average Return = 0.9200000166893005\n",
            "no_improvement_steps:  0\n",
            "step = 40400: loss = 0.2700166702270508\n",
            "step = 40400: Average Return = 0.8999999761581421\n",
            "no_improvement_steps:  0\n",
            "step = 40600: loss = 0.09592650830745697\n",
            "step = 40600: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 40800: loss = 0.01482392381876707\n",
            "step = 40800: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 41000: loss = 0.10200750827789307\n",
            "step = 41000: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 41200: loss = 0.12720757722854614\n",
            "step = 41200: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 41400: loss = 0.10069087892770767\n",
            "step = 41400: Average Return = 0.8799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 41600: loss = 0.09704762697219849\n",
            "step = 41600: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 41800: loss = 0.1455630213022232\n",
            "step = 41800: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 42000: loss = 0.09592296183109283\n",
            "step = 42000: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 42200: loss = 0.08125127851963043\n",
            "step = 42200: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 42400: loss = 0.07633550465106964\n",
            "step = 42400: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 42600: loss = 0.16868507862091064\n",
            "step = 42600: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 42800: loss = 0.06961045414209366\n",
            "step = 42800: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 43000: loss = 0.13335153460502625\n",
            "step = 43000: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 43200: loss = 0.09252113848924637\n",
            "step = 43200: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 43400: loss = 0.12955884635448456\n",
            "step = 43400: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 43600: loss = 0.07167838513851166\n",
            "step = 43600: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 43800: loss = 0.08455240726470947\n",
            "step = 43800: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 44000: loss = 0.10711874067783356\n",
            "step = 44000: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 44200: loss = 0.22954727709293365\n",
            "step = 44200: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 44400: loss = 0.09437062591314316\n",
            "step = 44400: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 44600: loss = 0.293393611907959\n",
            "step = 44600: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 44800: loss = 0.09073136746883392\n",
            "step = 44800: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 45000: loss = 0.012627147138118744\n",
            "step = 45000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 45200: loss = 0.11533376574516296\n",
            "step = 45200: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 45400: loss = 0.05983380228281021\n",
            "step = 45400: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 45600: loss = 0.08177986741065979\n",
            "step = 45600: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 45800: loss = 0.10068891942501068\n",
            "step = 45800: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 46000: loss = 0.12304224073886871\n",
            "step = 46000: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 46200: loss = 0.07022634893655777\n",
            "step = 46200: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 46400: loss = 0.1940259337425232\n",
            "step = 46400: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 46600: loss = 0.16440290212631226\n",
            "step = 46600: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 46800: loss = 0.08097919821739197\n",
            "step = 46800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 47000: loss = 0.19131451845169067\n",
            "step = 47000: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 47200: loss = 0.1400788426399231\n",
            "step = 47200: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 47400: loss = 0.09872906655073166\n",
            "step = 47400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 47600: loss = 0.11420844495296478\n",
            "step = 47600: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 47800: loss = 0.10892117768526077\n",
            "step = 47800: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 48000: loss = 0.12534373998641968\n",
            "step = 48000: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 48200: loss = 0.21573130786418915\n",
            "step = 48200: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 48400: loss = 0.03633521497249603\n",
            "step = 48400: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 48600: loss = 0.11826575547456741\n",
            "step = 48600: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 48800: loss = 0.09315751492977142\n",
            "step = 48800: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 49000: loss = 0.06793483346700668\n",
            "step = 49000: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 49200: loss = 0.06260676681995392\n",
            "step = 49200: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 49400: loss = 0.028713706880807877\n",
            "step = 49400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 49600: loss = 0.09327754378318787\n",
            "step = 49600: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 49800: loss = 0.19325146079063416\n",
            "step = 49800: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 50000: loss = 0.03965746611356735\n",
            "step = 50000: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 50200: loss = 0.05613419786095619\n",
            "step = 50200: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 50400: loss = 0.04057135060429573\n",
            "step = 50400: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 50600: loss = 0.17409612238407135\n",
            "step = 50600: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 50800: loss = 0.1277388036251068\n",
            "step = 50800: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 51000: loss = 0.15418903529644012\n",
            "step = 51000: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 51200: loss = 0.08927551656961441\n",
            "step = 51200: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 51400: loss = 0.07501240074634552\n",
            "step = 51400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 51600: loss = 0.09993124008178711\n",
            "step = 51600: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 51800: loss = 0.14811331033706665\n",
            "step = 51800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 52000: loss = 0.04755528271198273\n",
            "step = 52000: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 52200: loss = 0.02430640161037445\n",
            "step = 52200: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 52400: loss = 0.09571758657693863\n",
            "step = 52400: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 52600: loss = 0.12965920567512512\n",
            "step = 52600: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 52800: loss = 0.07501260936260223\n",
            "step = 52800: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 53000: loss = 0.07291609048843384\n",
            "step = 53000: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 53200: loss = 0.1570652723312378\n",
            "step = 53200: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 53400: loss = 0.1187402606010437\n",
            "step = 53400: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 53600: loss = 0.10827164351940155\n",
            "step = 53600: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 53800: loss = 0.13346701860427856\n",
            "step = 53800: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 54000: loss = 0.022406455129384995\n",
            "step = 54000: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 54200: loss = 0.08623909950256348\n",
            "step = 54200: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 54400: loss = 0.12878409028053284\n",
            "step = 54400: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 54600: loss = 0.07385118305683136\n",
            "step = 54600: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 54800: loss = 0.1356334090232849\n",
            "step = 54800: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 55000: loss = 0.10616704821586609\n",
            "step = 55000: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 55200: loss = 0.09515421837568283\n",
            "step = 55200: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 55400: loss = 0.04130972549319267\n",
            "step = 55400: Average Return = 0.41999998688697815\n",
            "no_improvement_steps:  0\n",
            "step = 55600: loss = 0.18295207619667053\n",
            "step = 55600: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 55800: loss = 0.08279790729284286\n",
            "step = 55800: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 56000: loss = 0.10933001339435577\n",
            "step = 56000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 56200: loss = 0.10199138522148132\n",
            "step = 56200: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 56400: loss = 0.16906379163265228\n",
            "step = 56400: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 56600: loss = 0.07477427273988724\n",
            "step = 56600: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 56800: loss = 0.16361841559410095\n",
            "step = 56800: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 57000: loss = 0.14810772240161896\n",
            "step = 57000: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 57200: loss = 0.05331600084900856\n",
            "step = 57200: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 57400: loss = 0.034945257008075714\n",
            "step = 57400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 57600: loss = 0.19078731536865234\n",
            "step = 57600: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 57800: loss = 0.05909227579832077\n",
            "step = 57800: Average Return = 0.4399999976158142\n",
            "no_improvement_steps:  0\n",
            "step = 58000: loss = 0.11863453686237335\n",
            "step = 58000: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 58200: loss = 0.1580556184053421\n",
            "step = 58200: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 58400: loss = 0.14714331924915314\n",
            "step = 58400: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 58600: loss = 0.09342120587825775\n",
            "step = 58600: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 58800: loss = 0.04148406535387039\n",
            "step = 58800: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 59000: loss = 0.09268355369567871\n",
            "step = 59000: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 59200: loss = 0.14146877825260162\n",
            "step = 59200: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 59400: loss = 0.051568128168582916\n",
            "step = 59400: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 59600: loss = 0.08785706758499146\n",
            "step = 59600: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 59800: loss = 0.11043544113636017\n",
            "step = 59800: Average Return = 0.8799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 60000: loss = 0.06815170496702194\n",
            "step = 60000: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 60200: loss = 0.053093355149030685\n",
            "step = 60200: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 60400: loss = 0.11972808092832565\n",
            "step = 60400: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 60600: loss = 0.06693372130393982\n",
            "step = 60600: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 60800: loss = 0.10336069762706757\n",
            "step = 60800: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 61000: loss = 0.09244278818368912\n",
            "step = 61000: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 61200: loss = 0.058250363916158676\n",
            "step = 61200: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 61400: loss = 0.07413429766893387\n",
            "step = 61400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 61600: loss = 0.016588497906923294\n",
            "step = 61600: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 61800: loss = 0.12079145759344101\n",
            "step = 61800: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 62000: loss = 0.0685894712805748\n",
            "step = 62000: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 62200: loss = 0.09780531376600266\n",
            "step = 62200: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 62400: loss = 0.12243689596652985\n",
            "step = 62400: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 62600: loss = 0.05794861912727356\n",
            "step = 62600: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 62800: loss = 0.1373061239719391\n",
            "step = 62800: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 63000: loss = 0.15943551063537598\n",
            "step = 63000: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 63200: loss = 0.317226767539978\n",
            "step = 63200: Average Return = 0.9200000166893005\n",
            "no_improvement_steps:  0\n",
            "step = 63400: loss = 0.1051042228937149\n",
            "step = 63400: Average Return = 0.8799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 63600: loss = 0.22769668698310852\n",
            "step = 63600: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 63800: loss = 0.013582203537225723\n",
            "step = 63800: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 64000: loss = 0.04829148203134537\n",
            "step = 64000: Average Return = 0.8799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 64200: loss = 0.1221839189529419\n",
            "step = 64200: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 64400: loss = 0.06121499091386795\n",
            "step = 64400: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 64600: loss = 0.19209983944892883\n",
            "step = 64600: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 64800: loss = 0.13115832209587097\n",
            "step = 64800: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 65000: loss = 0.10671783983707428\n",
            "step = 65000: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 65200: loss = 0.1795388162136078\n",
            "step = 65200: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 65400: loss = 0.18909476697444916\n",
            "step = 65400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 65600: loss = 0.1425035297870636\n",
            "step = 65600: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 65800: loss = 0.0762692540884018\n",
            "step = 65800: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 66000: loss = 0.11487722396850586\n",
            "step = 66000: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 66200: loss = 0.13694807887077332\n",
            "step = 66200: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 66400: loss = 0.12657396495342255\n",
            "step = 66400: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 66600: loss = 0.09858700633049011\n",
            "step = 66600: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 66800: loss = 0.041549526154994965\n",
            "step = 66800: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 67000: loss = 0.1349208503961563\n",
            "step = 67000: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 67200: loss = 0.145681232213974\n",
            "step = 67200: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 67400: loss = 0.20422109961509705\n",
            "step = 67400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 67600: loss = 0.18939542770385742\n",
            "step = 67600: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 67800: loss = 0.15578757226467133\n",
            "step = 67800: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 68000: loss = 0.01224982738494873\n",
            "step = 68000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 68200: loss = 0.0634462758898735\n",
            "step = 68200: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 68400: loss = 0.11792805790901184\n",
            "step = 68400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 68600: loss = 0.06846506893634796\n",
            "step = 68600: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 68800: loss = 0.11234509199857712\n",
            "step = 68800: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 69000: loss = 0.1726406216621399\n",
            "step = 69000: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 69200: loss = 0.15100465714931488\n",
            "step = 69200: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 69400: loss = 0.14644035696983337\n",
            "step = 69400: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 69600: loss = 0.03562241047620773\n",
            "step = 69600: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 69800: loss = 0.043856531381607056\n",
            "step = 69800: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 70000: loss = 0.2159019112586975\n",
            "step = 70000: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 70200: loss = 0.06080266833305359\n",
            "step = 70200: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 70400: loss = 0.19968563318252563\n",
            "step = 70400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 70600: loss = 0.153955340385437\n",
            "step = 70600: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 70800: loss = 0.06347325444221497\n",
            "step = 70800: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 71000: loss = 0.1248396709561348\n",
            "step = 71000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 71200: loss = 0.08754058927297592\n",
            "step = 71200: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 71400: loss = 0.09657877683639526\n",
            "step = 71400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 71600: loss = 0.016926884651184082\n",
            "step = 71600: Average Return = 0.9200000166893005\n",
            "no_improvement_steps:  0\n",
            "step = 71800: loss = 0.05088174715638161\n",
            "step = 71800: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 72000: loss = 0.15317247807979584\n",
            "step = 72000: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 72200: loss = 0.17331190407276154\n",
            "step = 72200: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 72400: loss = 0.08396679162979126\n",
            "step = 72400: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 72600: loss = 0.12845861911773682\n",
            "step = 72600: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 72800: loss = 0.13669952750205994\n",
            "step = 72800: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 73000: loss = 0.19621676206588745\n",
            "step = 73000: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 73200: loss = 0.1772138476371765\n",
            "step = 73200: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 73400: loss = 0.11984111368656158\n",
            "step = 73400: Average Return = 0.8799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 73600: loss = 0.07012908160686493\n",
            "step = 73600: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 73800: loss = 0.12863746285438538\n",
            "step = 73800: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 74000: loss = 0.18528224527835846\n",
            "step = 74000: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 74200: loss = 0.20383502542972565\n",
            "step = 74200: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 74400: loss = 0.06880024820566177\n",
            "step = 74400: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 74600: loss = 0.12964990735054016\n",
            "step = 74600: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 74800: loss = 0.21122828125953674\n",
            "step = 74800: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 75000: loss = 0.17994424700737\n",
            "step = 75000: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 75200: loss = 0.0593266487121582\n",
            "step = 75200: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 75400: loss = 0.08290780335664749\n",
            "step = 75400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 75600: loss = 0.05776748061180115\n",
            "step = 75600: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 75800: loss = 0.05705823749303818\n",
            "step = 75800: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 76000: loss = 0.1646135449409485\n",
            "step = 76000: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 76200: loss = 0.1366325467824936\n",
            "step = 76200: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 76400: loss = 0.08633290976285934\n",
            "step = 76400: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 76600: loss = 0.11514387279748917\n",
            "step = 76600: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 76800: loss = 0.09809055179357529\n",
            "step = 76800: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 77000: loss = 0.16654539108276367\n",
            "step = 77000: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 77200: loss = 0.10887850075960159\n",
            "step = 77200: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 77400: loss = 0.07207119464874268\n",
            "step = 77400: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 77600: loss = 0.043928906321525574\n",
            "step = 77600: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 77800: loss = 0.0595550574362278\n",
            "step = 77800: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 78000: loss = 0.14275886118412018\n",
            "step = 78000: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 78200: loss = 0.09073922783136368\n",
            "step = 78200: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 78400: loss = 0.06497789919376373\n",
            "step = 78400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 78600: loss = 0.0518002063035965\n",
            "step = 78600: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 78800: loss = 0.057247478514909744\n",
            "step = 78800: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 79000: loss = 0.14499936997890472\n",
            "step = 79000: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 79200: loss = 0.14852121472358704\n",
            "step = 79200: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 79400: loss = 0.136432945728302\n",
            "step = 79400: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 79600: loss = 0.08542881160974503\n",
            "step = 79600: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 79800: loss = 0.14565376937389374\n",
            "step = 79800: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 80000: loss = 0.06865599751472473\n",
            "step = 80000: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 80200: loss = 0.14866793155670166\n",
            "step = 80200: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 80400: loss = 0.15069496631622314\n",
            "step = 80400: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 80600: loss = 0.0521283820271492\n",
            "step = 80600: Average Return = 0.8799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 80800: loss = 0.04353093355894089\n",
            "step = 80800: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 81000: loss = 0.09308285266160965\n",
            "step = 81000: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 81200: loss = 0.1488274335861206\n",
            "step = 81200: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 81400: loss = 0.057899296283721924\n",
            "step = 81400: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 81600: loss = 0.03146019205451012\n",
            "step = 81600: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 81800: loss = 0.1505652666091919\n",
            "step = 81800: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 82000: loss = 0.16007646918296814\n",
            "step = 82000: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 82200: loss = 0.026813074946403503\n",
            "step = 82200: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 82400: loss = 0.05675966292619705\n",
            "step = 82400: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 82600: loss = 0.0824810266494751\n",
            "step = 82600: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 82800: loss = 0.11878762394189835\n",
            "step = 82800: Average Return = 0.8799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 83000: loss = 0.093156598508358\n",
            "step = 83000: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 83200: loss = 0.11897769570350647\n",
            "step = 83200: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 83400: loss = 0.16029705107212067\n",
            "step = 83400: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 83600: loss = 0.1630517542362213\n",
            "step = 83600: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 83800: loss = 0.19658292829990387\n",
            "step = 83800: Average Return = 0.9200000166893005\n",
            "no_improvement_steps:  0\n",
            "step = 84000: loss = 0.12566246092319489\n",
            "step = 84000: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 84200: loss = 0.09105837345123291\n",
            "step = 84200: Average Return = 0.9200000166893005\n",
            "no_improvement_steps:  0\n",
            "step = 84400: loss = 0.08594918251037598\n",
            "step = 84400: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 84600: loss = 0.10363799333572388\n",
            "step = 84600: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 84800: loss = 0.24970148503780365\n",
            "step = 84800: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 85000: loss = 0.03065900132060051\n",
            "step = 85000: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 85200: loss = 0.10362271219491959\n",
            "step = 85200: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 85400: loss = 0.06580646336078644\n",
            "step = 85400: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 85600: loss = 0.1113860160112381\n",
            "step = 85600: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 85800: loss = 0.022962216287851334\n",
            "step = 85800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 86000: loss = 0.1534593105316162\n",
            "step = 86000: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 86200: loss = 0.11781240254640579\n",
            "step = 86200: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 86400: loss = 0.019748395308852196\n",
            "step = 86400: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 86600: loss = 0.05218884348869324\n",
            "step = 86600: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 86800: loss = 0.035604607313871384\n",
            "step = 86800: Average Return = 0.9200000166893005\n",
            "no_improvement_steps:  0\n",
            "step = 87000: loss = 0.12753981351852417\n",
            "step = 87000: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 87200: loss = 0.1197998896241188\n",
            "step = 87200: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 87400: loss = 0.060142241418361664\n",
            "step = 87400: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 87600: loss = 0.049091070890426636\n",
            "step = 87600: Average Return = 0.9200000166893005\n",
            "no_improvement_steps:  0\n",
            "step = 87800: loss = 0.05073821544647217\n",
            "step = 87800: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 88000: loss = 0.14668500423431396\n",
            "step = 88000: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 88200: loss = 0.08353078365325928\n",
            "step = 88200: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 88400: loss = 0.07260826230049133\n",
            "step = 88400: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 88600: loss = 0.04295520484447479\n",
            "step = 88600: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 88800: loss = 0.041482992470264435\n",
            "step = 88800: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 89000: loss = 0.14854112267494202\n",
            "step = 89000: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 89200: loss = 0.1586422622203827\n",
            "step = 89200: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 89400: loss = 0.06883140653371811\n",
            "step = 89400: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 89600: loss = 0.21869182586669922\n",
            "step = 89600: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 89800: loss = 0.07557087391614914\n",
            "step = 89800: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 90000: loss = 0.04547220841050148\n",
            "step = 90000: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 90200: loss = 0.12414386868476868\n",
            "step = 90200: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 90400: loss = 0.09919475018978119\n",
            "step = 90400: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 90600: loss = 0.045353688299655914\n",
            "step = 90600: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 90800: loss = 0.11867615580558777\n",
            "step = 90800: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 91000: loss = 0.14968402683734894\n",
            "step = 91000: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 91200: loss = 0.14337699115276337\n",
            "step = 91200: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 91400: loss = 0.22094309329986572\n",
            "step = 91400: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 91600: loss = 0.12607403099536896\n",
            "step = 91600: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 91800: loss = 0.2112278938293457\n",
            "step = 91800: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 92000: loss = 0.06543562561273575\n",
            "step = 92000: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 92200: loss = 0.13696198165416718\n",
            "step = 92200: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 92400: loss = 0.054897405207157135\n",
            "step = 92400: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 92600: loss = 0.10879483819007874\n",
            "step = 92600: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 92800: loss = 0.17661598324775696\n",
            "step = 92800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 93000: loss = 0.11062301695346832\n",
            "step = 93000: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 93200: loss = 0.015726545825600624\n",
            "step = 93200: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 93400: loss = 0.06453819572925568\n",
            "step = 93400: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 93600: loss = 0.06960088759660721\n",
            "step = 93600: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 93800: loss = 0.12352655082941055\n",
            "step = 93800: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 94000: loss = 0.14500141143798828\n",
            "step = 94000: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 94200: loss = 0.10712289065122604\n",
            "step = 94200: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 94400: loss = 0.09883810579776764\n",
            "step = 94400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 94600: loss = 0.04412536323070526\n",
            "step = 94600: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 94800: loss = 0.016681987792253494\n",
            "step = 94800: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 95000: loss = 0.11061021685600281\n",
            "step = 95000: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 95200: loss = 0.1394062638282776\n",
            "step = 95200: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 95400: loss = 0.07709682732820511\n",
            "step = 95400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 95600: loss = 0.09244097769260406\n",
            "step = 95600: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 95800: loss = 0.08356823027133942\n",
            "step = 95800: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 96000: loss = 0.11918811500072479\n",
            "step = 96000: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 96200: loss = 0.06289313733577728\n",
            "step = 96200: Average Return = 0.9200000166893005\n",
            "no_improvement_steps:  0\n",
            "step = 96400: loss = 0.2003593146800995\n",
            "step = 96400: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 96600: loss = 0.2603287696838379\n",
            "step = 96600: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 96800: loss = 0.11240516602993011\n",
            "step = 96800: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 97000: loss = 0.1205330565571785\n",
            "step = 97000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 97200: loss = 0.15456542372703552\n",
            "step = 97200: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 97400: loss = 0.0975567102432251\n",
            "step = 97400: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 97600: loss = 0.08798541873693466\n",
            "step = 97600: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 97800: loss = 0.07196885347366333\n",
            "step = 97800: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 98000: loss = 0.07991272211074829\n",
            "step = 98000: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 98200: loss = 0.07545630633831024\n",
            "step = 98200: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 98400: loss = 0.016129637137055397\n",
            "step = 98400: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 98600: loss = 0.12221716344356537\n",
            "step = 98600: Average Return = 0.8999999761581421\n",
            "no_improvement_steps:  0\n",
            "step = 98800: loss = 0.1653665453195572\n",
            "step = 98800: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 99000: loss = 0.046613968908786774\n",
            "step = 99000: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 99200: loss = 0.16851970553398132\n",
            "step = 99200: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 99400: loss = 0.08156456053256989\n",
            "step = 99400: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 99600: loss = 0.190812349319458\n",
            "step = 99600: Average Return = 0.9200000166893005\n",
            "no_improvement_steps:  0\n",
            "step = 99800: loss = 0.11459031701087952\n",
            "step = 99800: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 100000: loss = 0.08662353456020355\n",
            "step = 100000: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_policy = tf.saved_model.load(policy_dir)"
      ],
      "metadata": {
        "id": "D9He1Uj9s6ur"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_return = compute_avg_return(eval_env, saved_policy, 5000)"
      ],
      "metadata": {
        "id": "wC5lJb8es9Lk"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_return"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2u4nbq2eQoa",
        "outputId": "8a14fe96-63e7-4640-dde0-00818386fa39"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8344"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_return = compute_avg_return(train_env, saved_policy, 5000)"
      ],
      "metadata": {
        "id": "E-gNrQ9teSAE"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_return"
      ],
      "metadata": {
        "id": "CCKzVR_U0wPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb4074ce-30d8-427e-cdca-39fcea1d99b1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5056"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = range(0, iter_count + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "#plt.ylim(top=250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "WIVpHQGzeTqj",
        "outputId": "c9bc3d49-0083-4bca-f072-d0a0a1220a45"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Iterations')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KNIuFszNF-PS"
      },
      "execution_count": 30,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
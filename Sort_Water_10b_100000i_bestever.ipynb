{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BlI8iHgEP1wB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0780f0e7-b2fe-4564-f848-58af209fe338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf_agents\n",
            "  Downloading tf_agents-0.19.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf_agents)\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf_agents) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (4.5.0)\n",
            "Collecting pygame==2.1.3 (from tf_agents)\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-probability~=0.23.0 (from tf_agents)\n",
            "  Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents) (0.0.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (0.5.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (0.1.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697628 sha256=1999e6876112268529d95d793ddf52e67bc4a8fd82912be44d8938a775c18df4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
            "Successfully built gym\n",
            "Installing collected packages: tensorflow-probability, pygame, gym, tf_agents\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.22.0\n",
            "    Uninstalling tensorflow-probability-0.22.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.22.0\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.23.0 pygame-2.1.3 tensorflow-probability-0.23.0 tf_agents-0.19.0\n",
            "Requirement already satisfied: tf-agents[reverb] in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.23.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
            "Collecting rlds (from tf-agents[reverb])\n",
            "  Downloading rlds-0.1.8-py3-none-manylinux2010_x86_64.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m799.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dm-reverb~=0.14.0 (from tf-agents[reverb])\n",
            "  Downloading dm_reverb-0.14.0-cp310-cp310-manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow~=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.15.0->tf-agents[reverb]) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb~=0.14.0->tf-agents[reverb]) (5.9.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.2.2)\n",
            "Installing collected packages: rlds, dm-reverb\n",
            "Successfully installed dm-reverb-0.14.0 rlds-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install tf_agents\n",
        "!pip install tf-agents[reverb]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CLWWFFWwP-Fk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tf_agents\n",
        "import reverb\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import epsilon_greedy_policy\n",
        "from tf_agents.policies import policy_saver\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/E6885_Project')\n",
        "import SortWaterEnv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.version.VERSION"
      ],
      "metadata": {
        "id": "S7-5FyITqOTK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bfbba0f2-198d-470d-d9ed-28959fb158a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "meuuIXmuQLEW"
      },
      "outputs": [],
      "source": [
        "num_iterations = 100000        #\n",
        "\n",
        "initial_collect_steps = 100     #\n",
        "collect_steps_per_iteration = 10#2#1   #\n",
        "replay_buffer_max_length = 100000  #\n",
        "\n",
        "batch_size = 64            #\n",
        "learning_rate = 1e-3        #\n",
        "log_interval = 200          #\n",
        "\n",
        "num_eval_episodes = 100        #\n",
        "eval_interval = 500#1000        #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_Be0fVlSQM1w"
      },
      "outputs": [],
      "source": [
        "############# create training and evaluation environment #############\n",
        "num_bottles = 10\n",
        "water_level = 4\n",
        "env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level, random_init=True)\n",
        "train_py_env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level, random_init=True)\n",
        "eval_py_env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level, random_init=True)\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KSsS5sENQQa2"
      },
      "outputs": [],
      "source": [
        "############ create a DQN agent ############\n",
        "fc_layer_params = (500, 250)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jddUDIr2QSTO"
      },
      "outputs": [],
      "source": [
        "# Customized Q netword\n",
        "class MaskedQNetwork(q_network.QNetwork):\n",
        "  def __init__(self, input_tensor_spec, action_spec, fc_layer_params=(100,), **kwargs):\n",
        "    # 从 input_tensor_spec 元组中提取观察值规格\n",
        "    observation_spec = input_tensor_spec[0]\n",
        "\n",
        "    # 调用基类的构造函数以构建网络\n",
        "    super(MaskedQNetwork, self).__init__(observation_spec, action_spec, fc_layer_params=fc_layer_params, **kwargs)\n",
        "\n",
        "  def call(self, observation, step_type=None, network_state=(), training=False):\n",
        "    # 直接调用父类的 call 方法，处理观察值\n",
        "    return super(MaskedQNetwork, self).call(\n",
        "        observation, step_type, network_state, training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rqi6-5F7RG-n"
      },
      "outputs": [],
      "source": [
        "observation_spec = train_env.observation_spec()\n",
        "action_spec = train_env.action_spec()\n",
        "\n",
        "# create Q-network\n",
        "q_net = MaskedQNetwork(\n",
        "    (observation_spec['observation'], observation_spec['action_mask']),\n",
        "    action_spec,\n",
        "    fc_layer_params=fc_layer_params\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jMoFiViiC7NV"
      },
      "outputs": [],
      "source": [
        "def observation_and_action_constraint_splitter(obs):\n",
        "\treturn obs['observation'], obs['action_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qI_nu75uRRqe"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter,\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1AhbI3ZfbFpm"
      },
      "outputs": [],
      "source": [
        "# an example, just to show what policies are used during evaluation and collecting\n",
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sH_fIsadbICb"
      },
      "outputs": [],
      "source": [
        "# an example, just show how to create epsilon_greedy_policy. Not to be used\n",
        "base_policy = agent.policy\n",
        "epsilon = 0.1  # 例如，使用 0.1 作为 epsilon 值\n",
        "epsilon_greedy_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(base_policy, epsilon=epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "z9kV-ZLvbJVC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63accbbc-0029-42bb-a707-c423a57ea86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TimeStep(\n",
            "{'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observation': <tf.Tensor: shape=(1, 10, 4), dtype=int32, numpy=\n",
            "array([[[1, 6, 6, 7],\n",
            "        [2, 2, 2, 6],\n",
            "        [3, 3, 3, 5],\n",
            "        [4, 4, 4, 4],\n",
            "        [5, 5, 5, 1],\n",
            "        [0, 0, 0, 0],\n",
            "        [0, 0, 0, 0],\n",
            "        [8, 8, 8, 7],\n",
            "        [8, 1, 7, 7],\n",
            "        [3, 1, 2, 6]]], dtype=int32)>,\n",
            "                 'action_mask': <tf.Tensor: shape=(1, 90), dtype=bool, numpy=\n",
            "array([[False, False, False, False,  True,  True, False, False, False,\n",
            "        False, False, False, False,  True,  True, False, False, False,\n",
            "        False, False, False, False,  True,  True, False, False, False,\n",
            "        False, False, False, False,  True,  True, False, False, False,\n",
            "        False, False, False, False,  True,  True, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False, False, False, False, False,  True,  True, False, False]])>}})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([86], dtype=int32)>, state=(), info=())"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# example of greedy policy choosing action\n",
        "example_py_env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level, random_init=True)\n",
        "example_env = tf_py_environment.TFPyEnvironment(example_py_env)\n",
        "time_step = example_env.reset()\n",
        "print(time_step)\n",
        "epsilon_greedy_policy.action(time_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_0MKdny2bK-Q"
      },
      "outputs": [],
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZuqXjbWrbMQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "690a86d8-d58b-4034-8125-826b80ae455e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1.0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# an example\n",
        "compute_avg_return(eval_env, epsilon_greedy_policy, num_eval_episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CQzc5_e4bP53"
      },
      "outputs": [],
      "source": [
        "################# Replay buffer #################\n",
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0XbLZjCtb__-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89fe7a3f-a617-445a-a656-a9f123cc59cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_TupleWrapper(Trajectory(\n",
              "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'observation': DictWrapper({'observation': BoundedTensorSpec(shape=(10, 4), dtype=tf.int32, name='observation', minimum=array(0, dtype=int32), maximum=array(8, dtype=int32)), 'action_mask': TensorSpec(shape=(90,), dtype=tf.bool, name='action_mask')}),\n",
              " 'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(89, dtype=int32)),\n",
              " 'policy_info': (),\n",
              " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
              " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))}))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "agent.collect_data_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TeCbnlphcesk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2a0d0e9-70ae-4794-e62c-d21682d81469"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('step_type',\n",
              " 'observation',\n",
              " 'action',\n",
              " 'policy_info',\n",
              " 'next_step_type',\n",
              " 'reward',\n",
              " 'discount')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "agent.collect_data_spec._fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "X3Hv74IhckJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "467fc3ed-dd3b-485b-a2e3-f89f6efdce75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TimeStep(\n",
              " {'step_type': array(1, dtype=int32),\n",
              "  'reward': array(0., dtype=float32),\n",
              "  'discount': array(1., dtype=float32),\n",
              "  'observation': {'observation': array([[0, 0, 0, 0],\n",
              "        [4, 4, 4, 4],\n",
              "        [1, 1, 0, 0],\n",
              "        [2, 2, 2, 0],\n",
              "        [7, 7, 7, 7],\n",
              "        [6, 6, 6, 6],\n",
              "        [3, 3, 3, 0],\n",
              "        [8, 8, 8, 8],\n",
              "        [2, 5, 5, 3],\n",
              "        [1, 1, 5, 5]], dtype=int32),\n",
              "                  'action_mask': array([False, False, False, False, False, False, False, False, False,\n",
              "         True, False, False, False, False, False, False, False, False,\n",
              "         True, False, False, False, False, False, False, False, False,\n",
              "         True, False, False, False, False, False, False, False, False,\n",
              "         True, False, False, False, False, False, False, False, False,\n",
              "         True, False, False, False, False, False, False, False, False,\n",
              "         True, False, False, False, False, False, False, False, False,\n",
              "         True, False, False, False, False, False, False, False, False,\n",
              "         True, False, False, False, False, False,  True, False, False,\n",
              "         True, False, False, False, False, False, False, False, False])}}),\n",
              " ())"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# an example.\n",
        "py_driver.PyDriver(\n",
        "    train_py_env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      epsilon_greedy_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(train_py_env.reset())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.get_state()"
      ],
      "metadata": {
        "id": "ohhyBZROzNh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "640f0d03-9dbf-49a2-a271-67595002aae1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'observation': array([[0, 0, 0, 0],\n",
              "        [2, 2, 2, 3],\n",
              "        [1, 6, 6, 1],\n",
              "        [4, 4, 4, 6],\n",
              "        [5, 5, 3, 8],\n",
              "        [0, 0, 0, 0],\n",
              "        [7, 7, 7, 3],\n",
              "        [8, 8, 8, 4],\n",
              "        [7, 5, 5, 1],\n",
              "        [2, 3, 1, 6]], dtype=int32),\n",
              " 'action_mask': array([False, False, False, False, False, False, False, False, False,\n",
              "         True, False, False, False,  True, False, False, False, False,\n",
              "         True, False, False, False,  True, False, False, False, False,\n",
              "         True, False, False, False,  True, False, False, False, False,\n",
              "         True, False, False, False,  True, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "         True, False, False, False, False,  True, False, False, False,\n",
              "         True, False, False, False, False,  True, False, False, False,\n",
              "         True, False, False, False, False,  True, False, False, False,\n",
              "         True, False, False, False, False,  True, False, False, False])}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nh7SqXgkdYNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80672898-c366-4bee-b51b-790026e6d34e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(Trajectory(\n",
              "{'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'observation': DictWrapper({'observation': TensorSpec(shape=(64, 2, 10, 4), dtype=tf.int32, name=None), 'action_mask': TensorSpec(shape=(64, 2, 90), dtype=tf.bool, name=None)}),\n",
              " 'action': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'policy_info': (),\n",
              " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
              " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None)}), SampleInfo(key=TensorSpec(shape=(64, 2), dtype=tf.uint64, name=None), probability=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), table_size=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), priority=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), times_sampled=TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)))>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Mp0I59Vwd11M"
      },
      "outputs": [],
      "source": [
        "iterator = iter(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pqBPd98nu52O"
      },
      "outputs": [],
      "source": [
        "# demo: only 1 episode\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = train_py_env.reset()\n",
        "\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    train_py_env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "######### early stopping ##########\n",
        "# set save direction\n",
        "tempdir = os.getenv(\"TEST_TMPDIR\", tempfile.gettempdir())\n",
        "# checkpointer\n",
        "# checkpoint_dir = os.path.join(tempdir, 'checkpoint')\n",
        "# train_checkpointer = common.Checkpointer(\n",
        "#     ckpt_dir=checkpoint_dir,\n",
        "#     max_to_keep=1,\n",
        "#     agent=agent,\n",
        "#     policy=agent.policy,\n",
        "#     replay_buffer=replay_buffer,\n",
        "#     global_step=train_step_counter\n",
        "# )\n",
        "# policy saver\n",
        "policy_dir = os.path.join(tempdir, 'policy')\n",
        "tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
        "\n",
        "# some threshold\n",
        "#best_avg_return = -float('inf')\n",
        "best_avg_return = 0.7\n",
        "no_improvement_steps = 0\n",
        "early_stopping_threshold = 10\n",
        "earlystop = False\n",
        "iter_thres = 10000\n",
        "######### early stopping ##########\n",
        "\n",
        "\n",
        "iter_count = 0\n",
        "for _ in range(num_iterations):\n",
        "  iter_count += 1\n",
        "  #print('iter_count: ',iter_count)\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)\n",
        "\n",
        "  ######### early stopping ##########\n",
        "    if avg_return > best_avg_return:\n",
        "      earlystop = True\n",
        "    if (avg_return > best_avg_return) and earlystop and (iter_count > iter_thres):\n",
        "      best_avg_return = avg_return\n",
        "      no_improvement_steps = 0\n",
        "      tf_policy_saver.save(policy_dir)  # save policy\n",
        "    elif earlystop and (iter_count > iter_thres):\n",
        "      no_improvement_steps += 0\n",
        "    print(\"no_improvement_steps: \",no_improvement_steps)\n",
        "  if no_improvement_steps >= early_stopping_threshold:\n",
        "    print(\"No improvement for {0} steps. Early stopping...\".format(no_improvement_steps))\n",
        "    break\n",
        "  ######### early stopping ##########"
      ],
      "metadata": {
        "id": "35k0m3HtqQyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d52855-1021-4894-e5eb-040412939ed9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 200: loss = 21024076.0\n",
            "step = 400: loss = 250304688.0\n",
            "step = 500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 600: loss = 21602046.0\n",
            "step = 800: loss = 8712065.0\n",
            "step = 1000: loss = 5645407.0\n",
            "step = 1000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 1200: loss = 689928.4375\n",
            "step = 1400: loss = 468532.1875\n",
            "step = 1500: Average Return = -0.9599999785423279\n",
            "no_improvement_steps:  0\n",
            "step = 1600: loss = 110989.25\n",
            "step = 1800: loss = 25279.328125\n",
            "step = 2000: loss = 8531.416015625\n",
            "step = 2000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 2200: loss = 5859.39208984375\n",
            "step = 2400: loss = 1983.024658203125\n",
            "step = 2500: Average Return = -0.9599999785423279\n",
            "no_improvement_steps:  0\n",
            "step = 2600: loss = 1489.4410400390625\n",
            "step = 2800: loss = 3576.656005859375\n",
            "step = 3000: loss = 1704.7164306640625\n",
            "step = 3000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 3200: loss = 1105.500244140625\n",
            "step = 3400: loss = 557.1576538085938\n",
            "step = 3500: Average Return = -0.9599999785423279\n",
            "no_improvement_steps:  0\n",
            "step = 3600: loss = 635.0942993164062\n",
            "step = 3800: loss = 369.9956970214844\n",
            "step = 4000: loss = 247.98184204101562\n",
            "step = 4000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 4200: loss = 187.6107177734375\n",
            "step = 4400: loss = 264.4964294433594\n",
            "step = 4500: Average Return = -0.9599999785423279\n",
            "no_improvement_steps:  0\n",
            "step = 4600: loss = 170.70999145507812\n",
            "step = 4800: loss = 216.96572875976562\n",
            "step = 5000: loss = 95.5699462890625\n",
            "step = 5000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 5200: loss = 119.1082763671875\n",
            "step = 5400: loss = 98.12492370605469\n",
            "step = 5500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 5600: loss = 51.68840789794922\n",
            "step = 5800: loss = 138.99722290039062\n",
            "step = 6000: loss = 24.609989166259766\n",
            "step = 6000: Average Return = -0.9599999785423279\n",
            "no_improvement_steps:  0\n",
            "step = 6200: loss = 31.702251434326172\n",
            "step = 6400: loss = 52.30818557739258\n",
            "step = 6500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 6600: loss = 37.62214279174805\n",
            "step = 6800: loss = 13.639379501342773\n",
            "step = 7000: loss = 15.082801818847656\n",
            "step = 7000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 7200: loss = 8.642658233642578\n",
            "step = 7400: loss = 7.509896755218506\n",
            "step = 7500: Average Return = -0.9399999976158142\n",
            "no_improvement_steps:  0\n",
            "step = 7600: loss = 6.568126201629639\n",
            "step = 7800: loss = 5.641543388366699\n",
            "step = 8000: loss = 5.589376449584961\n",
            "step = 8000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 8200: loss = 3.8861801624298096\n",
            "step = 8400: loss = 2.085070848464966\n",
            "step = 8500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 8600: loss = 2.2981748580932617\n",
            "step = 8800: loss = 1.2217037677764893\n",
            "step = 9000: loss = 1.1968817710876465\n",
            "step = 9000: Average Return = -0.9599999785423279\n",
            "no_improvement_steps:  0\n",
            "step = 9200: loss = 0.7152940034866333\n",
            "step = 9400: loss = 0.6987915635108948\n",
            "step = 9500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 9600: loss = 0.4131607413291931\n",
            "step = 9800: loss = 0.495874285697937\n",
            "step = 10000: loss = 0.24269840121269226\n",
            "step = 10000: Average Return = -0.9399999976158142\n",
            "no_improvement_steps:  0\n",
            "step = 10200: loss = 0.10128180682659149\n",
            "step = 10400: loss = 0.15693317353725433\n",
            "step = 10500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 10600: loss = 0.2476852834224701\n",
            "step = 10800: loss = 0.5948953628540039\n",
            "step = 11000: loss = 0.03617426007986069\n",
            "step = 11000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 11200: loss = 0.1272258758544922\n",
            "step = 11400: loss = 0.5460600852966309\n",
            "step = 11500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 11600: loss = 0.09192779660224915\n",
            "step = 11800: loss = 0.07191062718629837\n",
            "step = 12000: loss = 0.05004296451807022\n",
            "step = 12000: Average Return = -0.9599999785423279\n",
            "no_improvement_steps:  0\n",
            "step = 12200: loss = 1.5673747062683105\n",
            "step = 12400: loss = 1.468562126159668\n",
            "step = 12500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 12600: loss = 0.07874101400375366\n",
            "step = 12800: loss = 0.06792696565389633\n",
            "step = 13000: loss = 0.018267476931214333\n",
            "step = 13000: Average Return = -0.9599999785423279\n",
            "no_improvement_steps:  0\n",
            "step = 13200: loss = 0.036324188113212585\n",
            "step = 13400: loss = 0.047305624932050705\n",
            "step = 13500: Average Return = -0.9599999785423279\n",
            "no_improvement_steps:  0\n",
            "step = 13600: loss = 0.03655280917882919\n",
            "step = 13800: loss = 0.01530470885336399\n",
            "step = 14000: loss = 0.05501536279916763\n",
            "step = 14000: Average Return = -0.9599999785423279\n",
            "no_improvement_steps:  0\n",
            "step = 14200: loss = 0.01431953813880682\n",
            "step = 14400: loss = 0.04573318734765053\n",
            "step = 14500: Average Return = -0.9399999976158142\n",
            "no_improvement_steps:  0\n",
            "step = 14600: loss = 0.025028761476278305\n",
            "step = 14800: loss = 0.014519940130412579\n",
            "step = 15000: loss = 0.03509186580777168\n",
            "step = 15000: Average Return = -0.8999999761581421\n",
            "no_improvement_steps:  0\n",
            "step = 15200: loss = 0.043728165328502655\n",
            "step = 15400: loss = 0.0754738599061966\n",
            "step = 15500: Average Return = -0.9399999976158142\n",
            "no_improvement_steps:  0\n",
            "step = 15600: loss = 0.07338137179613113\n",
            "step = 15800: loss = 0.07644814997911453\n",
            "step = 16000: loss = 0.08745156973600388\n",
            "step = 16000: Average Return = -0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 16200: loss = 0.05220517888665199\n",
            "step = 16400: loss = 0.012898493558168411\n",
            "step = 16500: Average Return = -0.9399999976158142\n",
            "no_improvement_steps:  0\n",
            "step = 16600: loss = 0.021424464881420135\n",
            "step = 16800: loss = 0.10235662013292313\n",
            "step = 17000: loss = 0.0413503497838974\n",
            "step = 17000: Average Return = -0.8999999761581421\n",
            "no_improvement_steps:  0\n",
            "step = 17200: loss = 0.04198536276817322\n",
            "step = 17400: loss = 0.06374284625053406\n",
            "step = 17500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 17600: loss = 0.08940568566322327\n",
            "step = 17800: loss = 0.012881824746727943\n",
            "step = 18000: loss = 0.03155309334397316\n",
            "step = 18000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 18200: loss = 0.028704889118671417\n",
            "step = 18400: loss = 0.04151470959186554\n",
            "step = 18500: Average Return = -0.9399999976158142\n",
            "no_improvement_steps:  0\n",
            "step = 18600: loss = 0.062327560037374496\n",
            "step = 18800: loss = 0.042240604758262634\n",
            "step = 19000: loss = 0.001079668989405036\n",
            "step = 19000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 19200: loss = 0.01941577158868313\n",
            "step = 19400: loss = 0.02319660410284996\n",
            "step = 19500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 19600: loss = 0.009603673592209816\n",
            "step = 19800: loss = 0.03860911726951599\n",
            "step = 20000: loss = 0.03851214796304703\n",
            "step = 20000: Average Return = -0.8799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 20200: loss = 0.029828796163201332\n",
            "step = 20400: loss = 0.017503544688224792\n",
            "step = 20500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 20600: loss = 0.00889213103801012\n",
            "step = 20800: loss = 0.010047512128949165\n",
            "step = 21000: loss = 0.019059395417571068\n",
            "step = 21000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 21200: loss = 0.09548421949148178\n",
            "step = 21400: loss = 0.04921822249889374\n",
            "step = 21500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 21600: loss = 0.01661173440515995\n",
            "step = 21800: loss = 0.01722181960940361\n",
            "step = 22000: loss = 0.06334040313959122\n",
            "step = 22000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 22200: loss = 0.048320844769477844\n",
            "step = 22400: loss = 0.030980153009295464\n",
            "step = 22500: Average Return = -0.9599999785423279\n",
            "no_improvement_steps:  0\n",
            "step = 22600: loss = 0.08368872106075287\n",
            "step = 22800: loss = 0.06352165341377258\n",
            "step = 23000: loss = 0.05607344210147858\n",
            "step = 23000: Average Return = -0.9200000166893005\n",
            "no_improvement_steps:  0\n",
            "step = 23200: loss = 0.049035463482141495\n",
            "step = 23400: loss = 0.04113007336854935\n",
            "step = 23500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 23600: loss = 0.013265077024698257\n",
            "step = 23800: loss = 0.03263625502586365\n",
            "step = 24000: loss = 0.01964459754526615\n",
            "step = 24000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 24200: loss = 0.06012653559446335\n",
            "step = 24400: loss = 0.01823195070028305\n",
            "step = 24500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 24600: loss = 0.08265461027622223\n",
            "step = 24800: loss = 0.0059202322736382484\n",
            "step = 25000: loss = 0.015812603756785393\n",
            "step = 25000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 25200: loss = 0.025812441483139992\n",
            "step = 25400: loss = 0.00465232040733099\n",
            "step = 25500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 25600: loss = 0.009195517748594284\n",
            "step = 25800: loss = 0.03744528070092201\n",
            "step = 26000: loss = 0.04318450018763542\n",
            "step = 26000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 26200: loss = 0.008713680319488049\n",
            "step = 26400: loss = 0.012528473511338234\n",
            "step = 26500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 26600: loss = 0.11006183922290802\n",
            "step = 26800: loss = 0.014559851959347725\n",
            "step = 27000: loss = 0.011106958612799644\n",
            "step = 27000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 27200: loss = 0.010783395729959011\n",
            "step = 27400: loss = 0.006802568212151527\n",
            "step = 27500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 27600: loss = 0.012515426613390446\n",
            "step = 27800: loss = 0.00025609982549212873\n",
            "step = 28000: loss = 0.014683143235743046\n",
            "step = 28000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 28200: loss = 0.013746947050094604\n",
            "step = 28400: loss = 0.015480312518775463\n",
            "step = 28500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 28600: loss = 0.0050551919266581535\n",
            "step = 28800: loss = 0.012049010023474693\n",
            "step = 29000: loss = 0.0071821436285972595\n",
            "step = 29000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 29200: loss = 0.006692355033010244\n",
            "step = 29400: loss = 0.004529994912445545\n",
            "step = 29500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 29600: loss = 0.008018476888537407\n",
            "step = 29800: loss = 0.0037873582914471626\n",
            "step = 30000: loss = 0.007205819245427847\n",
            "step = 30000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 30200: loss = 0.003468889044597745\n",
            "step = 30400: loss = 0.011278104037046432\n",
            "step = 30500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 30600: loss = 0.05149060860276222\n",
            "step = 30800: loss = 0.0031688103917986155\n",
            "step = 31000: loss = 0.008298042230308056\n",
            "step = 31000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 31200: loss = 0.0051954565569758415\n",
            "step = 31400: loss = 0.13979089260101318\n",
            "step = 31500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 31600: loss = 0.005296306684613228\n",
            "step = 31800: loss = 0.05234472453594208\n",
            "step = 32000: loss = 0.005045806057751179\n",
            "step = 32000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 32200: loss = 0.007065805606544018\n",
            "step = 32400: loss = 0.0034147468395531178\n",
            "step = 32500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 32600: loss = 0.003383277915418148\n",
            "step = 32800: loss = 0.003379673231393099\n",
            "step = 33000: loss = 0.04785057157278061\n",
            "step = 33000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 33200: loss = 0.05087021738290787\n",
            "step = 33400: loss = 0.0021186324302107096\n",
            "step = 33500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 33600: loss = 0.00307914474979043\n",
            "step = 33800: loss = 0.002886613132432103\n",
            "step = 34000: loss = 0.0019652871415019035\n",
            "step = 34000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 34200: loss = 0.0044273994863033295\n",
            "step = 34400: loss = 0.002035384764894843\n",
            "step = 34500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 34600: loss = 0.0033292591106146574\n",
            "step = 34800: loss = 0.003319374518468976\n",
            "step = 35000: loss = 0.0008099761907942593\n",
            "step = 35000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 35200: loss = 0.0030893199145793915\n",
            "step = 35400: loss = 0.0008617748972028494\n",
            "step = 35500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 35600: loss = 0.05281911790370941\n",
            "step = 35800: loss = 0.004385833162814379\n",
            "step = 36000: loss = 0.0009663656819611788\n",
            "step = 36000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 36200: loss = 0.0008662960026413202\n",
            "step = 36400: loss = 0.0022784462198615074\n",
            "step = 36500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 36600: loss = 0.0008468445157632232\n",
            "step = 36800: loss = 0.004970252979546785\n",
            "step = 37000: loss = 0.0029569524340331554\n",
            "step = 37000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 37200: loss = 0.0034364720340818167\n",
            "step = 37400: loss = 0.001646755961701274\n",
            "step = 37500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 37600: loss = 0.0024078525602817535\n",
            "step = 37800: loss = 0.0036983145400881767\n",
            "step = 38000: loss = 0.00217838236130774\n",
            "step = 38000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 38200: loss = 0.0009578439639881253\n",
            "step = 38400: loss = 0.0015331665053963661\n",
            "step = 38500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 38600: loss = 0.0021530899684876204\n",
            "step = 38800: loss = 0.0014386208495125175\n",
            "step = 39000: loss = 0.0022176040802150965\n",
            "step = 39000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 39200: loss = 0.0007905725506134331\n",
            "step = 39400: loss = 0.0036385166458785534\n",
            "step = 39500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 39600: loss = 0.002854549093171954\n",
            "step = 39800: loss = 0.003922044299542904\n",
            "step = 40000: loss = 0.0014148488407954574\n",
            "step = 40000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 40200: loss = 0.003842501901090145\n",
            "step = 40400: loss = 0.005256340838968754\n",
            "step = 40500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 40600: loss = 0.0027060911525040865\n",
            "step = 40800: loss = 0.003357482608407736\n",
            "step = 41000: loss = 0.0020356106106191874\n",
            "step = 41000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 41200: loss = 0.05300624668598175\n",
            "step = 41400: loss = 0.05451066046953201\n",
            "step = 41500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 41600: loss = 0.004176286049187183\n",
            "step = 41800: loss = 0.0013482308713719249\n",
            "step = 42000: loss = 0.003185953013598919\n",
            "step = 42000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 42200: loss = 0.0014566973550245166\n",
            "step = 42400: loss = 0.0019412051187828183\n",
            "step = 42500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 42600: loss = 0.0013081177603453398\n",
            "step = 42800: loss = 0.003234125440940261\n",
            "step = 43000: loss = 0.0018847559113055468\n",
            "step = 43000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 43200: loss = 0.05291313678026199\n",
            "step = 43400: loss = 0.0018022562144324183\n",
            "step = 43500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 43600: loss = 0.0035873656161129475\n",
            "step = 43800: loss = 0.0013208562741056085\n",
            "step = 44000: loss = 0.0007817283621989191\n",
            "step = 44000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 44200: loss = 0.0028715236112475395\n",
            "step = 44400: loss = 0.001855853945016861\n",
            "step = 44500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 44600: loss = 0.0021849723998457193\n",
            "step = 44800: loss = 0.0016533175949007273\n",
            "step = 45000: loss = 0.005179158877581358\n",
            "step = 45000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 45200: loss = 0.001266168663278222\n",
            "step = 45400: loss = 0.0020655025728046894\n",
            "step = 45500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 45600: loss = 0.001938508590683341\n",
            "step = 45800: loss = 0.0014660954475402832\n",
            "step = 46000: loss = 0.0015037533594295382\n",
            "step = 46000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 46200: loss = 0.001110506127588451\n",
            "step = 46400: loss = 0.0019557341001927853\n",
            "step = 46500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 46600: loss = 0.00016753713134676218\n",
            "step = 46800: loss = 0.0010595428757369518\n",
            "step = 47000: loss = 0.0015065852785483003\n",
            "step = 47000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 47200: loss = 0.0011009585577994585\n",
            "step = 47400: loss = 0.004684956278651953\n",
            "step = 47500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 47600: loss = 0.001595949288457632\n",
            "step = 47800: loss = 0.0005846458952873945\n",
            "step = 48000: loss = 0.0018394030630588531\n",
            "step = 48000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 48200: loss = 0.0001225169689860195\n",
            "step = 48400: loss = 0.0019388573709875345\n",
            "step = 48500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 48600: loss = 0.0014870312297716737\n",
            "step = 48800: loss = 0.05507601425051689\n",
            "step = 49000: loss = 0.000548118376173079\n",
            "step = 49000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 49200: loss = 0.0001022077995003201\n",
            "step = 49400: loss = 0.001932604005560279\n",
            "step = 49500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 49600: loss = 0.0011213744292035699\n",
            "step = 49800: loss = 0.0005827994900755584\n",
            "step = 50000: loss = 0.0020663924515247345\n",
            "step = 50000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 50200: loss = 0.001982615329325199\n",
            "step = 50400: loss = 0.0007157254149205983\n",
            "step = 50500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 50600: loss = 0.0016806144267320633\n",
            "step = 50800: loss = 0.000681077130138874\n",
            "step = 51000: loss = 0.0007388151134364307\n",
            "step = 51000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 51200: loss = 0.0023215007968246937\n",
            "step = 51400: loss = 0.0029556702356785536\n",
            "step = 51500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 51600: loss = 0.0013722889125347137\n",
            "step = 51800: loss = 0.0011920440010726452\n",
            "step = 52000: loss = 0.004216503351926804\n",
            "step = 52000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 52200: loss = 0.0011555503588169813\n",
            "step = 52400: loss = 0.003780718194320798\n",
            "step = 52500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 52600: loss = 0.00221715634688735\n",
            "step = 52800: loss = 0.0031841788440942764\n",
            "step = 53000: loss = 0.0012134229764342308\n",
            "step = 53000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 53200: loss = 0.00321568688377738\n",
            "step = 53400: loss = 0.0013273436343297362\n",
            "step = 53500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 53600: loss = 0.0010440051555633545\n",
            "step = 53800: loss = 0.00120064290240407\n",
            "step = 54000: loss = 0.0012163528008386493\n",
            "step = 54000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 54200: loss = 0.001634847605600953\n",
            "step = 54400: loss = 0.0028564659878611565\n",
            "step = 54500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 54600: loss = 0.0014268661616370082\n",
            "step = 54800: loss = 0.052761681377887726\n",
            "step = 55000: loss = 0.00017374756862409413\n",
            "step = 55000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 55200: loss = 0.0024831786286085844\n",
            "step = 55400: loss = 0.0005870612221769989\n",
            "step = 55500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 55600: loss = 0.0016867853701114655\n",
            "step = 55800: loss = 0.0011863976251333952\n",
            "step = 56000: loss = 0.002259437460452318\n",
            "step = 56000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 56200: loss = 0.0012832095380872488\n",
            "step = 56400: loss = 0.002305174246430397\n",
            "step = 56500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 56600: loss = 0.0012241682270541787\n",
            "step = 56800: loss = 0.0012769160093739629\n",
            "step = 57000: loss = 0.003469101618975401\n",
            "step = 57000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 57200: loss = 0.001367434742860496\n",
            "step = 57400: loss = 0.004382168874144554\n",
            "step = 57500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 57600: loss = 0.0017912621842697263\n",
            "step = 57800: loss = 0.0028877449221909046\n",
            "step = 58000: loss = 0.004049392882734537\n",
            "step = 58000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 58200: loss = 0.0017699915915727615\n",
            "step = 58400: loss = 0.0012532779946923256\n",
            "step = 58500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 58600: loss = 0.0023796269670128822\n",
            "step = 58800: loss = 0.0012673638993874192\n",
            "step = 59000: loss = 0.002314501442015171\n",
            "step = 59000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 59200: loss = 0.002395791932940483\n",
            "step = 59400: loss = 0.05172784626483917\n",
            "step = 59500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 59600: loss = 0.0018762960098683834\n",
            "step = 59800: loss = 0.003188949776813388\n",
            "step = 60000: loss = 0.0017237892607226968\n",
            "step = 60000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 60200: loss = 0.002312618074938655\n",
            "step = 60400: loss = 0.00488812243565917\n",
            "step = 60500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 60600: loss = 0.0011719652684405446\n",
            "step = 60800: loss = 0.001734257908537984\n",
            "step = 61000: loss = 0.0018620339687913656\n",
            "step = 61000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 61200: loss = 0.001386459800414741\n",
            "step = 61400: loss = 0.051862191408872604\n",
            "step = 61500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 61600: loss = 0.0036389781162142754\n",
            "step = 61800: loss = 0.0032168575562536716\n",
            "step = 62000: loss = 0.003951840102672577\n",
            "step = 62000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 62200: loss = 0.0007242447463795543\n",
            "step = 62400: loss = 0.0019200851675122976\n",
            "step = 62500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 62600: loss = 0.0026416704058647156\n",
            "step = 62800: loss = 0.0017473900225013494\n",
            "step = 63000: loss = 0.0014361016219481826\n",
            "step = 63000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 63200: loss = 0.002419950207695365\n",
            "step = 63400: loss = 0.0012488870415836573\n",
            "step = 63500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 63600: loss = 0.0006576601881533861\n",
            "step = 63800: loss = 0.0017998021794483066\n",
            "step = 64000: loss = 0.0020232212264090776\n",
            "step = 64000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 64200: loss = 0.0007154345512390137\n",
            "step = 64400: loss = 0.0017672746907919645\n",
            "step = 64500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 64600: loss = 0.001804848201572895\n",
            "step = 64800: loss = 0.0017715709982439876\n",
            "step = 65000: loss = 0.0022464985959231853\n",
            "step = 65000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 65200: loss = 0.0017766633536666632\n",
            "step = 65400: loss = 0.00014872981410007924\n",
            "step = 65500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 65600: loss = 0.0022823193576186895\n",
            "step = 65800: loss = 0.0006426451145671308\n",
            "step = 66000: loss = 0.003606512676924467\n",
            "step = 66000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 66200: loss = 0.0007949451683089137\n",
            "step = 66400: loss = 0.0019757654517889023\n",
            "step = 66500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 66600: loss = 0.0028929098043590784\n",
            "step = 66800: loss = 0.05309215188026428\n",
            "step = 67000: loss = 0.001410300494171679\n",
            "step = 67000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 67200: loss = 0.053503841161727905\n",
            "step = 67400: loss = 0.0015790294855833054\n",
            "step = 67500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 67600: loss = 0.0028603696264326572\n",
            "step = 67800: loss = 0.002130727516487241\n",
            "step = 68000: loss = 0.0029139756225049496\n",
            "step = 68000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 68200: loss = 0.0007635694346390665\n",
            "step = 68400: loss = 0.0045327721163630486\n",
            "step = 68500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 68600: loss = 0.0020373968873173\n",
            "step = 68800: loss = 0.0025938646867871284\n",
            "step = 69000: loss = 0.0006917326827533543\n",
            "step = 69000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 69200: loss = 0.0018313543405383825\n",
            "step = 69400: loss = 0.003980750218033791\n",
            "step = 69500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 69600: loss = 0.0013032930437475443\n",
            "step = 69800: loss = 0.001864906749688089\n",
            "step = 70000: loss = 0.0014218712458387017\n",
            "step = 70000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 70200: loss = 0.0023248307406902313\n",
            "step = 70400: loss = 0.002002893015742302\n",
            "step = 70500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 70600: loss = 0.0006804623990319669\n",
            "step = 70800: loss = 0.0013791662640869617\n",
            "step = 71000: loss = 0.003356886561959982\n",
            "step = 71000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 71200: loss = 0.0006495323032140732\n",
            "step = 71400: loss = 0.0017172947991639376\n",
            "step = 71500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 71600: loss = 0.005863759201020002\n",
            "step = 71800: loss = 0.0001647443714318797\n",
            "step = 72000: loss = 0.0026936130598187447\n",
            "step = 72000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 72200: loss = 0.05356481671333313\n",
            "step = 72400: loss = 0.0014997420366853476\n",
            "step = 72500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 72600: loss = 0.0012091925600543618\n",
            "step = 72800: loss = 0.006080673076212406\n",
            "step = 73000: loss = 0.0031205355189740658\n",
            "step = 73000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 73200: loss = 0.003210505936294794\n",
            "step = 73400: loss = 0.0021079243160784245\n",
            "step = 73500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 73600: loss = 0.0011593486415222287\n",
            "step = 73800: loss = 0.002219622954726219\n",
            "step = 74000: loss = 0.0011747439857572317\n",
            "step = 74000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 74200: loss = 0.002136912429705262\n",
            "step = 74400: loss = 0.002450095722451806\n",
            "step = 74500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 74600: loss = 0.0016803377075120807\n",
            "step = 74800: loss = 0.0033474108204245567\n",
            "step = 75000: loss = 0.0007342986646108329\n",
            "step = 75000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 75200: loss = 0.0025617284700274467\n",
            "step = 75400: loss = 0.001876485999673605\n",
            "step = 75500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 75600: loss = 0.00270463852211833\n",
            "step = 75800: loss = 0.0032616970129311085\n",
            "step = 76000: loss = 0.0007423398783430457\n",
            "step = 76000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 76200: loss = 0.0037396380212157965\n",
            "step = 76400: loss = 0.0019754855893552303\n",
            "step = 76500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 76600: loss = 0.05163223296403885\n",
            "step = 76800: loss = 0.0013100715586915612\n",
            "step = 77000: loss = 0.0006710243178531528\n",
            "step = 77000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 77200: loss = 0.0007896513561718166\n",
            "step = 77400: loss = 0.003077598288655281\n",
            "step = 77500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 77600: loss = 0.0019899571780115366\n",
            "step = 77800: loss = 0.0007560780504718423\n",
            "step = 78000: loss = 0.0023379256017506123\n",
            "step = 78000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 78200: loss = 0.0023632727097719908\n",
            "step = 78400: loss = 0.0013493755832314491\n",
            "step = 78500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 78600: loss = 0.0013136930065229535\n",
            "step = 78800: loss = 0.0014332010177895427\n",
            "step = 79000: loss = 0.0007307631894946098\n",
            "step = 79000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 79200: loss = 0.002957304008305073\n",
            "step = 79400: loss = 0.002787955803796649\n",
            "step = 79500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 79600: loss = 0.00123748485930264\n",
            "step = 79800: loss = 0.0017204729374498129\n",
            "step = 80000: loss = 0.0024783227127045393\n",
            "step = 80000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 80200: loss = 0.002564786933362484\n",
            "step = 80400: loss = 0.05177604407072067\n",
            "step = 80500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 80600: loss = 0.0007255907403305173\n",
            "step = 80800: loss = 0.004198293201625347\n",
            "step = 81000: loss = 0.0017987568862736225\n",
            "step = 81000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 81200: loss = 0.0033633848652243614\n",
            "step = 81400: loss = 0.05390571057796478\n",
            "step = 81500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 81600: loss = 0.0017802307847887278\n",
            "step = 81800: loss = 0.0006756853545084596\n",
            "step = 82000: loss = 0.0017642180901020765\n",
            "step = 82000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 82200: loss = 0.00303539982996881\n",
            "step = 82400: loss = 0.0007455862360075116\n",
            "step = 82500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 82600: loss = 0.0022320267744362354\n",
            "step = 82800: loss = 0.0017743378411978483\n",
            "step = 83000: loss = 0.0008281793561764061\n",
            "step = 83000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 83200: loss = 0.0018201412167400122\n",
            "step = 83400: loss = 0.002930935937911272\n",
            "step = 83500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 83600: loss = 0.0029211074579507113\n",
            "step = 83800: loss = 0.001667903969064355\n",
            "step = 84000: loss = 0.0006436624098569155\n",
            "step = 84000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 84200: loss = 0.0019723293371498585\n",
            "step = 84400: loss = 0.0023540675174444914\n",
            "step = 84500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 84600: loss = 0.0017996151000261307\n",
            "step = 84800: loss = 0.0007644557626917958\n",
            "step = 85000: loss = 0.0027838870882987976\n",
            "step = 85000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 85200: loss = 0.005226992070674896\n",
            "step = 85400: loss = 0.002277309074997902\n",
            "step = 85500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 85600: loss = 0.003336310386657715\n",
            "step = 85800: loss = 0.001709433039650321\n",
            "step = 86000: loss = 0.0033990321680903435\n",
            "step = 86000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 86200: loss = 0.0026918037328869104\n",
            "step = 86400: loss = 0.001992165809497237\n",
            "step = 86500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 86600: loss = 0.001893072621896863\n",
            "step = 86800: loss = 0.00015124137280508876\n",
            "step = 87000: loss = 0.001949878642335534\n",
            "step = 87000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 87200: loss = 0.05525820702314377\n",
            "step = 87400: loss = 0.0014198588905856013\n",
            "step = 87500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 87600: loss = 0.0025719930417835712\n",
            "step = 87800: loss = 0.001913011888973415\n",
            "step = 88000: loss = 0.0019161670934408903\n",
            "step = 88000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 88200: loss = 0.0005917096859775484\n",
            "step = 88400: loss = 0.0027474267408251762\n",
            "step = 88500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 88600: loss = 0.0025929915718734264\n",
            "step = 88800: loss = 0.002503358293324709\n",
            "step = 89000: loss = 0.0005611286032944918\n",
            "step = 89000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 89200: loss = 0.0033117609564214945\n",
            "step = 89400: loss = 0.0001527056738268584\n",
            "step = 89500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 89600: loss = 0.0024870343040674925\n",
            "step = 89800: loss = 0.0015360022662207484\n",
            "step = 90000: loss = 0.001157807419076562\n",
            "step = 90000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 90200: loss = 0.0015814164653420448\n",
            "step = 90400: loss = 0.0016333688981831074\n",
            "step = 90500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 90600: loss = 0.0035618823021650314\n",
            "step = 90800: loss = 0.0023285895586013794\n",
            "step = 91000: loss = 0.0014969357289373875\n",
            "step = 91000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 91200: loss = 0.00047371943946927786\n",
            "step = 91400: loss = 0.002300358610227704\n",
            "step = 91500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 91600: loss = 0.00011234365229029208\n",
            "step = 91800: loss = 0.0006015708786435425\n",
            "step = 92000: loss = 0.001835451228544116\n",
            "step = 92000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 92200: loss = 0.0010500140488147736\n",
            "step = 92400: loss = 0.001220462960191071\n",
            "step = 92500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 92600: loss = 0.0010698091937229037\n",
            "step = 92800: loss = 0.0016710290219634771\n",
            "step = 93000: loss = 0.0015249855350703\n",
            "step = 93000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 93200: loss = 0.0005934736109338701\n",
            "step = 93400: loss = 0.001117657171562314\n",
            "step = 93500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 93600: loss = 0.0019721926655620337\n",
            "step = 93800: loss = 0.0021253954619169235\n",
            "step = 94000: loss = 0.0037839857395738363\n",
            "step = 94000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 94200: loss = 0.0020589367486536503\n",
            "step = 94400: loss = 0.05295472592115402\n",
            "step = 94500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 94600: loss = 0.000660769990645349\n",
            "step = 94800: loss = 0.05416880548000336\n",
            "step = 95000: loss = 0.05439218506217003\n",
            "step = 95000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 95200: loss = 0.0021620718762278557\n",
            "step = 95400: loss = 0.0026198686100542545\n",
            "step = 95500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 95600: loss = 0.0015122268814593554\n",
            "step = 95800: loss = 0.0019172297324985266\n",
            "step = 96000: loss = 0.0025253447238355875\n",
            "step = 96000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 96200: loss = 0.0011709246318787336\n",
            "step = 96400: loss = 0.0027004049625247717\n",
            "step = 96500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 96600: loss = 0.003650153521448374\n",
            "step = 96800: loss = 0.002606651047244668\n",
            "step = 97000: loss = 0.003234598785638809\n",
            "step = 97000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 97200: loss = 0.002250566380098462\n",
            "step = 97400: loss = 0.0017860938096418977\n",
            "step = 97500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 97600: loss = 0.0013725594617426395\n",
            "step = 97800: loss = 0.053241755813360214\n",
            "step = 98000: loss = 0.0005029071471653879\n",
            "step = 98000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 98200: loss = 0.0014069346943870187\n",
            "step = 98400: loss = 0.000557739520445466\n",
            "step = 98500: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 98600: loss = 0.0009314363123849034\n",
            "step = 98800: loss = 0.001660267123952508\n",
            "step = 99000: loss = 0.0014648627256974578\n",
            "step = 99000: Average Return = -1.0\n",
            "no_improvement_steps:  0\n",
            "step = 99200: loss = 0.0008427027496509254\n",
            "step = 99400: loss = 0.0006009757053107023\n",
            "step = 99500: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n",
            "step = 99600: loss = 0.0013414241839200258\n",
            "step = 99800: loss = 0.0009111528634093702\n",
            "step = 100000: loss = 0.0013185897842049599\n",
            "step = 100000: Average Return = -0.9800000190734863\n",
            "no_improvement_steps:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  saved_policy = tf.saved_model.load(policy_dir)\n",
        "  avg_return = compute_avg_return(eval_env, saved_policy, 1000)\n",
        "  print(avg_return)\n",
        "  avg_return = compute_avg_return(train_env, saved_policy, 1000)\n",
        "  print(avg_return)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "D9He1Uj9s6ur"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = range(0, iter_count + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "#plt.ylim(top=250)"
      ],
      "metadata": {
        "id": "WIVpHQGzeTqj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "2597aa6e-01c0-4643-d7f7-c3293cac7d98"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Iterations')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import csv\n",
        "\n",
        "with open('10b_returns.csv', 'w', newline='') as file:\n",
        "  writer = csv.writer(file)\n",
        "  writer.writerow(list(iterations))\n",
        "  writer.writerow(returns)\n",
        "\n",
        "files.download('10b_returns.csv')"
      ],
      "metadata": {
        "id": "KNIuFszNF-PS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "bda5afad-2e14-4571-80be-982e2accaa10"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_13695721-7820-4d9c-9a17-ae2526fb566d\", \"10b_returns.csv\", 2253)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
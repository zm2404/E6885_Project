{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlI8iHgEP1wB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "996075c3-e3c1-4439-9704-7e794ebbf468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf_agents\n",
            "  Downloading tf_agents-0.19.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf_agents)\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf_agents) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (4.5.0)\n",
            "Collecting pygame==2.1.3 (from tf_agents)\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-probability~=0.23.0 (from tf_agents)\n",
            "  Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents) (0.0.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (0.5.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (0.1.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697631 sha256=d672334c60712a47bcd4744600c8b5f367350140439c1e0017958ba3206ee4d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
            "Successfully built gym\n",
            "Installing collected packages: tensorflow-probability, pygame, gym, tf_agents\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.22.0\n",
            "    Uninstalling tensorflow-probability-0.22.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.22.0\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.23.0 pygame-2.1.3 tensorflow-probability-0.23.0 tf_agents-0.19.0\n",
            "Requirement already satisfied: tf-agents[reverb] in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.23.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
            "Collecting rlds (from tf-agents[reverb])\n",
            "  Downloading rlds-0.1.8-py3-none-manylinux2010_x86_64.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m732.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dm-reverb~=0.14.0 (from tf-agents[reverb])\n",
            "  Downloading dm_reverb-0.14.0-cp310-cp310-manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow~=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.15.0->tf-agents[reverb]) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb~=0.14.0->tf-agents[reverb]) (5.9.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.2.2)\n",
            "Installing collected packages: rlds, dm-reverb\n",
            "Successfully installed dm-reverb-0.14.0 rlds-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install tf_agents\n",
        "!pip install tf-agents[reverb]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLWWFFWwP-Fk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tf_agents\n",
        "import reverb\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import epsilon_greedy_policy\n",
        "from tf_agents.policies import policy_saver\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/E6885_Project')\n",
        "import SortWaterEnv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.version.VERSION"
      ],
      "metadata": {
        "id": "S7-5FyITqOTK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78cdd800-93dd-4e58-c0b0-acf895f08234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meuuIXmuQLEW"
      },
      "outputs": [],
      "source": [
        "num_iterations = 100000        #\n",
        "\n",
        "initial_collect_steps = 100     #\n",
        "collect_steps_per_iteration = 10#2#1   #\n",
        "replay_buffer_max_length = 100000  #\n",
        "\n",
        "batch_size = 64            #\n",
        "learning_rate = 1e-4 #1e-3        #\n",
        "log_interval = 200          #\n",
        "\n",
        "num_eval_episodes = 100        #\n",
        "eval_interval = 200#500#1000        #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Be0fVlSQM1w"
      },
      "outputs": [],
      "source": [
        "############# create training and evaluation environment #############\n",
        "num_bottles = 5\n",
        "water_level = 4\n",
        "env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level, random_init=True)\n",
        "train_py_env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level, random_init=True)\n",
        "eval_py_env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level, random_init=True)\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSsS5sENQQa2"
      },
      "outputs": [],
      "source": [
        "############ create a DQN agent ############\n",
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jddUDIr2QSTO"
      },
      "outputs": [],
      "source": [
        "# Customized Q netword\n",
        "class MaskedQNetwork(q_network.QNetwork):\n",
        "  def __init__(self, input_tensor_spec, action_spec, fc_layer_params=(100,), **kwargs):\n",
        "    # 从 input_tensor_spec 元组中提取观察值规格\n",
        "    observation_spec = input_tensor_spec[0]\n",
        "\n",
        "    # 调用基类的构造函数以构建网络\n",
        "    super(MaskedQNetwork, self).__init__(observation_spec, action_spec, fc_layer_params=fc_layer_params, **kwargs)\n",
        "\n",
        "  def call(self, observation, step_type=None, network_state=(), training=False):\n",
        "    # 直接调用父类的 call 方法，处理观察值\n",
        "    return super(MaskedQNetwork, self).call(\n",
        "        observation, step_type, network_state, training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqi6-5F7RG-n"
      },
      "outputs": [],
      "source": [
        "observation_spec = train_env.observation_spec()\n",
        "action_spec = train_env.action_spec()\n",
        "\n",
        "# create Q-network\n",
        "q_net = MaskedQNetwork(\n",
        "    (observation_spec['observation'], observation_spec['action_mask']),\n",
        "    action_spec,\n",
        "    fc_layer_params=fc_layer_params\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMoFiViiC7NV"
      },
      "outputs": [],
      "source": [
        "def observation_and_action_constraint_splitter(obs):\n",
        "\treturn obs['observation'], obs['action_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qI_nu75uRRqe"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter,\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AhbI3ZfbFpm"
      },
      "outputs": [],
      "source": [
        "# an example, just to show what policies are used during evaluation and collecting\n",
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH_fIsadbICb"
      },
      "outputs": [],
      "source": [
        "# an example, just show how to create epsilon_greedy_policy. Not to be used\n",
        "base_policy = agent.policy\n",
        "epsilon = 0.1  # 例如，使用 0.1 作为 epsilon 值\n",
        "epsilon_greedy_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(base_policy, epsilon=epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9kV-ZLvbJVC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d8ae22d-577f-4024-dbe9-1c832aabb76d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TimeStep(\n",
            "{'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observation': <tf.Tensor: shape=(1, 5, 4), dtype=int32, numpy=\n",
            "array([[[1, 3, 2, 3],\n",
            "        [2, 2, 3, 2],\n",
            "        [0, 0, 0, 0],\n",
            "        [1, 1, 3, 1],\n",
            "        [0, 0, 0, 0]]], dtype=int32)>,\n",
            "                 'action_mask': <tf.Tensor: shape=(1, 20), dtype=bool, numpy=\n",
            "array([[False,  True, False,  True, False,  True, False,  True, False,\n",
            "        False, False, False, False, False,  True,  True, False, False,\n",
            "        False, False]])>}})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# example of greedy policy choosing action\n",
        "example_py_env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level, random_init=True)\n",
        "example_env = tf_py_environment.TFPyEnvironment(example_py_env)\n",
        "time_step = example_env.reset()\n",
        "print(time_step)\n",
        "epsilon_greedy_policy.action(time_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0MKdny2bK-Q"
      },
      "outputs": [],
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuqXjbWrbMQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92945107-df6e-464f-c6fa-ce524bf91648"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.94"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# an example\n",
        "compute_avg_return(eval_env, epsilon_greedy_policy, num_eval_episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQzc5_e4bP53"
      },
      "outputs": [],
      "source": [
        "################# Replay buffer #################\n",
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XbLZjCtb__-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76633fec-3e06-49cf-fed7-7f3caa0127aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_TupleWrapper(Trajectory(\n",
              "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'observation': DictWrapper({'observation': BoundedTensorSpec(shape=(5, 4), dtype=tf.int32, name='observation', minimum=array(0, dtype=int32), maximum=array(3, dtype=int32)), 'action_mask': TensorSpec(shape=(20,), dtype=tf.bool, name='action_mask')}),\n",
              " 'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(19, dtype=int32)),\n",
              " 'policy_info': (),\n",
              " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
              " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))}))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "agent.collect_data_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeCbnlphcesk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cf9de99-3488-4157-e647-ce57e8a57fff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('step_type',\n",
              " 'observation',\n",
              " 'action',\n",
              " 'policy_info',\n",
              " 'next_step_type',\n",
              " 'reward',\n",
              " 'discount')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "agent.collect_data_spec._fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3Hv74IhckJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6872b41c-3cc1-404b-9633-a2e7a90a6979"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TimeStep(\n",
              " {'step_type': array(1, dtype=int32),\n",
              "  'reward': array(0., dtype=float32),\n",
              "  'discount': array(1., dtype=float32),\n",
              "  'observation': {'observation': array([[1, 1, 0, 0],\n",
              "        [2, 2, 3, 0],\n",
              "        [3, 3, 1, 2],\n",
              "        [1, 3, 2, 0],\n",
              "        [0, 0, 0, 0]], dtype=int32),\n",
              "                  'action_mask': array([False, False, False,  True, False, False, False,  True, False,\n",
              "        False,  True,  True, False, False, False,  True, False, False,\n",
              "        False, False])}}),\n",
              " ())"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# an example.\n",
        "py_driver.PyDriver(\n",
        "    train_py_env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      epsilon_greedy_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(train_py_env.reset())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.get_state()"
      ],
      "metadata": {
        "id": "ohhyBZROzNh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "192af7f8-e542-48f6-a655-0236e63a90b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'observation': array([[1, 1, 3, 2],\n",
              "        [0, 0, 0, 0],\n",
              "        [3, 3, 3, 2],\n",
              "        [1, 2, 1, 2],\n",
              "        [0, 0, 0, 0]], dtype=int32),\n",
              " 'action_mask': array([ True, False, False,  True, False, False, False, False, False,\n",
              "         True, False,  True, False,  True, False,  True, False, False,\n",
              "        False, False])}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh7SqXgkdYNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38bf09a-276c-42b0-c73a-98870b3c4f05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(Trajectory(\n",
              "{'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'observation': DictWrapper({'observation': TensorSpec(shape=(64, 2, 5, 4), dtype=tf.int32, name=None), 'action_mask': TensorSpec(shape=(64, 2, 20), dtype=tf.bool, name=None)}),\n",
              " 'action': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'policy_info': (),\n",
              " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
              " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None)}), SampleInfo(key=TensorSpec(shape=(64, 2), dtype=tf.uint64, name=None), probability=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), table_size=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), priority=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), times_sampled=TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)))>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mp0I59Vwd11M"
      },
      "outputs": [],
      "source": [
        "iterator = iter(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqBPd98nu52O"
      },
      "outputs": [],
      "source": [
        "# demo: only 1 episode\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = train_py_env.reset()\n",
        "\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    train_py_env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "######### early stopping ##########\n",
        "# set save direction\n",
        "tempdir = os.getenv(\"TEST_TMPDIR\", tempfile.gettempdir())\n",
        "# checkpointer\n",
        "# checkpoint_dir = os.path.join(tempdir, 'checkpoint')\n",
        "# train_checkpointer = common.Checkpointer(\n",
        "#     ckpt_dir=checkpoint_dir,\n",
        "#     max_to_keep=1,\n",
        "#     agent=agent,\n",
        "#     policy=agent.policy,\n",
        "#     replay_buffer=replay_buffer,\n",
        "#     global_step=train_step_counter\n",
        "# )\n",
        "# policy saver\n",
        "policy_dir = os.path.join(tempdir, 'policy')\n",
        "tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
        "\n",
        "# some threshold\n",
        "#best_avg_return = -float('inf')\n",
        "best_avg_return = 0.7\n",
        "no_improvement_steps = 0\n",
        "early_stopping_threshold = 10\n",
        "earlystop = False\n",
        "iter_thres = 10000\n",
        "######### early stopping ##########\n",
        "\n",
        "\n",
        "iter_count = 0\n",
        "for _ in range(num_iterations):\n",
        "  iter_count += 1\n",
        "  #print('iter_count: ',iter_count)\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)\n",
        "\n",
        "  ######### early stopping ##########\n",
        "    if avg_return > best_avg_return:\n",
        "      earlystop = True\n",
        "    if (avg_return > best_avg_return) and earlystop and (iter_count > iter_thres):\n",
        "      best_avg_return = avg_return\n",
        "      no_improvement_steps = 0\n",
        "      tf_policy_saver.save(policy_dir)  # save policy\n",
        "    elif earlystop and (iter_count > iter_thres):\n",
        "      no_improvement_steps += 0\n",
        "    print(\"no_improvement_steps: \",no_improvement_steps)\n",
        "  if no_improvement_steps >= early_stopping_threshold:\n",
        "    print(\"No improvement for {0} steps. Early stopping...\".format(no_improvement_steps))\n",
        "    break\n",
        "  ######### early stopping ##########"
      ],
      "metadata": {
        "id": "35k0m3HtqQyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc8154f1-65aa-4baa-e964-114ad91c4126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 200: loss = 0.045661427080631256\n",
            "step = 200: Average Return = -0.36000001430511475\n",
            "no_improvement_steps:  0\n",
            "step = 400: loss = 0.0876261293888092\n",
            "step = 400: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 600: loss = 0.03508215770125389\n",
            "step = 600: Average Return = 0.2199999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 800: loss = 0.0756940171122551\n",
            "step = 800: Average Return = 0.2800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 1000: loss = 0.10796326398849487\n",
            "step = 1000: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 1200: loss = 0.17479318380355835\n",
            "step = 1200: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 1400: loss = 0.045765046030282974\n",
            "step = 1400: Average Return = 0.25999999046325684\n",
            "no_improvement_steps:  0\n",
            "step = 1600: loss = 0.0937848836183548\n",
            "step = 1600: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 1800: loss = 0.12716250121593475\n",
            "step = 1800: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 2000: loss = 0.09597884863615036\n",
            "step = 2000: Average Return = 0.25999999046325684\n",
            "no_improvement_steps:  0\n",
            "step = 2200: loss = 0.11879433691501617\n",
            "step = 2200: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 2400: loss = 0.22990912199020386\n",
            "step = 2400: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 2600: loss = 0.053084034472703934\n",
            "step = 2600: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 2800: loss = 0.11436723917722702\n",
            "step = 2800: Average Return = 0.3799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 3000: loss = 0.07861588895320892\n",
            "step = 3000: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 3200: loss = 0.09788990765810013\n",
            "step = 3200: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 3400: loss = 0.041816189885139465\n",
            "step = 3400: Average Return = 0.4399999976158142\n",
            "no_improvement_steps:  0\n",
            "step = 3600: loss = 0.08871504664421082\n",
            "step = 3600: Average Return = 0.41999998688697815\n",
            "no_improvement_steps:  0\n",
            "step = 3800: loss = 0.1475147306919098\n",
            "step = 3800: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 4000: loss = 0.03981098160147667\n",
            "step = 4000: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 4200: loss = 0.19088441133499146\n",
            "step = 4200: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 4400: loss = 0.047559574246406555\n",
            "step = 4400: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 4600: loss = 0.11756422370672226\n",
            "step = 4600: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 4800: loss = 0.04841994866728783\n",
            "step = 4800: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 5000: loss = 0.11205976456403732\n",
            "step = 5000: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 5200: loss = 0.13900625705718994\n",
            "step = 5200: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 5400: loss = 0.06001650169491768\n",
            "step = 5400: Average Return = 0.30000001192092896\n",
            "no_improvement_steps:  0\n",
            "step = 5600: loss = 0.06464288383722305\n",
            "step = 5600: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 5800: loss = 0.12923911213874817\n",
            "step = 5800: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 6000: loss = 0.011403674259781837\n",
            "step = 6000: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 6200: loss = 0.04708484932780266\n",
            "step = 6200: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 6400: loss = 0.07625715434551239\n",
            "step = 6400: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 6600: loss = 0.008737335912883282\n",
            "step = 6600: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 6800: loss = 0.09252314269542694\n",
            "step = 6800: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 7000: loss = 0.13580666482448578\n",
            "step = 7000: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 7200: loss = 0.15511606633663177\n",
            "step = 7200: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 7400: loss = 0.046360746026039124\n",
            "step = 7400: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 7600: loss = 0.09200901538133621\n",
            "step = 7600: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 7800: loss = 0.016324477270245552\n",
            "step = 7800: Average Return = 0.3799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 8000: loss = 0.01633264869451523\n",
            "step = 8000: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 8200: loss = 0.08694913983345032\n",
            "step = 8200: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 8400: loss = 0.11816202104091644\n",
            "step = 8400: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 8600: loss = 0.053166404366493225\n",
            "step = 8600: Average Return = 0.4000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 8800: loss = 0.08734497427940369\n",
            "step = 8800: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 9000: loss = 0.17962223291397095\n",
            "step = 9000: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 9200: loss = 0.09528283774852753\n",
            "step = 9200: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 9400: loss = 0.09472540020942688\n",
            "step = 9400: Average Return = 0.36000001430511475\n",
            "no_improvement_steps:  0\n",
            "step = 9600: loss = 0.052488118410110474\n",
            "step = 9600: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 9800: loss = 0.05118434503674507\n",
            "step = 9800: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 10000: loss = 0.010789165273308754\n",
            "step = 10000: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 10200: loss = 0.01437379140406847\n",
            "step = 10200: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 10400: loss = 0.009136954322457314\n",
            "step = 10400: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 10600: loss = 0.09318991005420685\n",
            "step = 10600: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 10800: loss = 0.046898990869522095\n",
            "step = 10800: Average Return = 0.4000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 11000: loss = 0.016043858602643013\n",
            "step = 11000: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 11200: loss = 0.045610517263412476\n",
            "step = 11200: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 11400: loss = 0.045763254165649414\n",
            "step = 11400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 11600: loss = 0.14429008960723877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
            "WARNING:absl:`0/reward` is not a valid tf.function parameter name. Sanitizing to `arg_0_reward`.\n",
            "WARNING:absl:`0/discount` is not a valid tf.function parameter name. Sanitizing to `arg_0_discount`.\n",
            "WARNING:absl:`0/observation/action_mask` is not a valid tf.function parameter name. Sanitizing to `arg_0_observation_action_mask`.\n",
            "WARNING:absl:`0/observation/observation` is not a valid tf.function parameter name. Sanitizing to `arg_0_observation_observation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 11600: Average Return = 0.7400000095367432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no_improvement_steps:  0\n",
            "step = 11800: loss = 0.16110828518867493\n",
            "step = 11800: Average Return = 0.4000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 12000: loss = 0.05171647295355797\n",
            "step = 12000: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 12200: loss = 0.04867056384682655\n",
            "step = 12200: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 12400: loss = 0.13216686248779297\n",
            "step = 12400: Average Return = 0.36000001430511475\n",
            "no_improvement_steps:  0\n",
            "step = 12600: loss = 0.083018958568573\n",
            "step = 12600: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 12800: loss = 0.010100247338414192\n",
            "step = 12800: Average Return = 0.4000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 13000: loss = 0.0059566255658864975\n",
            "step = 13000: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 13200: loss = 0.10052758455276489\n",
            "step = 13200: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 13400: loss = 0.047886595129966736\n",
            "step = 13400: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 13600: loss = 0.048118073493242264\n",
            "step = 13600: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 13800: loss = 0.010956520214676857\n",
            "step = 13800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 14000: loss = 0.013047652319073677\n",
            "step = 14000: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 14200: loss = 0.06264310330152512\n",
            "step = 14200: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 14400: loss = 0.04967600479722023\n",
            "step = 14400: Average Return = 0.41999998688697815\n",
            "no_improvement_steps:  0\n",
            "step = 14600: loss = 0.09172497689723969\n",
            "step = 14600: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 14800: loss = 0.04604458063840866\n",
            "step = 14800: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 15000: loss = 0.015463152900338173\n",
            "step = 15000: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 15200: loss = 0.052866704761981964\n",
            "step = 15200: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 15400: loss = 0.008115097880363464\n",
            "step = 15400: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 15600: loss = 0.06110937520861626\n",
            "step = 15600: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 15800: loss = 0.2692323327064514\n",
            "step = 15800: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 16000: loss = 0.010217074304819107\n",
            "step = 16000: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 16200: loss = 0.009937608614563942\n",
            "step = 16200: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 16400: loss = 0.09512719511985779\n",
            "step = 16400: Average Return = 0.7599999904632568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no_improvement_steps:  0\n",
            "step = 16600: loss = 0.044211164116859436\n",
            "step = 16600: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 16800: loss = 0.10420551896095276\n",
            "step = 16800: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 17000: loss = 0.0459790863096714\n",
            "step = 17000: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 17200: loss = 0.04865546524524689\n",
            "step = 17200: Average Return = 0.4000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 17400: loss = 0.11941425502300262\n",
            "step = 17400: Average Return = 0.8399999737739563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no_improvement_steps:  0\n",
            "step = 17600: loss = 0.006334109231829643\n",
            "step = 17600: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 17800: loss = 0.0509752482175827\n",
            "step = 17800: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 18000: loss = 0.12874048948287964\n",
            "step = 18000: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 18200: loss = 0.11367420852184296\n",
            "step = 18200: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 18400: loss = 0.04442790895700455\n",
            "step = 18400: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 18600: loss = 0.1341037005186081\n",
            "step = 18600: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 18800: loss = 0.051868222653865814\n",
            "step = 18800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 19000: loss = 0.13899371027946472\n",
            "step = 19000: Average Return = 0.36000001430511475\n",
            "no_improvement_steps:  0\n",
            "step = 19200: loss = 0.046342603862285614\n",
            "step = 19200: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 19400: loss = 0.09826625883579254\n",
            "step = 19400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 19600: loss = 0.005158195272088051\n",
            "step = 19600: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 19800: loss = 0.008612517267465591\n",
            "step = 19800: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 20000: loss = 0.010248932987451553\n",
            "step = 20000: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 20200: loss = 0.09237201511859894\n",
            "step = 20200: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 20400: loss = 0.09219437837600708\n",
            "step = 20400: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 20600: loss = 0.007173317484557629\n",
            "step = 20600: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 20800: loss = 0.009921341203153133\n",
            "step = 20800: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 21000: loss = 0.01065339520573616\n",
            "step = 21000: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 21200: loss = 0.08010143786668777\n",
            "step = 21200: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 21400: loss = 0.007246108725667\n",
            "step = 21400: Average Return = 0.4000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 21600: loss = 0.006968568079173565\n",
            "step = 21600: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 21800: loss = 0.10126348584890366\n",
            "step = 21800: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 22000: loss = 0.058579158037900925\n",
            "step = 22000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 22200: loss = 0.09325075149536133\n",
            "step = 22200: Average Return = 0.3799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 22400: loss = 0.10027696192264557\n",
            "step = 22400: Average Return = 0.4399999976158142\n",
            "no_improvement_steps:  0\n",
            "step = 22600: loss = 0.1018044501543045\n",
            "step = 22600: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 22800: loss = 0.1910015046596527\n",
            "step = 22800: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 23000: loss = 0.006884113885462284\n",
            "step = 23000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 23200: loss = 0.014281503856182098\n",
            "step = 23200: Average Return = 0.3799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 23400: loss = 0.104695625603199\n",
            "step = 23400: Average Return = 0.2199999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 23600: loss = 0.046765364706516266\n",
            "step = 23600: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 23800: loss = 0.0850667804479599\n",
            "step = 23800: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 24000: loss = 0.0492420457303524\n",
            "step = 24000: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 24200: loss = 0.0927172303199768\n",
            "step = 24200: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 24400: loss = 0.05667882040143013\n",
            "step = 24400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 24600: loss = 0.05037888512015343\n",
            "step = 24600: Average Return = 0.4000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 24800: loss = 0.14475283026695251\n",
            "step = 24800: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 25000: loss = 0.044538166373968124\n",
            "step = 25000: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 25200: loss = 0.009694382548332214\n",
            "step = 25200: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 25400: loss = 0.12699265778064728\n",
            "step = 25400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 25600: loss = 0.09602659195661545\n",
            "step = 25600: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 25800: loss = 0.05335410684347153\n",
            "step = 25800: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 26000: loss = 0.006653217598795891\n",
            "step = 26000: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 26200: loss = 0.006672138348221779\n",
            "step = 26200: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 26400: loss = 0.006347611080855131\n",
            "step = 26400: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 26600: loss = 0.10136929154396057\n",
            "step = 26600: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 26800: loss = 0.006174175068736076\n",
            "step = 26800: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 27000: loss = 0.007555275224149227\n",
            "step = 27000: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 27200: loss = 0.09697400033473969\n",
            "step = 27200: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 27400: loss = 0.09428109228610992\n",
            "step = 27400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 27600: loss = 0.14285944402217865\n",
            "step = 27600: Average Return = 0.8799999952316284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no_improvement_steps:  0\n",
            "step = 27800: loss = 0.14072299003601074\n",
            "step = 27800: Average Return = 0.8799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 28000: loss = 0.005657223053276539\n",
            "step = 28000: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 28200: loss = 0.06589601933956146\n",
            "step = 28200: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 28400: loss = 0.06387290358543396\n",
            "step = 28400: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 28600: loss = 0.007881498895585537\n",
            "step = 28600: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 28800: loss = 0.14647798240184784\n",
            "step = 28800: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 29000: loss = 0.054757993668317795\n",
            "step = 29000: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 29200: loss = 0.05030292272567749\n",
            "step = 29200: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 29400: loss = 0.15052390098571777\n",
            "step = 29400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 29600: loss = 0.09455910325050354\n",
            "step = 29600: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 29800: loss = 0.0065774982795119286\n",
            "step = 29800: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 30000: loss = 0.13538719713687897\n",
            "step = 30000: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 30200: loss = 0.11133095622062683\n",
            "step = 30200: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 30400: loss = 0.10017521679401398\n",
            "step = 30400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 30600: loss = 0.1312505304813385\n",
            "step = 30600: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 30800: loss = 0.14177946746349335\n",
            "step = 30800: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 31000: loss = 0.05329989269375801\n",
            "step = 31000: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 31200: loss = 0.0526498481631279\n",
            "step = 31200: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 31400: loss = 0.14717072248458862\n",
            "step = 31400: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 31600: loss = 0.0485941544175148\n",
            "step = 31600: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 31800: loss = 0.005628499668091536\n",
            "step = 31800: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 32000: loss = 0.006803847849369049\n",
            "step = 32000: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 32200: loss = 0.005446786992251873\n",
            "step = 32200: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 32400: loss = 0.00579376146197319\n",
            "step = 32400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 32600: loss = 0.10001645982265472\n",
            "step = 32600: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 32800: loss = 0.004609070718288422\n",
            "step = 32800: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 33000: loss = 0.1030036062002182\n",
            "step = 33000: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 33200: loss = 0.053206171840429306\n",
            "step = 33200: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 33400: loss = 0.0521341934800148\n",
            "step = 33400: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 33600: loss = 0.09999324381351471\n",
            "step = 33600: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 33800: loss = 0.10234624147415161\n",
            "step = 33800: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 34000: loss = 0.09012278914451599\n",
            "step = 34000: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 34200: loss = 0.004493231885135174\n",
            "step = 34200: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 34400: loss = 0.045835770666599274\n",
            "step = 34400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 34600: loss = 0.1471046656370163\n",
            "step = 34600: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 34800: loss = 0.09516048431396484\n",
            "step = 34800: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 35000: loss = 0.04586712643504143\n",
            "step = 35000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 35200: loss = 0.06259141117334366\n",
            "step = 35200: Average Return = 0.41999998688697815\n",
            "no_improvement_steps:  0\n",
            "step = 35400: loss = 0.09507885575294495\n",
            "step = 35400: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 35600: loss = 0.04780762270092964\n",
            "step = 35600: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 35800: loss = 0.04423830285668373\n",
            "step = 35800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 36000: loss = 0.0494019091129303\n",
            "step = 36000: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 36200: loss = 0.006046703085303307\n",
            "step = 36200: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 36400: loss = 0.09921925514936447\n",
            "step = 36400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 36600: loss = 0.059312108904123306\n",
            "step = 36600: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 36800: loss = 0.14704746007919312\n",
            "step = 36800: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 37000: loss = 0.005019232630729675\n",
            "step = 37000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 37200: loss = 0.0062530129216611385\n",
            "step = 37200: Average Return = 0.41999998688697815\n",
            "no_improvement_steps:  0\n",
            "step = 37400: loss = 0.05039229616522789\n",
            "step = 37400: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 37600: loss = 0.05096267908811569\n",
            "step = 37600: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 37800: loss = 0.0044318875297904015\n",
            "step = 37800: Average Return = 0.4399999976158142\n",
            "no_improvement_steps:  0\n",
            "step = 38000: loss = 0.006334664765745401\n",
            "step = 38000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 38200: loss = 0.05484054237604141\n",
            "step = 38200: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 38400: loss = 0.006331787444651127\n",
            "step = 38400: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 38600: loss = 0.08771572262048721\n",
            "step = 38600: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 38800: loss = 0.006369282491505146\n",
            "step = 38800: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 39000: loss = 0.06075282394886017\n",
            "step = 39000: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 39200: loss = 0.15066953003406525\n",
            "step = 39200: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 39400: loss = 0.16854988038539886\n",
            "step = 39400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 39600: loss = 0.09558218717575073\n",
            "step = 39600: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 39800: loss = 0.08937671035528183\n",
            "step = 39800: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 40000: loss = 0.0629228875041008\n",
            "step = 40000: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 40200: loss = 0.003537979908287525\n",
            "step = 40200: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 40400: loss = 0.14489911496639252\n",
            "step = 40400: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 40600: loss = 0.10143532603979111\n",
            "step = 40600: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 40800: loss = 0.05305199325084686\n",
            "step = 40800: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 41000: loss = 0.004870637319982052\n",
            "step = 41000: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 41200: loss = 0.005161048844456673\n",
            "step = 41200: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 41400: loss = 0.004800258670002222\n",
            "step = 41400: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 41600: loss = 0.11077321320772171\n",
            "step = 41600: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 41800: loss = 0.05216857045888901\n",
            "step = 41800: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 42000: loss = 0.0468624010682106\n",
            "step = 42000: Average Return = 0.36000001430511475\n",
            "no_improvement_steps:  0\n",
            "step = 42200: loss = 0.10915164649486542\n",
            "step = 42200: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 42400: loss = 0.08799074590206146\n",
            "step = 42400: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 42600: loss = 0.010610532015562057\n",
            "step = 42600: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 42800: loss = 0.046091631054878235\n",
            "step = 42800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 43000: loss = 0.0405697263777256\n",
            "step = 43000: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 43200: loss = 0.09834285080432892\n",
            "step = 43200: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 43400: loss = 0.11368127167224884\n",
            "step = 43400: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 43600: loss = 0.0076072365045547485\n",
            "step = 43600: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 43800: loss = 0.052501484751701355\n",
            "step = 43800: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 44000: loss = 0.1418529748916626\n",
            "step = 44000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 44200: loss = 0.004907524678856134\n",
            "step = 44200: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 44400: loss = 0.005325982812792063\n",
            "step = 44400: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 44600: loss = 0.052721161395311356\n",
            "step = 44600: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 44800: loss = 0.1108604446053505\n",
            "step = 44800: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 45000: loss = 0.17329593002796173\n",
            "step = 45000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 45200: loss = 0.10659973323345184\n",
            "step = 45200: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 45400: loss = 0.056453924626111984\n",
            "step = 45400: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 45600: loss = 0.05205994099378586\n",
            "step = 45600: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 45800: loss = 0.05223516747355461\n",
            "step = 45800: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 46000: loss = 0.006396591663360596\n",
            "step = 46000: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 46200: loss = 0.09376399964094162\n",
            "step = 46200: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 46400: loss = 0.050282083451747894\n",
            "step = 46400: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 46600: loss = 0.10066395252943039\n",
            "step = 46600: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 46800: loss = 0.0991799458861351\n",
            "step = 46800: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 47000: loss = 0.005895144771784544\n",
            "step = 47000: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 47200: loss = 0.1400885283946991\n",
            "step = 47200: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 47400: loss = 0.1422160416841507\n",
            "step = 47400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 47600: loss = 0.15747904777526855\n",
            "step = 47600: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 47800: loss = 0.05950388312339783\n",
            "step = 47800: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 48000: loss = 0.0470893569290638\n",
            "step = 48000: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 48200: loss = 0.004705778323113918\n",
            "step = 48200: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 48400: loss = 0.11435499787330627\n",
            "step = 48400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 48600: loss = 0.10573678463697433\n",
            "step = 48600: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 48800: loss = 0.0036752368323504925\n",
            "step = 48800: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 49000: loss = 0.054579026997089386\n",
            "step = 49000: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 49200: loss = 0.09894296526908875\n",
            "step = 49200: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 49400: loss = 0.18919631838798523\n",
            "step = 49400: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 49600: loss = 0.05213680490851402\n",
            "step = 49600: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 49800: loss = 0.008251454681158066\n",
            "step = 49800: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 50000: loss = 0.048549920320510864\n",
            "step = 50000: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 50200: loss = 0.006614969111979008\n",
            "step = 50200: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 50400: loss = 0.0073897154070436954\n",
            "step = 50400: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 50600: loss = 0.04290833696722984\n",
            "step = 50600: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 50800: loss = 0.006690014619380236\n",
            "step = 50800: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 51000: loss = 0.05247510224580765\n",
            "step = 51000: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 51200: loss = 0.10346036404371262\n",
            "step = 51200: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 51400: loss = 0.1079193651676178\n",
            "step = 51400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 51600: loss = 0.10115990042686462\n",
            "step = 51600: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 51800: loss = 0.09291980415582657\n",
            "step = 51800: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 52000: loss = 0.097415491938591\n",
            "step = 52000: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 52200: loss = 0.010293279774487019\n",
            "step = 52200: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 52400: loss = 0.005704974755644798\n",
            "step = 52400: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 52600: loss = 0.10881255567073822\n",
            "step = 52600: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 52800: loss = 0.13817480206489563\n",
            "step = 52800: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 53000: loss = 0.007071901112794876\n",
            "step = 53000: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 53200: loss = 0.19088153541088104\n",
            "step = 53200: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 53400: loss = 0.004963752347975969\n",
            "step = 53400: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 53600: loss = 0.05116730183362961\n",
            "step = 53600: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 53800: loss = 0.2269762009382248\n",
            "step = 53800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 54000: loss = 0.008127506822347641\n",
            "step = 54000: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 54200: loss = 0.00502792838960886\n",
            "step = 54200: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 54400: loss = 0.008110228925943375\n",
            "step = 54400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 54600: loss = 0.10073380172252655\n",
            "step = 54600: Average Return = 0.8799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 54800: loss = 0.05106964334845543\n",
            "step = 54800: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 55000: loss = 0.15210753679275513\n",
            "step = 55000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 55200: loss = 0.043291252106428146\n",
            "step = 55200: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 55400: loss = 0.0051027690060436726\n",
            "step = 55400: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 55600: loss = 0.05918210372328758\n",
            "step = 55600: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 55800: loss = 0.004523640498518944\n",
            "step = 55800: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 56000: loss = 0.14317645132541656\n",
            "step = 56000: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 56200: loss = 0.007469682488590479\n",
            "step = 56200: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 56400: loss = 0.09990690648555756\n",
            "step = 56400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 56600: loss = 0.10771438479423523\n",
            "step = 56600: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 56800: loss = 0.052152279764413834\n",
            "step = 56800: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 57000: loss = 0.09116347134113312\n",
            "step = 57000: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 57200: loss = 0.1470375955104828\n",
            "step = 57200: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 57400: loss = 0.10296551883220673\n",
            "step = 57400: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 57600: loss = 0.15125197172164917\n",
            "step = 57600: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 57800: loss = 0.04642324894666672\n",
            "step = 57800: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 58000: loss = 0.04341502860188484\n",
            "step = 58000: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 58200: loss = 0.0599580816924572\n",
            "step = 58200: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 58400: loss = 0.007272476330399513\n",
            "step = 58400: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 58600: loss = 0.052748072892427444\n",
            "step = 58600: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 58800: loss = 0.00582178495824337\n",
            "step = 58800: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 59000: loss = 0.005494296550750732\n",
            "step = 59000: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 59200: loss = 0.053088605403900146\n",
            "step = 59200: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 59400: loss = 0.10928693413734436\n",
            "step = 59400: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 59600: loss = 0.0071150269359350204\n",
            "step = 59600: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 59800: loss = 0.0460943803191185\n",
            "step = 59800: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 60000: loss = 0.005870114546269178\n",
            "step = 60000: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 60200: loss = 0.0055932896211743355\n",
            "step = 60200: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 60400: loss = 0.05655694380402565\n",
            "step = 60400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 60600: loss = 0.048941731452941895\n",
            "step = 60600: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 60800: loss = 0.10672575235366821\n",
            "step = 60800: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 61000: loss = 0.09231621772050858\n",
            "step = 61000: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 61200: loss = 0.0995277389883995\n",
            "step = 61200: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 61400: loss = 0.04865768551826477\n",
            "step = 61400: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 61600: loss = 0.005870130844414234\n",
            "step = 61600: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 61800: loss = 0.006656818091869354\n",
            "step = 61800: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 62000: loss = 0.05629817396402359\n",
            "step = 62000: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 62200: loss = 0.006118546240031719\n",
            "step = 62200: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 62400: loss = 0.10572297871112823\n",
            "step = 62400: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 62600: loss = 0.15612317621707916\n",
            "step = 62600: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 62800: loss = 0.15279999375343323\n",
            "step = 62800: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 63000: loss = 0.006731193047016859\n",
            "step = 63000: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 63200: loss = 0.003881567157804966\n",
            "step = 63200: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 63400: loss = 0.10614092648029327\n",
            "step = 63400: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 63600: loss = 0.05036495625972748\n",
            "step = 63600: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 63800: loss = 0.09552963823080063\n",
            "step = 63800: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 64000: loss = 0.1450449377298355\n",
            "step = 64000: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 64200: loss = 0.06070849299430847\n",
            "step = 64200: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 64400: loss = 0.052992016077041626\n",
            "step = 64400: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 64600: loss = 0.1477743685245514\n",
            "step = 64600: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 64800: loss = 0.04533770680427551\n",
            "step = 64800: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 65000: loss = 0.004946517758071423\n",
            "step = 65000: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 65200: loss = 0.04316557198762894\n",
            "step = 65200: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 65400: loss = 0.006902044173330069\n",
            "step = 65400: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 65600: loss = 0.05150759965181351\n",
            "step = 65600: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 65800: loss = 0.009171495214104652\n",
            "step = 65800: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 66000: loss = 0.004771591629832983\n",
            "step = 66000: Average Return = 0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 66200: loss = 0.04504673555493355\n",
            "step = 66200: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 66400: loss = 0.05529174581170082\n",
            "step = 66400: Average Return = 0.5\n",
            "no_improvement_steps:  0\n",
            "step = 66600: loss = 0.052076488733291626\n",
            "step = 66600: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 66800: loss = 0.005628729239106178\n",
            "step = 66800: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 67000: loss = 0.004356278106570244\n",
            "step = 67000: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 67200: loss = 0.008213127963244915\n",
            "step = 67200: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 67400: loss = 0.1376817226409912\n",
            "step = 67400: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 67600: loss = 0.05421271547675133\n",
            "step = 67600: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 67800: loss = 0.003964569419622421\n",
            "step = 67800: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 68000: loss = 0.05575491115450859\n",
            "step = 68000: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 68200: loss = 0.14236733317375183\n",
            "step = 68200: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 68400: loss = 0.048522111028432846\n",
            "step = 68400: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 68600: loss = 0.04699709638953209\n",
            "step = 68600: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 68800: loss = 0.09786351025104523\n",
            "step = 68800: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 69000: loss = 0.04057721048593521\n",
            "step = 69000: Average Return = 0.9200000166893005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no_improvement_steps:  0\n",
            "step = 69200: loss = 0.04404187947511673\n",
            "step = 69200: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 69400: loss = 0.056631945073604584\n",
            "step = 69400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 69600: loss = 0.006819566711783409\n",
            "step = 69600: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 69800: loss = 0.004086694214493036\n",
            "step = 69800: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 70000: loss = 0.004175154492259026\n",
            "step = 70000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 70200: loss = 0.0977645069360733\n",
            "step = 70200: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 70400: loss = 0.09137320518493652\n",
            "step = 70400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 70600: loss = 0.04822912439703941\n",
            "step = 70600: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 70800: loss = 0.09074075520038605\n",
            "step = 70800: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 71000: loss = 0.005608623847365379\n",
            "step = 71000: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 71200: loss = 0.10921252518892288\n",
            "step = 71200: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 71400: loss = 0.05830123648047447\n",
            "step = 71400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 71600: loss = 0.003984851762652397\n",
            "step = 71600: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 71800: loss = 0.10677288472652435\n",
            "step = 71800: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 72000: loss = 0.05156293511390686\n",
            "step = 72000: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 72200: loss = 0.004642030224204063\n",
            "step = 72200: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 72400: loss = 0.05607009306550026\n",
            "step = 72400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 72600: loss = 0.06054879352450371\n",
            "step = 72600: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 72800: loss = 0.05428220331668854\n",
            "step = 72800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 73000: loss = 0.0034184667747467756\n",
            "step = 73000: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 73200: loss = 0.061753276735544205\n",
            "step = 73200: Average Return = 0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 73400: loss = 0.005913741886615753\n",
            "step = 73400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 73600: loss = 0.003653591265901923\n",
            "step = 73600: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 73800: loss = 0.05408736690878868\n",
            "step = 73800: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 74000: loss = 0.04421209171414375\n",
            "step = 74000: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 74200: loss = 0.004955837968736887\n",
            "step = 74200: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 74400: loss = 0.05301060900092125\n",
            "step = 74400: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 74600: loss = 0.05543076619505882\n",
            "step = 74600: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 74800: loss = 0.005836299620568752\n",
            "step = 74800: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 75000: loss = 0.008909307420253754\n",
            "step = 75000: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 75200: loss = 0.09108158946037292\n",
            "step = 75200: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 75400: loss = 0.0041950661689043045\n",
            "step = 75400: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 75600: loss = 0.051298368722200394\n",
            "step = 75600: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 75800: loss = 0.006498680915683508\n",
            "step = 75800: Average Return = 0.8999999761581421\n",
            "no_improvement_steps:  0\n",
            "step = 76000: loss = 0.04218786209821701\n",
            "step = 76000: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 76200: loss = 0.005764713976532221\n",
            "step = 76200: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 76400: loss = 0.005236854776740074\n",
            "step = 76400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 76600: loss = 0.05205550417304039\n",
            "step = 76600: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 76800: loss = 0.009381181560456753\n",
            "step = 76800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 77000: loss = 0.04911063238978386\n",
            "step = 77000: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 77200: loss = 0.06563887745141983\n",
            "step = 77200: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 77400: loss = 0.048351794481277466\n",
            "step = 77400: Average Return = 0.8999999761581421\n",
            "no_improvement_steps:  0\n",
            "step = 77600: loss = 0.05030343681573868\n",
            "step = 77600: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 77800: loss = 0.007132410537451506\n",
            "step = 77800: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 78000: loss = 0.0530872605741024\n",
            "step = 78000: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 78200: loss = 0.006371446885168552\n",
            "step = 78200: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 78400: loss = 0.00438238400965929\n",
            "step = 78400: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 78600: loss = 0.060000572353601456\n",
            "step = 78600: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 78800: loss = 0.09998205304145813\n",
            "step = 78800: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 79000: loss = 0.007557912729680538\n",
            "step = 79000: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 79200: loss = 0.059507936239242554\n",
            "step = 79200: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 79400: loss = 0.005879206117242575\n",
            "step = 79400: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 79600: loss = 0.11216176301240921\n",
            "step = 79600: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 79800: loss = 0.053069423884153366\n",
            "step = 79800: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 80000: loss = 0.06265561282634735\n",
            "step = 80000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 80200: loss = 0.10310180485248566\n",
            "step = 80200: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 80400: loss = 0.008830522187054157\n",
            "step = 80400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 80600: loss = 0.05941574275493622\n",
            "step = 80600: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 80800: loss = 0.00473457295447588\n",
            "step = 80800: Average Return = 0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 81000: loss = 0.10421448945999146\n",
            "step = 81000: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 81200: loss = 0.003480464220046997\n",
            "step = 81200: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 81400: loss = 0.1016480028629303\n",
            "step = 81400: Average Return = 0.8999999761581421\n",
            "no_improvement_steps:  0\n",
            "step = 81600: loss = 0.005159362684935331\n",
            "step = 81600: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 81800: loss = 0.05426144599914551\n",
            "step = 81800: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 82000: loss = 0.0596805177628994\n",
            "step = 82000: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 82200: loss = 0.06300827860832214\n",
            "step = 82200: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 82400: loss = 0.10855616629123688\n",
            "step = 82400: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 82600: loss = 0.004263098351657391\n",
            "step = 82600: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 82800: loss = 0.003576128976419568\n",
            "step = 82800: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 83000: loss = 0.04881321266293526\n",
            "step = 83000: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 83200: loss = 0.00611643074080348\n",
            "step = 83200: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 83400: loss = 0.1389496624469757\n",
            "step = 83400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 83600: loss = 0.10952054709196091\n",
            "step = 83600: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 83800: loss = 0.049285367131233215\n",
            "step = 83800: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 84000: loss = 0.10423815995454788\n",
            "step = 84000: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 84200: loss = 0.06761804223060608\n",
            "step = 84200: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 84400: loss = 0.1441836804151535\n",
            "step = 84400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 84600: loss = 0.00525909336283803\n",
            "step = 84600: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 84800: loss = 0.00522582745179534\n",
            "step = 84800: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 85000: loss = 0.11105534434318542\n",
            "step = 85000: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 85200: loss = 0.08825434744358063\n",
            "step = 85200: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 85400: loss = 0.051397982984781265\n",
            "step = 85400: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 85600: loss = 0.1097155511379242\n",
            "step = 85600: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 85800: loss = 0.005679969675838947\n",
            "step = 85800: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 86000: loss = 0.19446274638175964\n",
            "step = 86000: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 86200: loss = 0.050365570932626724\n",
            "step = 86200: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 86400: loss = 0.004706826992332935\n",
            "step = 86400: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 86600: loss = 0.004641920328140259\n",
            "step = 86600: Average Return = 0.9200000166893005\n",
            "no_improvement_steps:  0\n",
            "step = 86800: loss = 0.05491648614406586\n",
            "step = 86800: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 87000: loss = 0.0028000010643154383\n",
            "step = 87000: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 87200: loss = 0.10218144953250885\n",
            "step = 87200: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 87400: loss = 0.1062048003077507\n",
            "step = 87400: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 87600: loss = 0.05666187033057213\n",
            "step = 87600: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 87800: loss = 0.004792713560163975\n",
            "step = 87800: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 88000: loss = 0.06012937054038048\n",
            "step = 88000: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 88200: loss = 0.006230463273823261\n",
            "step = 88200: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 88400: loss = 0.004333979915827513\n",
            "step = 88400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 88600: loss = 0.004319760948419571\n",
            "step = 88600: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 88800: loss = 0.005278680939227343\n",
            "step = 88800: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 89000: loss = 0.04134759679436684\n",
            "step = 89000: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 89200: loss = 0.09951291978359222\n",
            "step = 89200: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 89400: loss = 0.060070011764764786\n",
            "step = 89400: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 89600: loss = 0.006782366894185543\n",
            "step = 89600: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 89800: loss = 0.00791467260569334\n",
            "step = 89800: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 90000: loss = 0.005396535620093346\n",
            "step = 90000: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 90200: loss = 0.00551106920465827\n",
            "step = 90200: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 90400: loss = 0.0474902018904686\n",
            "step = 90400: Average Return = 0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 90600: loss = 0.0947435200214386\n",
            "step = 90600: Average Return = 0.9200000166893005\n",
            "no_improvement_steps:  0\n",
            "step = 90800: loss = 0.04211542010307312\n",
            "step = 90800: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 91000: loss = 0.04801381379365921\n",
            "step = 91000: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 91200: loss = 0.0946788415312767\n",
            "step = 91200: Average Return = 0.8999999761581421\n",
            "no_improvement_steps:  0\n",
            "step = 91400: loss = 0.0032437348272651434\n",
            "step = 91400: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 91600: loss = 0.10778968036174774\n",
            "step = 91600: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 91800: loss = 0.005761205684393644\n",
            "step = 91800: Average Return = 0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 92000: loss = 0.0047520180232822895\n",
            "step = 92000: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 92200: loss = 0.19620463252067566\n",
            "step = 92200: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 92400: loss = 0.059435199946165085\n",
            "step = 92400: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 92600: loss = 0.006577268708497286\n",
            "step = 92600: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 92800: loss = 0.09385663270950317\n",
            "step = 92800: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 93000: loss = 0.044637005776166916\n",
            "step = 93000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 93200: loss = 0.053577765822410583\n",
            "step = 93200: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 93400: loss = 0.1490854024887085\n",
            "step = 93400: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 93600: loss = 0.005393522791564465\n",
            "step = 93600: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 93800: loss = 0.005524943582713604\n",
            "step = 93800: Average Return = 0.800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 94000: loss = 0.18269991874694824\n",
            "step = 94000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 94200: loss = 0.1456831991672516\n",
            "step = 94200: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 94400: loss = 0.05412135273218155\n",
            "step = 94400: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 94600: loss = 0.10918323695659637\n",
            "step = 94600: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 94800: loss = 0.10509049892425537\n",
            "step = 94800: Average Return = 0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 95000: loss = 0.0039162058383226395\n",
            "step = 95000: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 95200: loss = 0.058884572237730026\n",
            "step = 95200: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 95400: loss = 0.09359803795814514\n",
            "step = 95400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 95600: loss = 0.003792776260524988\n",
            "step = 95600: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 95800: loss = 0.09484141319990158\n",
            "step = 95800: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 96000: loss = 0.049606628715991974\n",
            "step = 96000: Average Return = 0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 96200: loss = 0.0060449400916695595\n",
            "step = 96200: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 96400: loss = 0.005964493378996849\n",
            "step = 96400: Average Return = 0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 96600: loss = 0.11571455746889114\n",
            "step = 96600: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 96800: loss = 0.0890289843082428\n",
            "step = 96800: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 97000: loss = 0.06524520367383957\n",
            "step = 97000: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 97200: loss = 0.003844201099127531\n",
            "step = 97200: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 97400: loss = 0.04913719370961189\n",
            "step = 97400: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 97600: loss = 0.005992335267364979\n",
            "step = 97600: Average Return = 0.8399999737739563\n",
            "no_improvement_steps:  0\n",
            "step = 97800: loss = 0.1028672382235527\n",
            "step = 97800: Average Return = 0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 98000: loss = 0.059871431440114975\n",
            "step = 98000: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 98200: loss = 0.0035095736384391785\n",
            "step = 98200: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 98400: loss = 0.16242676973342896\n",
            "step = 98400: Average Return = 0.8199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 98600: loss = 0.0032061561942100525\n",
            "step = 98600: Average Return = 0.6200000047683716\n",
            "no_improvement_steps:  0\n",
            "step = 98800: loss = 0.06360538303852081\n",
            "step = 98800: Average Return = 0.5199999809265137\n",
            "no_improvement_steps:  0\n",
            "step = 99000: loss = 0.0506662093102932\n",
            "step = 99000: Average Return = 0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 99200: loss = 0.005478029139339924\n",
            "step = 99200: Average Return = 0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 99400: loss = 0.04302374646067619\n",
            "step = 99400: Average Return = 0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 99600: loss = 0.005722294561564922\n",
            "step = 99600: Average Return = 0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 99800: loss = 0.04773639142513275\n",
            "step = 99800: Average Return = 0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 100000: loss = 0.051526911556720734\n",
            "step = 100000: Average Return = 0.7599999904632568\n",
            "no_improvement_steps:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_policy = tf.saved_model.load(policy_dir)"
      ],
      "metadata": {
        "id": "D9He1Uj9s6ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_return = compute_avg_return(eval_env, saved_policy, 5000)"
      ],
      "metadata": {
        "id": "wC5lJb8es9Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_return"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2u4nbq2eQoa",
        "outputId": "eb96976f-fd7c-48f2-8fe0-135cae14f459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7716"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_return = compute_avg_return(train_env, saved_policy, 5000)"
      ],
      "metadata": {
        "id": "E-gNrQ9teSAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_return"
      ],
      "metadata": {
        "id": "CCKzVR_U0wPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43a723f-64c7-40c8-a8cf-c15de70cb436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7064"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = range(0, iter_count + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "#plt.ylim(top=250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "WIVpHQGzeTqj",
        "outputId": "407b1d20-8a4b-4dee-87f9-e75f11b6464f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Iterations')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KNIuFszNF-PS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
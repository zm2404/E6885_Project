{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BlI8iHgEP1wB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d0ecab2-04e7-45fb-dc1d-73480f3a56a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf_agents\n",
            "  Downloading tf_agents-0.19.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf_agents)\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf_agents) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (4.5.0)\n",
            "Collecting pygame==2.1.3 (from tf_agents)\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-probability~=0.23.0 (from tf_agents)\n",
            "  Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents) (0.0.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (0.5.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (0.1.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697627 sha256=0c4db863596e8e3b040488bcd52186aba4e2b3b2efb2b19b30c642db172daec2\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
            "Successfully built gym\n",
            "Installing collected packages: tensorflow-probability, pygame, gym, tf_agents\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.22.0\n",
            "    Uninstalling tensorflow-probability-0.22.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.22.0\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.23.0 pygame-2.1.3 tensorflow-probability-0.23.0 tf_agents-0.19.0\n",
            "Requirement already satisfied: tf-agents[reverb] in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.23.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
            "Collecting rlds (from tf-agents[reverb])\n",
            "  Downloading rlds-0.1.8-py3-none-manylinux2010_x86_64.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dm-reverb~=0.14.0 (from tf-agents[reverb])\n",
            "  Downloading dm_reverb-0.14.0-cp310-cp310-manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow~=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.15.0->tf-agents[reverb]) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb~=0.14.0->tf-agents[reverb]) (5.9.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.2.2)\n",
            "Installing collected packages: rlds, dm-reverb\n",
            "Successfully installed dm-reverb-0.14.0 rlds-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install tf_agents\n",
        "!pip install tf-agents[reverb]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CLWWFFWwP-Fk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tf_agents\n",
        "import reverb\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import epsilon_greedy_policy\n",
        "from tf_agents.policies import policy_saver\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/E6885_Project')\n",
        "import SortWaterEnv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.version.VERSION"
      ],
      "metadata": {
        "id": "S7-5FyITqOTK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3d3d0bce-24c3-4d27-f559-8e5273c25cdc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "meuuIXmuQLEW"
      },
      "outputs": [],
      "source": [
        "num_iterations = 100000        #\n",
        "\n",
        "initial_collect_steps = 100     #\n",
        "collect_steps_per_iteration = 10#2#1   #\n",
        "replay_buffer_max_length = 100000  #\n",
        "\n",
        "batch_size = 64            #\n",
        "learning_rate = 1e-4 #1e-3        #\n",
        "log_interval = 200          #\n",
        "\n",
        "num_eval_episodes = 100        #\n",
        "eval_interval = 200#500#1000        #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_Be0fVlSQM1w"
      },
      "outputs": [],
      "source": [
        "############# create training and evaluation environment #############\n",
        "num_bottles = 7\n",
        "water_level = 4\n",
        "env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level, random_init=True)\n",
        "train_py_env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level, random_init=True)\n",
        "eval_py_env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level, random_init=True)\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KSsS5sENQQa2"
      },
      "outputs": [],
      "source": [
        "############ create a DQN agent ############\n",
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jddUDIr2QSTO"
      },
      "outputs": [],
      "source": [
        "# Customized Q netword\n",
        "class MaskedQNetwork(q_network.QNetwork):\n",
        "  def __init__(self, input_tensor_spec, action_spec, fc_layer_params=(100,), **kwargs):\n",
        "    # 从 input_tensor_spec 元组中提取观察值规格\n",
        "    observation_spec = input_tensor_spec[0]\n",
        "\n",
        "    # 调用基类的构造函数以构建网络\n",
        "    super(MaskedQNetwork, self).__init__(observation_spec, action_spec, fc_layer_params=fc_layer_params, **kwargs)\n",
        "\n",
        "  def call(self, observation, step_type=None, network_state=(), training=False):\n",
        "    # 直接调用父类的 call 方法，处理观察值\n",
        "    return super(MaskedQNetwork, self).call(\n",
        "        observation, step_type, network_state, training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rqi6-5F7RG-n"
      },
      "outputs": [],
      "source": [
        "observation_spec = train_env.observation_spec()\n",
        "action_spec = train_env.action_spec()\n",
        "\n",
        "# create Q-network\n",
        "q_net = MaskedQNetwork(\n",
        "    (observation_spec['observation'], observation_spec['action_mask']),\n",
        "    action_spec,\n",
        "    fc_layer_params=fc_layer_params\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jMoFiViiC7NV"
      },
      "outputs": [],
      "source": [
        "def observation_and_action_constraint_splitter(obs):\n",
        "\treturn obs['observation'], obs['action_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qI_nu75uRRqe"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter,\n",
        "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1AhbI3ZfbFpm"
      },
      "outputs": [],
      "source": [
        "# an example, just to show what policies are used during evaluation and collecting\n",
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sH_fIsadbICb"
      },
      "outputs": [],
      "source": [
        "# an example, just show how to create epsilon_greedy_policy. Not to be used\n",
        "base_policy = agent.policy\n",
        "epsilon = 0.1  # 例如，使用 0.1 作为 epsilon 值\n",
        "epsilon_greedy_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(base_policy, epsilon=epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "z9kV-ZLvbJVC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd747cdb-76e9-4db3-b4a7-8d2847df158e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TimeStep(\n",
            "{'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': {'observation': <tf.Tensor: shape=(1, 7, 4), dtype=int32, numpy=\n",
            "array([[[0, 0, 0, 0],\n",
            "        [2, 2, 2, 3],\n",
            "        [4, 2, 5, 1],\n",
            "        [4, 4, 3, 1],\n",
            "        [5, 5, 4, 1],\n",
            "        [3, 3, 5, 1],\n",
            "        [0, 0, 0, 0]]], dtype=int32)>,\n",
            "                 'action_mask': <tf.Tensor: shape=(1, 42), dtype=bool, numpy=\n",
            "array([[False, False, False, False, False, False,  True, False, False,\n",
            "        False, False,  True,  True, False, False, False, False,  True,\n",
            "         True, False, False, False, False,  True,  True, False, False,\n",
            "        False, False,  True,  True, False, False, False, False,  True,\n",
            "        False, False, False, False, False, False]])>}})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([23], dtype=int32)>, state=(), info=())"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# example of greedy policy choosing action\n",
        "example_py_env = SortWaterEnv.WaterSortEnv(num_bottles=num_bottles, water_level=water_level, random_init=True)\n",
        "example_env = tf_py_environment.TFPyEnvironment(example_py_env)\n",
        "time_step = example_env.reset()\n",
        "print(time_step)\n",
        "epsilon_greedy_policy.action(time_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_0MKdny2bK-Q"
      },
      "outputs": [],
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZuqXjbWrbMQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15cfe04-6720-42d5-9534-1228c08ac2af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1.0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# an example\n",
        "compute_avg_return(eval_env, epsilon_greedy_policy, num_eval_episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CQzc5_e4bP53"
      },
      "outputs": [],
      "source": [
        "################# Replay buffer #################\n",
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0XbLZjCtb__-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f0da85-f9ce-4a9e-9296-c87dee1577d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_TupleWrapper(Trajectory(\n",
              "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'observation': DictWrapper({'observation': BoundedTensorSpec(shape=(7, 4), dtype=tf.int32, name='observation', minimum=array(0, dtype=int32), maximum=array(5, dtype=int32)), 'action_mask': TensorSpec(shape=(42,), dtype=tf.bool, name='action_mask')}),\n",
              " 'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(41, dtype=int32)),\n",
              " 'policy_info': (),\n",
              " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
              " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))}))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "agent.collect_data_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TeCbnlphcesk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b849609-7c8d-4787-e2e6-0aad2e22c644"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('step_type',\n",
              " 'observation',\n",
              " 'action',\n",
              " 'policy_info',\n",
              " 'next_step_type',\n",
              " 'reward',\n",
              " 'discount')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "agent.collect_data_spec._fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "X3Hv74IhckJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d64880e-805a-4d1a-867c-3254548ce2f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TimeStep(\n",
              " {'step_type': array(1, dtype=int32),\n",
              "  'reward': array(0., dtype=float32),\n",
              "  'discount': array(1., dtype=float32),\n",
              "  'observation': {'observation': array([[1, 1, 0, 0],\n",
              "        [2, 0, 0, 0],\n",
              "        [3, 3, 1, 2],\n",
              "        [2, 0, 0, 0],\n",
              "        [5, 5, 4, 4],\n",
              "        [5, 5, 4, 1],\n",
              "        [3, 3, 4, 2]], dtype=int32),\n",
              "                  'action_mask': array([False, False, False, False, False, False, False, False,  True,\n",
              "        False, False, False, False,  True,  True, False, False, False,\n",
              "        False,  True, False, False, False, False, False, False, False,\n",
              "        False, False, False,  True, False, False, False, False, False,\n",
              "        False,  True, False,  True, False, False])}}),\n",
              " ())"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# an example.\n",
        "py_driver.PyDriver(\n",
        "    train_py_env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      epsilon_greedy_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(train_py_env.reset())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.get_state()"
      ],
      "metadata": {
        "id": "ohhyBZROzNh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4883cea3-ff21-4cea-b79a-a0cabbd84ad9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'observation': array([[0, 0, 0, 0],\n",
              "        [2, 1, 1, 3],\n",
              "        [0, 0, 0, 0],\n",
              "        [4, 4, 4, 3],\n",
              "        [5, 5, 4, 3],\n",
              "        [2, 1, 1, 3],\n",
              "        [2, 5, 5, 2]], dtype=int32),\n",
              " 'action_mask': array([False, False, False, False, False, False,  True,  True, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "         True, False,  True, False, False, False,  True, False,  True,\n",
              "        False, False, False,  True, False,  True, False, False, False,\n",
              "         True, False,  True, False, False, False])}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nh7SqXgkdYNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27811c9-260a-4c03-ef87-39f94722c124"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(Trajectory(\n",
              "{'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'observation': DictWrapper({'observation': TensorSpec(shape=(64, 2, 7, 4), dtype=tf.int32, name=None), 'action_mask': TensorSpec(shape=(64, 2, 42), dtype=tf.bool, name=None)}),\n",
              " 'action': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'policy_info': (),\n",
              " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
              " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None)}), SampleInfo(key=TensorSpec(shape=(64, 2), dtype=tf.uint64, name=None), probability=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), table_size=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), priority=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), times_sampled=TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)))>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Mp0I59Vwd11M"
      },
      "outputs": [],
      "source": [
        "iterator = iter(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pqBPd98nu52O"
      },
      "outputs": [],
      "source": [
        "# demo: only 1 episode\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = train_py_env.reset()\n",
        "\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    train_py_env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "######### early stopping ##########\n",
        "# set save direction\n",
        "tempdir = os.getenv(\"TEST_TMPDIR\", tempfile.gettempdir())\n",
        "# checkpointer\n",
        "# checkpoint_dir = os.path.join(tempdir, 'checkpoint')\n",
        "# train_checkpointer = common.Checkpointer(\n",
        "#     ckpt_dir=checkpoint_dir,\n",
        "#     max_to_keep=1,\n",
        "#     agent=agent,\n",
        "#     policy=agent.policy,\n",
        "#     replay_buffer=replay_buffer,\n",
        "#     global_step=train_step_counter\n",
        "# )\n",
        "# policy saver\n",
        "policy_dir = os.path.join(tempdir, 'policy')\n",
        "tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
        "\n",
        "# some threshold\n",
        "#best_avg_return = -float('inf')\n",
        "best_avg_return = 0.7\n",
        "no_improvement_steps = 0\n",
        "early_stopping_threshold = 10\n",
        "earlystop = False\n",
        "iter_thres = 10000\n",
        "######### early stopping ##########\n",
        "\n",
        "\n",
        "iter_count = 0\n",
        "for _ in range(num_iterations):\n",
        "  iter_count += 1\n",
        "  #print('iter_count: ',iter_count)\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)\n",
        "\n",
        "  ######### early stopping ##########\n",
        "    if avg_return > best_avg_return:\n",
        "      earlystop = True\n",
        "    if (avg_return > best_avg_return) and earlystop and (iter_count > iter_thres):\n",
        "      best_avg_return = avg_return\n",
        "      no_improvement_steps = 0\n",
        "      tf_policy_saver.save(policy_dir)  # save policy\n",
        "    elif earlystop and (iter_count > iter_thres):\n",
        "      no_improvement_steps += 0\n",
        "    print(\"no_improvement_steps: \",no_improvement_steps)\n",
        "  if no_improvement_steps >= early_stopping_threshold:\n",
        "    print(\"No improvement for {0} steps. Early stopping...\".format(no_improvement_steps))\n",
        "    break\n",
        "  ######### early stopping ##########"
      ],
      "metadata": {
        "id": "35k0m3HtqQyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61a83214-5268-4572-b8a6-cc0573937d69"
      },
      "execution_count": 24,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 200: loss = 0.03137896582484245\n",
            "step = 200: Average Return = -0.8600000143051147\n",
            "no_improvement_steps:  0\n",
            "step = 400: loss = 0.06236375868320465\n",
            "step = 400: Average Return = -0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 600: loss = 0.04768951237201691\n",
            "step = 600: Average Return = -0.7799999713897705\n",
            "no_improvement_steps:  0\n",
            "step = 800: loss = 0.03894064575433731\n",
            "step = 800: Average Return = -0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 1000: loss = 0.03656430542469025\n",
            "step = 1000: Average Return = -0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 1200: loss = 0.05239572376012802\n",
            "step = 1200: Average Return = -0.8799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 1400: loss = 0.04961619898676872\n",
            "step = 1400: Average Return = -0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 1600: loss = 0.013802826404571533\n",
            "step = 1600: Average Return = -0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 1800: loss = 0.041410304605960846\n",
            "step = 1800: Average Return = -0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 2000: loss = 0.07331004738807678\n",
            "step = 2000: Average Return = -0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 2200: loss = 0.06362967193126678\n",
            "step = 2200: Average Return = -0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 2400: loss = 0.010324090719223022\n",
            "step = 2400: Average Return = -0.7400000095367432\n",
            "no_improvement_steps:  0\n",
            "step = 2600: loss = 0.035292837768793106\n",
            "step = 2600: Average Return = -0.7200000286102295\n",
            "no_improvement_steps:  0\n",
            "step = 2800: loss = 0.11638826876878738\n",
            "step = 2800: Average Return = -0.6000000238418579\n",
            "no_improvement_steps:  0\n",
            "step = 3000: loss = 0.04492928832769394\n",
            "step = 3000: Average Return = -0.6399999856948853\n",
            "no_improvement_steps:  0\n",
            "step = 3200: loss = 0.11689327657222748\n",
            "step = 3200: Average Return = -0.699999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 3400: loss = 0.0680660605430603\n",
            "step = 3400: Average Return = -0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 3600: loss = 0.04364641010761261\n",
            "step = 3600: Average Return = -0.6600000262260437\n",
            "no_improvement_steps:  0\n",
            "step = 3800: loss = 0.06598594784736633\n",
            "step = 3800: Average Return = -0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 4000: loss = 0.11872970312833786\n",
            "step = 4000: Average Return = -0.4399999976158142\n",
            "no_improvement_steps:  0\n",
            "step = 4200: loss = 0.02979928068816662\n",
            "step = 4200: Average Return = -0.3400000035762787\n",
            "no_improvement_steps:  0\n",
            "step = 4400: loss = 0.06028060242533684\n",
            "step = 4400: Average Return = -0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 4600: loss = 0.06348631531000137\n",
            "step = 4600: Average Return = -0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 4800: loss = 0.03767349570989609\n",
            "step = 4800: Average Return = -0.7599999904632568\n",
            "no_improvement_steps:  0\n",
            "step = 5000: loss = 0.024100113660097122\n",
            "step = 5000: Average Return = -0.5\n",
            "no_improvement_steps:  0\n",
            "step = 5200: loss = 0.06375111639499664\n",
            "step = 5200: Average Return = -0.5799999833106995\n",
            "no_improvement_steps:  0\n",
            "step = 5400: loss = 0.031745173037052155\n",
            "step = 5400: Average Return = -0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 5600: loss = 0.024087131023406982\n",
            "step = 5600: Average Return = -0.4000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 5800: loss = 0.0514490008354187\n",
            "step = 5800: Average Return = -0.3400000035762787\n",
            "no_improvement_steps:  0\n",
            "step = 6000: loss = 0.053694259375333786\n",
            "step = 6000: Average Return = -0.23999999463558197\n",
            "no_improvement_steps:  0\n",
            "step = 6200: loss = 0.07081236690282822\n",
            "step = 6200: Average Return = -0.5600000023841858\n",
            "no_improvement_steps:  0\n",
            "step = 6400: loss = 0.09067879617214203\n",
            "step = 6400: Average Return = -0.3799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 6600: loss = 0.06128093972802162\n",
            "step = 6600: Average Return = -0.4000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 6800: loss = 0.06808862835168839\n",
            "step = 6800: Average Return = -0.5\n",
            "no_improvement_steps:  0\n",
            "step = 7000: loss = 0.02354271523654461\n",
            "step = 7000: Average Return = -0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 7200: loss = 0.05156176537275314\n",
            "step = 7200: Average Return = -0.23999999463558197\n",
            "no_improvement_steps:  0\n",
            "step = 7400: loss = 0.06644091755151749\n",
            "step = 7400: Average Return = -0.2800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 7600: loss = 0.047298677265644073\n",
            "step = 7600: Average Return = -0.6800000071525574\n",
            "no_improvement_steps:  0\n",
            "step = 7800: loss = 0.04613816365599632\n",
            "step = 7800: Average Return = -0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 8000: loss = 0.09244939684867859\n",
            "step = 8000: Average Return = -0.30000001192092896\n",
            "no_improvement_steps:  0\n",
            "step = 8200: loss = 0.04205965995788574\n",
            "step = 8200: Average Return = -0.36000001430511475\n",
            "no_improvement_steps:  0\n",
            "step = 8400: loss = 0.022669745609164238\n",
            "step = 8400: Average Return = -0.46000000834465027\n",
            "no_improvement_steps:  0\n",
            "step = 8600: loss = 0.035007599741220474\n",
            "step = 8600: Average Return = -0.5400000214576721\n",
            "no_improvement_steps:  0\n",
            "step = 8800: loss = 0.0967666506767273\n",
            "step = 8800: Average Return = -0.47999998927116394\n",
            "no_improvement_steps:  0\n",
            "step = 9000: loss = 0.06653972715139389\n",
            "step = 9000: Average Return = -0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 9200: loss = 0.09085826575756073\n",
            "step = 9200: Average Return = -0.2800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 9400: loss = 0.06168097257614136\n",
            "step = 9400: Average Return = -0.3400000035762787\n",
            "no_improvement_steps:  0\n",
            "step = 9600: loss = 0.08187311887741089\n",
            "step = 9600: Average Return = -0.41999998688697815\n",
            "no_improvement_steps:  0\n",
            "step = 9800: loss = 0.06451882421970367\n",
            "step = 9800: Average Return = -0.4399999976158142\n",
            "no_improvement_steps:  0\n",
            "step = 10000: loss = 0.05487402528524399\n",
            "step = 10000: Average Return = -0.4000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 10200: loss = 0.07887168973684311\n",
            "step = 10200: Average Return = -0.25999999046325684\n",
            "no_improvement_steps:  0\n",
            "step = 10400: loss = 0.11206850409507751\n",
            "step = 10400: Average Return = -0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 10600: loss = 0.02507897838950157\n",
            "step = 10600: Average Return = -0.20000000298023224\n",
            "no_improvement_steps:  0\n",
            "step = 10800: loss = 0.04395951330661774\n",
            "step = 10800: Average Return = -0.36000001430511475\n",
            "no_improvement_steps:  0\n",
            "step = 11000: loss = 0.11195285618305206\n",
            "step = 11000: Average Return = -0.3199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 11200: loss = 0.07047875225543976\n",
            "step = 11200: Average Return = -0.3799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 11400: loss = 0.05555375665426254\n",
            "step = 11400: Average Return = -0.25999999046325684\n",
            "no_improvement_steps:  0\n",
            "step = 11600: loss = 0.089433453977108\n",
            "step = 11600: Average Return = -0.2800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 11800: loss = 0.04087914153933525\n",
            "step = 11800: Average Return = -0.30000001192092896\n",
            "no_improvement_steps:  0\n",
            "step = 12000: loss = 0.10900981724262238\n",
            "step = 12000: Average Return = -0.23999999463558197\n",
            "no_improvement_steps:  0\n",
            "step = 12200: loss = 0.10297930240631104\n",
            "step = 12200: Average Return = -0.20000000298023224\n",
            "no_improvement_steps:  0\n",
            "step = 12400: loss = 0.0901222750544548\n",
            "step = 12400: Average Return = -0.2199999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 12600: loss = 0.07555340230464935\n",
            "step = 12600: Average Return = -0.3199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 12800: loss = 0.06579068303108215\n",
            "step = 12800: Average Return = -0.23999999463558197\n",
            "no_improvement_steps:  0\n",
            "step = 13000: loss = 0.06211705878376961\n",
            "step = 13000: Average Return = -0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 13200: loss = 0.07431615889072418\n",
            "step = 13200: Average Return = -0.2800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 13400: loss = 0.028874194249510765\n",
            "step = 13400: Average Return = -0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 13600: loss = 0.01793579012155533\n",
            "step = 13600: Average Return = -0.2800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 13800: loss = 0.12864181399345398\n",
            "step = 13800: Average Return = -0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 14000: loss = 0.07524985820055008\n",
            "step = 14000: Average Return = -0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 14200: loss = 0.10853838920593262\n",
            "step = 14200: Average Return = -0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 14400: loss = 0.0672789067029953\n",
            "step = 14400: Average Return = -0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 14600: loss = 0.088290736079216\n",
            "step = 14600: Average Return = -0.20000000298023224\n",
            "no_improvement_steps:  0\n",
            "step = 14800: loss = 0.05597779154777527\n",
            "step = 14800: Average Return = -0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 15000: loss = 0.10314464569091797\n",
            "step = 15000: Average Return = -0.3799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 15200: loss = 0.05081997439265251\n",
            "step = 15200: Average Return = -0.30000001192092896\n",
            "no_improvement_steps:  0\n",
            "step = 15400: loss = 0.09157884120941162\n",
            "step = 15400: Average Return = -0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 15600: loss = 0.07181601971387863\n",
            "step = 15600: Average Return = -0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 15800: loss = 0.05667662248015404\n",
            "step = 15800: Average Return = -0.30000001192092896\n",
            "no_improvement_steps:  0\n",
            "step = 16000: loss = 0.0878087729215622\n",
            "step = 16000: Average Return = -0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 16200: loss = 0.08863288164138794\n",
            "step = 16200: Average Return = -0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 16400: loss = 0.045608580112457275\n",
            "step = 16400: Average Return = -0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 16600: loss = 0.05303732305765152\n",
            "step = 16600: Average Return = -0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 16800: loss = 0.031154923141002655\n",
            "step = 16800: Average Return = -0.2199999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 17000: loss = 0.01374578382819891\n",
            "step = 17000: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 17200: loss = 0.04782262444496155\n",
            "step = 17200: Average Return = -0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 17400: loss = 0.11192132532596588\n",
            "step = 17400: Average Return = -0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 17600: loss = 0.07850223779678345\n",
            "step = 17600: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 17800: loss = 0.06478376686573029\n",
            "step = 17800: Average Return = -0.20000000298023224\n",
            "no_improvement_steps:  0\n",
            "step = 18000: loss = 0.14237931370735168\n",
            "step = 18000: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 18200: loss = 0.015345366671681404\n",
            "step = 18200: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 18400: loss = 0.08067643642425537\n",
            "step = 18400: Average Return = -0.2800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 18600: loss = 0.09339133650064468\n",
            "step = 18600: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 18800: loss = 0.03764832019805908\n",
            "step = 18800: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 19000: loss = 0.07307136803865433\n",
            "step = 19000: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 19200: loss = 0.02919195033609867\n",
            "step = 19200: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 19400: loss = 0.11066649854183197\n",
            "step = 19400: Average Return = -0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 19600: loss = 0.07067748159170151\n",
            "step = 19600: Average Return = -0.25999999046325684\n",
            "no_improvement_steps:  0\n",
            "step = 19800: loss = 0.052063681185245514\n",
            "step = 19800: Average Return = 0.25999999046325684\n",
            "no_improvement_steps:  0\n",
            "step = 20000: loss = 0.07925186306238174\n",
            "step = 20000: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 20200: loss = 0.13517343997955322\n",
            "step = 20200: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 20400: loss = 0.13305170834064484\n",
            "step = 20400: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 20600: loss = 0.13009391725063324\n",
            "step = 20600: Average Return = -0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 20800: loss = 0.0902789756655693\n",
            "step = 20800: Average Return = -0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 21000: loss = 0.04821774736046791\n",
            "step = 21000: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 21200: loss = 0.056610316038131714\n",
            "step = 21200: Average Return = -0.20000000298023224\n",
            "no_improvement_steps:  0\n",
            "step = 21400: loss = 0.12112694978713989\n",
            "step = 21400: Average Return = -0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 21600: loss = 0.07144466787576675\n",
            "step = 21600: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 21800: loss = 0.0855645090341568\n",
            "step = 21800: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 22000: loss = 0.11488469690084457\n",
            "step = 22000: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 22200: loss = 0.07003958523273468\n",
            "step = 22200: Average Return = -0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 22400: loss = 0.12757305800914764\n",
            "step = 22400: Average Return = -0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 22600: loss = 0.03802141547203064\n",
            "step = 22600: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 22800: loss = 0.04056163877248764\n",
            "step = 22800: Average Return = -0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 23000: loss = 0.026051076129078865\n",
            "step = 23000: Average Return = -0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 23200: loss = 0.07757662236690521\n",
            "step = 23200: Average Return = -0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 23400: loss = 0.05662399157881737\n",
            "step = 23400: Average Return = -0.25999999046325684\n",
            "no_improvement_steps:  0\n",
            "step = 23600: loss = 0.058245949447155\n",
            "step = 23600: Average Return = -0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 23800: loss = 0.08735890686511993\n",
            "step = 23800: Average Return = -0.2800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 24000: loss = 0.08737710863351822\n",
            "step = 24000: Average Return = -0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 24200: loss = 0.039894551038742065\n",
            "step = 24200: Average Return = -0.20000000298023224\n",
            "no_improvement_steps:  0\n",
            "step = 24400: loss = 0.0694848895072937\n",
            "step = 24400: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 24600: loss = 0.08886329084634781\n",
            "step = 24600: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 24800: loss = 0.06279836595058441\n",
            "step = 24800: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 25000: loss = 0.0409490168094635\n",
            "step = 25000: Average Return = -0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 25200: loss = 0.04219266027212143\n",
            "step = 25200: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 25400: loss = 0.09707166254520416\n",
            "step = 25400: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 25600: loss = 0.05686511471867561\n",
            "step = 25600: Average Return = -0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 25800: loss = 0.08730966597795486\n",
            "step = 25800: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 26000: loss = 0.07666907459497452\n",
            "step = 26000: Average Return = -0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 26200: loss = 0.11706966161727905\n",
            "step = 26200: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 26400: loss = 0.026655269786715508\n",
            "step = 26400: Average Return = -0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 26600: loss = 0.10894880443811417\n",
            "step = 26600: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 26800: loss = 0.014882308430969715\n",
            "step = 26800: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 27000: loss = 0.08759696781635284\n",
            "step = 27000: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 27200: loss = 0.014518548734486103\n",
            "step = 27200: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 27400: loss = 0.04852379858493805\n",
            "step = 27400: Average Return = -0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 27600: loss = 0.062309905886650085\n",
            "step = 27600: Average Return = -0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 27800: loss = 0.06542183458805084\n",
            "step = 27800: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 28000: loss = 0.13579881191253662\n",
            "step = 28000: Average Return = -0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 28200: loss = 0.03917187824845314\n",
            "step = 28200: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 28400: loss = 0.13983595371246338\n",
            "step = 28400: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 28600: loss = 0.12557557225227356\n",
            "step = 28600: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 28800: loss = 0.05810549855232239\n",
            "step = 28800: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 29000: loss = 0.10268573462963104\n",
            "step = 29000: Average Return = -0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 29200: loss = 0.05135689303278923\n",
            "step = 29200: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 29400: loss = 0.06642118096351624\n",
            "step = 29400: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 29600: loss = 0.0808577761054039\n",
            "step = 29600: Average Return = -0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 29800: loss = 0.06357656419277191\n",
            "step = 29800: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 30000: loss = 0.09039969742298126\n",
            "step = 30000: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 30200: loss = 0.10356944054365158\n",
            "step = 30200: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 30400: loss = 0.04276764765381813\n",
            "step = 30400: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 30600: loss = 0.0883604884147644\n",
            "step = 30600: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 30800: loss = 0.05136435851454735\n",
            "step = 30800: Average Return = 0.20000000298023224\n",
            "no_improvement_steps:  0\n",
            "step = 31000: loss = 0.09475027769804001\n",
            "step = 31000: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 31200: loss = 0.06708405911922455\n",
            "step = 31200: Average Return = -0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 31400: loss = 0.08054395020008087\n",
            "step = 31400: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 31600: loss = 0.16553625464439392\n",
            "step = 31600: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 31800: loss = 0.03609049320220947\n",
            "step = 31800: Average Return = -0.2800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 32000: loss = 0.09168735146522522\n",
            "step = 32000: Average Return = -0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 32200: loss = 0.06516999006271362\n",
            "step = 32200: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 32400: loss = 0.04502815753221512\n",
            "step = 32400: Average Return = -0.20000000298023224\n",
            "no_improvement_steps:  0\n",
            "step = 32600: loss = 0.17355424165725708\n",
            "step = 32600: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 32800: loss = 0.1485508382320404\n",
            "step = 32800: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 33000: loss = 0.14970837533473969\n",
            "step = 33000: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 33200: loss = 0.0759332925081253\n",
            "step = 33200: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 33400: loss = 0.10096225887537003\n",
            "step = 33400: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 33600: loss = 0.11588495969772339\n",
            "step = 33600: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 33800: loss = 0.040045566856861115\n",
            "step = 33800: Average Return = -0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 34000: loss = 0.07372231036424637\n",
            "step = 34000: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 34200: loss = 0.029516076669096947\n",
            "step = 34200: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 34400: loss = 0.16720907390117645\n",
            "step = 34400: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 34600: loss = 0.07195790112018585\n",
            "step = 34600: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 34800: loss = 0.008759019896388054\n",
            "step = 34800: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 35000: loss = 0.16155797243118286\n",
            "step = 35000: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 35200: loss = 0.055512685328722\n",
            "step = 35200: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 35400: loss = 0.04531605541706085\n",
            "step = 35400: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 35600: loss = 0.03207538276910782\n",
            "step = 35600: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 35800: loss = 0.025303063914179802\n",
            "step = 35800: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 36000: loss = 0.030111905187368393\n",
            "step = 36000: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 36200: loss = 0.10634604096412659\n",
            "step = 36200: Average Return = -0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 36400: loss = 0.069541335105896\n",
            "step = 36400: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 36600: loss = 0.038394879549741745\n",
            "step = 36600: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 36800: loss = 0.00986688956618309\n",
            "step = 36800: Average Return = -0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 37000: loss = 0.04626253992319107\n",
            "step = 37000: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 37200: loss = 0.07642978429794312\n",
            "step = 37200: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 37400: loss = 0.08997476100921631\n",
            "step = 37400: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 37600: loss = 0.09993918240070343\n",
            "step = 37600: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 37800: loss = 0.1727656126022339\n",
            "step = 37800: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 38000: loss = 0.16389885544776917\n",
            "step = 38000: Average Return = -0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 38200: loss = 0.05646127462387085\n",
            "step = 38200: Average Return = 0.2199999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 38400: loss = 0.06217695027589798\n",
            "step = 38400: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 38600: loss = 0.03231882303953171\n",
            "step = 38600: Average Return = -0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 38800: loss = 0.06923883408308029\n",
            "step = 38800: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 39000: loss = 0.044203005731105804\n",
            "step = 39000: Average Return = -0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 39200: loss = 0.08633895218372345\n",
            "step = 39200: Average Return = -0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 39400: loss = 0.08494148403406143\n",
            "step = 39400: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 39600: loss = 0.10621733963489532\n",
            "step = 39600: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 39800: loss = 0.07382942736148834\n",
            "step = 39800: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 40000: loss = 0.018525119870901108\n",
            "step = 40000: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 40200: loss = 0.13961544632911682\n",
            "step = 40200: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 40400: loss = 0.06531824171543121\n",
            "step = 40400: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 40600: loss = 0.12161973118782043\n",
            "step = 40600: Average Return = -0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 40800: loss = 0.0898301973938942\n",
            "step = 40800: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 41000: loss = 0.09678978472948074\n",
            "step = 41000: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 41200: loss = 0.09197872877120972\n",
            "step = 41200: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 41400: loss = 0.058096744120121\n",
            "step = 41400: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 41600: loss = 0.017893241718411446\n",
            "step = 41600: Average Return = -0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 41800: loss = 0.033455830067396164\n",
            "step = 41800: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 42000: loss = 0.04242635890841484\n",
            "step = 42000: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 42200: loss = 0.03659127652645111\n",
            "step = 42200: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 42400: loss = 0.08704259246587753\n",
            "step = 42400: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 42600: loss = 0.04418105259537697\n",
            "step = 42600: Average Return = -0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 42800: loss = 0.09838441759347916\n",
            "step = 42800: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 43000: loss = 0.08143384009599686\n",
            "step = 43000: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 43200: loss = 0.010573267936706543\n",
            "step = 43200: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 43400: loss = 0.05150245130062103\n",
            "step = 43400: Average Return = -0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 43600: loss = 0.0581519715487957\n",
            "step = 43600: Average Return = -0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 43800: loss = 0.032012034207582474\n",
            "step = 43800: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 44000: loss = 0.0473204180598259\n",
            "step = 44000: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 44200: loss = 0.021366797387599945\n",
            "step = 44200: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 44400: loss = 0.1010928601026535\n",
            "step = 44400: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 44600: loss = 0.14489728212356567\n",
            "step = 44600: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 44800: loss = 0.07062080502510071\n",
            "step = 44800: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 45000: loss = 0.06331805884838104\n",
            "step = 45000: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 45200: loss = 0.1263774186372757\n",
            "step = 45200: Average Return = -0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 45400: loss = 0.019635312259197235\n",
            "step = 45400: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 45600: loss = 0.054612237960100174\n",
            "step = 45600: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 45800: loss = 0.052918557077646255\n",
            "step = 45800: Average Return = -0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 46000: loss = 0.015879638493061066\n",
            "step = 46000: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 46200: loss = 0.0675574466586113\n",
            "step = 46200: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 46400: loss = 0.07284580171108246\n",
            "step = 46400: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 46600: loss = 0.09744003415107727\n",
            "step = 46600: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 46800: loss = 0.06430768221616745\n",
            "step = 46800: Average Return = -0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 47000: loss = 0.03407464548945427\n",
            "step = 47000: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 47200: loss = 0.04078309237957001\n",
            "step = 47200: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 47400: loss = 0.08013851940631866\n",
            "step = 47400: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 47600: loss = 0.04230699688196182\n",
            "step = 47600: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 47800: loss = 0.1240934506058693\n",
            "step = 47800: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 48000: loss = 0.18138760328292847\n",
            "step = 48000: Average Return = -0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 48200: loss = 0.13347163796424866\n",
            "step = 48200: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 48400: loss = 0.04829975217580795\n",
            "step = 48400: Average Return = 0.2199999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 48600: loss = 0.046043761074543\n",
            "step = 48600: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 48800: loss = 0.08855757862329483\n",
            "step = 48800: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 49000: loss = 0.0679243803024292\n",
            "step = 49000: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 49200: loss = 0.17356529831886292\n",
            "step = 49200: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 49400: loss = 0.03564504161477089\n",
            "step = 49400: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 49600: loss = 0.06729507446289062\n",
            "step = 49600: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 49800: loss = 0.01829424872994423\n",
            "step = 49800: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 50000: loss = 0.014656588435173035\n",
            "step = 50000: Average Return = -0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 50200: loss = 0.08720773458480835\n",
            "step = 50200: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 50400: loss = 0.07080396264791489\n",
            "step = 50400: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 50600: loss = 0.10573279112577438\n",
            "step = 50600: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 50800: loss = 0.012371066957712173\n",
            "step = 50800: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 51000: loss = 0.035338301211595535\n",
            "step = 51000: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 51200: loss = 0.042270004749298096\n",
            "step = 51200: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 51400: loss = 0.11339417845010757\n",
            "step = 51400: Average Return = -0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 51600: loss = 0.06244839355349541\n",
            "step = 51600: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 51800: loss = 0.10181625932455063\n",
            "step = 51800: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 52000: loss = 0.017533941194415092\n",
            "step = 52000: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 52200: loss = 0.048309218138456345\n",
            "step = 52200: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 52400: loss = 0.09749941527843475\n",
            "step = 52400: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 52600: loss = 0.04264207184314728\n",
            "step = 52600: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 52800: loss = 0.06831924617290497\n",
            "step = 52800: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 53000: loss = 0.05093681067228317\n",
            "step = 53000: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 53200: loss = 0.058292217552661896\n",
            "step = 53200: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 53400: loss = 0.1506773829460144\n",
            "step = 53400: Average Return = 0.23999999463558197\n",
            "no_improvement_steps:  0\n",
            "step = 53600: loss = 0.08620263636112213\n",
            "step = 53600: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 53800: loss = 0.1245994120836258\n",
            "step = 53800: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 54000: loss = 0.01655953750014305\n",
            "step = 54000: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 54200: loss = 0.131671741604805\n",
            "step = 54200: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 54400: loss = 0.10616609454154968\n",
            "step = 54400: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 54600: loss = 0.06716948747634888\n",
            "step = 54600: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 54800: loss = 0.02338552474975586\n",
            "step = 54800: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 55000: loss = 0.0822506695985794\n",
            "step = 55000: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 55200: loss = 0.04125712066888809\n",
            "step = 55200: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 55400: loss = 0.0877087339758873\n",
            "step = 55400: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 55600: loss = 0.03535863012075424\n",
            "step = 55600: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 55800: loss = 0.09819541871547699\n",
            "step = 55800: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 56000: loss = 0.014903143979609013\n",
            "step = 56000: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 56200: loss = 0.08509364724159241\n",
            "step = 56200: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 56400: loss = 0.04517432302236557\n",
            "step = 56400: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 56600: loss = 0.06738544255495071\n",
            "step = 56600: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 56800: loss = 0.024729803204536438\n",
            "step = 56800: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 57000: loss = 0.049014680087566376\n",
            "step = 57000: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 57200: loss = 0.08390486985445023\n",
            "step = 57200: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 57400: loss = 0.11393874883651733\n",
            "step = 57400: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 57600: loss = 0.15798762440681458\n",
            "step = 57600: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 57800: loss = 0.11265075206756592\n",
            "step = 57800: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 58000: loss = 0.08809994906187057\n",
            "step = 58000: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 58200: loss = 0.07709890604019165\n",
            "step = 58200: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 58400: loss = 0.10301963984966278\n",
            "step = 58400: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 58600: loss = 0.016950026154518127\n",
            "step = 58600: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 58800: loss = 0.06720582395792007\n",
            "step = 58800: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 59000: loss = 0.0750771164894104\n",
            "step = 59000: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 59200: loss = 0.010937931016087532\n",
            "step = 59200: Average Return = 0.25999999046325684\n",
            "no_improvement_steps:  0\n",
            "step = 59400: loss = 0.1953008472919464\n",
            "step = 59400: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 59600: loss = 0.14371562004089355\n",
            "step = 59600: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 59800: loss = 0.02180703915655613\n",
            "step = 59800: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 60000: loss = 0.1155024841427803\n",
            "step = 60000: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 60200: loss = 0.02900719828903675\n",
            "step = 60200: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 60400: loss = 0.031357668340206146\n",
            "step = 60400: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 60600: loss = 0.052324019372463226\n",
            "step = 60600: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 60800: loss = 0.01436065137386322\n",
            "step = 60800: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 61000: loss = 0.04327114671468735\n",
            "step = 61000: Average Return = 0.23999999463558197\n",
            "no_improvement_steps:  0\n",
            "step = 61200: loss = 0.043945200741291046\n",
            "step = 61200: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 61400: loss = 0.04714592173695564\n",
            "step = 61400: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 61600: loss = 0.19250382483005524\n",
            "step = 61600: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 61800: loss = 0.1554493010044098\n",
            "step = 61800: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 62000: loss = 0.03329010307788849\n",
            "step = 62000: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 62200: loss = 0.011957388371229172\n",
            "step = 62200: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 62400: loss = 0.06132269278168678\n",
            "step = 62400: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 62600: loss = 0.14375938475131989\n",
            "step = 62600: Average Return = 0.3799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 62800: loss = 0.06734757125377655\n",
            "step = 62800: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 63000: loss = 0.09382551908493042\n",
            "step = 63000: Average Return = 0.23999999463558197\n",
            "no_improvement_steps:  0\n",
            "step = 63200: loss = 0.010225788690149784\n",
            "step = 63200: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 63400: loss = 0.07067050039768219\n",
            "step = 63400: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 63600: loss = 0.09646935015916824\n",
            "step = 63600: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 63800: loss = 0.19542670249938965\n",
            "step = 63800: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 64000: loss = 0.09953487664461136\n",
            "step = 64000: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 64200: loss = 0.15471258759498596\n",
            "step = 64200: Average Return = -0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 64400: loss = 0.04515758156776428\n",
            "step = 64400: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 64600: loss = 0.07573357224464417\n",
            "step = 64600: Average Return = 0.2199999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 64800: loss = 0.04350023716688156\n",
            "step = 64800: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 65000: loss = 0.04859158396720886\n",
            "step = 65000: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 65200: loss = 0.02348795160651207\n",
            "step = 65200: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 65400: loss = 0.12419536709785461\n",
            "step = 65400: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 65600: loss = 0.12540102005004883\n",
            "step = 65600: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 65800: loss = 0.07215605676174164\n",
            "step = 65800: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 66000: loss = 0.037073809653520584\n",
            "step = 66000: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 66200: loss = 0.07374446839094162\n",
            "step = 66200: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 66400: loss = 0.04638603329658508\n",
            "step = 66400: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 66600: loss = 0.015177296474575996\n",
            "step = 66600: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 66800: loss = 0.051681917160749435\n",
            "step = 66800: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 67000: loss = 0.013306479901075363\n",
            "step = 67000: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 67200: loss = 0.040795065462589264\n",
            "step = 67200: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 67400: loss = 0.01702600158751011\n",
            "step = 67400: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 67600: loss = 0.03868693858385086\n",
            "step = 67600: Average Return = 0.25999999046325684\n",
            "no_improvement_steps:  0\n",
            "step = 67800: loss = 0.051500070840120316\n",
            "step = 67800: Average Return = -0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 68000: loss = 0.11985024809837341\n",
            "step = 68000: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 68200: loss = 0.04054214805364609\n",
            "step = 68200: Average Return = 0.2199999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 68400: loss = 0.22363117337226868\n",
            "step = 68400: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 68600: loss = 0.06337548792362213\n",
            "step = 68600: Average Return = 0.4000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 68800: loss = 0.04841024428606033\n",
            "step = 68800: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 69000: loss = 0.11787369847297668\n",
            "step = 69000: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 69200: loss = 0.04962461441755295\n",
            "step = 69200: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 69400: loss = 0.05741381645202637\n",
            "step = 69400: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 69600: loss = 0.07554885745048523\n",
            "step = 69600: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 69800: loss = 0.10803046822547913\n",
            "step = 69800: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 70000: loss = 0.1178441271185875\n",
            "step = 70000: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 70200: loss = 0.0804302990436554\n",
            "step = 70200: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 70400: loss = 0.11919830739498138\n",
            "step = 70400: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 70600: loss = 0.09061522036790848\n",
            "step = 70600: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 70800: loss = 0.14658449590206146\n",
            "step = 70800: Average Return = 0.30000001192092896\n",
            "no_improvement_steps:  0\n",
            "step = 71000: loss = 0.08339234441518784\n",
            "step = 71000: Average Return = 0.2800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 71200: loss = 0.011712983250617981\n",
            "step = 71200: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 71400: loss = 0.11591681838035583\n",
            "step = 71400: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 71600: loss = 0.12921926379203796\n",
            "step = 71600: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 71800: loss = 0.10376928001642227\n",
            "step = 71800: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 72000: loss = 0.03767675906419754\n",
            "step = 72000: Average Return = 0.3199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 72200: loss = 0.22819849848747253\n",
            "step = 72200: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 72400: loss = 0.04176782816648483\n",
            "step = 72400: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 72600: loss = 0.04197380691766739\n",
            "step = 72600: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 72800: loss = 0.06872368603944778\n",
            "step = 72800: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 73000: loss = 0.15538211166858673\n",
            "step = 73000: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 73200: loss = 0.12271683663129807\n",
            "step = 73200: Average Return = 0.20000000298023224\n",
            "no_improvement_steps:  0\n",
            "step = 73400: loss = 0.0680844634771347\n",
            "step = 73400: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 73600: loss = 0.06197960302233696\n",
            "step = 73600: Average Return = 0.2800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 73800: loss = 0.0749882161617279\n",
            "step = 73800: Average Return = 0.3799999952316284\n",
            "no_improvement_steps:  0\n",
            "step = 74000: loss = 0.014410838484764099\n",
            "step = 74000: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 74200: loss = 0.10838973522186279\n",
            "step = 74200: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 74400: loss = 0.04153180122375488\n",
            "step = 74400: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 74600: loss = 0.048795007169246674\n",
            "step = 74600: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 74800: loss = 0.04967242479324341\n",
            "step = 74800: Average Return = 0.3199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 75000: loss = 0.11001000553369522\n",
            "step = 75000: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 75200: loss = 0.0929788202047348\n",
            "step = 75200: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 75400: loss = 0.0336558073759079\n",
            "step = 75400: Average Return = 0.2800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 75600: loss = 0.010206932201981544\n",
            "step = 75600: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 75800: loss = 0.1309932917356491\n",
            "step = 75800: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 76000: loss = 0.055987466126680374\n",
            "step = 76000: Average Return = 0.2199999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 76200: loss = 0.07116194069385529\n",
            "step = 76200: Average Return = 0.20000000298023224\n",
            "no_improvement_steps:  0\n",
            "step = 76400: loss = 0.12301106005907059\n",
            "step = 76400: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 76600: loss = 0.05164606124162674\n",
            "step = 76600: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 76800: loss = 0.0068812863901257515\n",
            "step = 76800: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 77000: loss = 0.062300633639097214\n",
            "step = 77000: Average Return = 0.2800000011920929\n",
            "no_improvement_steps:  0\n",
            "step = 77200: loss = 0.06463219225406647\n",
            "step = 77200: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 77400: loss = 0.05021410062909126\n",
            "step = 77400: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 77600: loss = 0.06570069491863251\n",
            "step = 77600: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 77800: loss = 0.017641160637140274\n",
            "step = 77800: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 78000: loss = 0.10910603404045105\n",
            "step = 78000: Average Return = 0.20000000298023224\n",
            "no_improvement_steps:  0\n",
            "step = 78200: loss = 0.034940894693136215\n",
            "step = 78200: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 78400: loss = 0.11573734134435654\n",
            "step = 78400: Average Return = 0.20000000298023224\n",
            "no_improvement_steps:  0\n",
            "step = 78600: loss = 0.047429125756025314\n",
            "step = 78600: Average Return = -0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 78800: loss = 0.042579665780067444\n",
            "step = 78800: Average Return = 0.2199999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 79000: loss = 0.11255117505788803\n",
            "step = 79000: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 79200: loss = 0.057472944259643555\n",
            "step = 79200: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 79400: loss = 0.0497732050716877\n",
            "step = 79400: Average Return = 0.2199999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 79600: loss = 0.09073460847139359\n",
            "step = 79600: Average Return = 0.2199999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 79800: loss = 0.119337297976017\n",
            "step = 79800: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 80000: loss = 0.05712760612368584\n",
            "step = 80000: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 80200: loss = 0.09318670630455017\n",
            "step = 80200: Average Return = 0.20000000298023224\n",
            "no_improvement_steps:  0\n",
            "step = 80400: loss = 0.017614303156733513\n",
            "step = 80400: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 80600: loss = 0.1200719028711319\n",
            "step = 80600: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 80800: loss = 0.09469916671514511\n",
            "step = 80800: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 81000: loss = 0.016451474279165268\n",
            "step = 81000: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 81200: loss = 0.04290977120399475\n",
            "step = 81200: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 81400: loss = 0.11490695178508759\n",
            "step = 81400: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 81600: loss = 0.06480856239795685\n",
            "step = 81600: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 81800: loss = 0.0816221833229065\n",
            "step = 81800: Average Return = 0.25999999046325684\n",
            "no_improvement_steps:  0\n",
            "step = 82000: loss = 0.04027600586414337\n",
            "step = 82000: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 82200: loss = 0.06615973263978958\n",
            "step = 82200: Average Return = 0.2199999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 82400: loss = 0.06260079890489578\n",
            "step = 82400: Average Return = 0.3400000035762787\n",
            "no_improvement_steps:  0\n",
            "step = 82600: loss = 0.07374589890241623\n",
            "step = 82600: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 82800: loss = 0.11021186411380768\n",
            "step = 82800: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 83000: loss = 0.1639155149459839\n",
            "step = 83000: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 83200: loss = 0.18443365395069122\n",
            "step = 83200: Average Return = 0.25999999046325684\n",
            "no_improvement_steps:  0\n",
            "step = 83400: loss = 0.15646135807037354\n",
            "step = 83400: Average Return = 0.3400000035762787\n",
            "no_improvement_steps:  0\n",
            "step = 83600: loss = 0.09523934870958328\n",
            "step = 83600: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 83800: loss = 0.0716802328824997\n",
            "step = 83800: Average Return = 0.23999999463558197\n",
            "no_improvement_steps:  0\n",
            "step = 84000: loss = 0.07110242545604706\n",
            "step = 84000: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 84200: loss = 0.07140002399682999\n",
            "step = 84200: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 84400: loss = 0.0761256068944931\n",
            "step = 84400: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 84600: loss = 0.07202716171741486\n",
            "step = 84600: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 84800: loss = 0.09143057465553284\n",
            "step = 84800: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 85000: loss = 0.020894456654787064\n",
            "step = 85000: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 85200: loss = 0.07389294356107712\n",
            "step = 85200: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 85400: loss = 0.1235351413488388\n",
            "step = 85400: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 85600: loss = 0.043429579585790634\n",
            "step = 85600: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 85800: loss = 0.05698714405298233\n",
            "step = 85800: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 86000: loss = 0.11358565092086792\n",
            "step = 86000: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 86200: loss = 0.010498070158064365\n",
            "step = 86200: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 86400: loss = 0.12602390348911285\n",
            "step = 86400: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 86600: loss = 0.014933286234736443\n",
            "step = 86600: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 86800: loss = 0.10581149160861969\n",
            "step = 86800: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 87000: loss = 0.06426998972892761\n",
            "step = 87000: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 87200: loss = 0.01564428023993969\n",
            "step = 87200: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 87400: loss = 0.03804319351911545\n",
            "step = 87400: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 87600: loss = 0.08350616693496704\n",
            "step = 87600: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 87800: loss = 0.010860266163945198\n",
            "step = 87800: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 88000: loss = 0.04591362178325653\n",
            "step = 88000: Average Return = 0.20000000298023224\n",
            "no_improvement_steps:  0\n",
            "step = 88200: loss = 0.12926644086837769\n",
            "step = 88200: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 88400: loss = 0.08261990547180176\n",
            "step = 88400: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 88600: loss = 0.04406927525997162\n",
            "step = 88600: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 88800: loss = 0.10028520226478577\n",
            "step = 88800: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 89000: loss = 0.21070024371147156\n",
            "step = 89000: Average Return = 0.23999999463558197\n",
            "no_improvement_steps:  0\n",
            "step = 89200: loss = 0.07679575681686401\n",
            "step = 89200: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 89400: loss = 0.04404760152101517\n",
            "step = 89400: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 89600: loss = 0.05845850333571434\n",
            "step = 89600: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 89800: loss = 0.1211816817522049\n",
            "step = 89800: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 90000: loss = 0.10560192912817001\n",
            "step = 90000: Average Return = -0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 90200: loss = 0.012129003182053566\n",
            "step = 90200: Average Return = 0.25999999046325684\n",
            "no_improvement_steps:  0\n",
            "step = 90400: loss = 0.07840755581855774\n",
            "step = 90400: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 90600: loss = 0.03544038161635399\n",
            "step = 90600: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 90800: loss = 0.03803868219256401\n",
            "step = 90800: Average Return = -0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 91000: loss = 0.046852242201566696\n",
            "step = 91000: Average Return = 0.30000001192092896\n",
            "no_improvement_steps:  0\n",
            "step = 91200: loss = 0.13873493671417236\n",
            "step = 91200: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 91400: loss = 0.05435154587030411\n",
            "step = 91400: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 91600: loss = 0.04049885272979736\n",
            "step = 91600: Average Return = 0.11999999731779099\n",
            "no_improvement_steps:  0\n",
            "step = 91800: loss = 0.09723614156246185\n",
            "step = 91800: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 92000: loss = 0.03243735805153847\n",
            "step = 92000: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 92200: loss = 0.009969362057745457\n",
            "step = 92200: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 92400: loss = 0.14675407111644745\n",
            "step = 92400: Average Return = 0.3400000035762787\n",
            "no_improvement_steps:  0\n",
            "step = 92600: loss = 0.015165857970714569\n",
            "step = 92600: Average Return = 0.3199999928474426\n",
            "no_improvement_steps:  0\n",
            "step = 92800: loss = 0.01033596321940422\n",
            "step = 92800: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 93000: loss = 0.011680703610181808\n",
            "step = 93000: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 93200: loss = 0.05252964422106743\n",
            "step = 93200: Average Return = -0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 93400: loss = 0.06707486510276794\n",
            "step = 93400: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 93600: loss = 0.09492763876914978\n",
            "step = 93600: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 93800: loss = 0.06736773252487183\n",
            "step = 93800: Average Return = 0.30000001192092896\n",
            "no_improvement_steps:  0\n",
            "step = 94000: loss = 0.13931873440742493\n",
            "step = 94000: Average Return = 0.23999999463558197\n",
            "no_improvement_steps:  0\n",
            "step = 94200: loss = 0.07090402394533157\n",
            "step = 94200: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 94400: loss = 0.1544543355703354\n",
            "step = 94400: Average Return = 0.2199999988079071\n",
            "no_improvement_steps:  0\n",
            "step = 94600: loss = 0.06219585984945297\n",
            "step = 94600: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 94800: loss = 0.09038138389587402\n",
            "step = 94800: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 95000: loss = 0.011666705831885338\n",
            "step = 95000: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 95200: loss = 0.013881048187613487\n",
            "step = 95200: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 95400: loss = 0.06813199073076248\n",
            "step = 95400: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 95600: loss = 0.1248788982629776\n",
            "step = 95600: Average Return = -0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 95800: loss = 0.013850963674485683\n",
            "step = 95800: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 96000: loss = 0.047089021652936935\n",
            "step = 96000: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 96200: loss = 0.0756702721118927\n",
            "step = 96200: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 96400: loss = 0.03591252118349075\n",
            "step = 96400: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 96600: loss = 0.0588640496134758\n",
            "step = 96600: Average Return = 0.25999999046325684\n",
            "no_improvement_steps:  0\n",
            "step = 96800: loss = 0.013155404478311539\n",
            "step = 96800: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 97000: loss = 0.13745373487472534\n",
            "step = 97000: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 97200: loss = 0.08895545452833176\n",
            "step = 97200: Average Return = -0.03999999910593033\n",
            "no_improvement_steps:  0\n",
            "step = 97400: loss = 0.0725269541144371\n",
            "step = 97400: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 97600: loss = 0.06600790470838547\n",
            "step = 97600: Average Return = 0.0\n",
            "no_improvement_steps:  0\n",
            "step = 97800: loss = 0.038377903401851654\n",
            "step = 97800: Average Return = 0.18000000715255737\n",
            "no_improvement_steps:  0\n",
            "step = 98000: loss = 0.032787758857011795\n",
            "step = 98000: Average Return = 0.07999999821186066\n",
            "no_improvement_steps:  0\n",
            "step = 98200: loss = 0.08687851577997208\n",
            "step = 98200: Average Return = 0.10000000149011612\n",
            "no_improvement_steps:  0\n",
            "step = 98400: loss = 0.07376246154308319\n",
            "step = 98400: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 98600: loss = 0.04389200732111931\n",
            "step = 98600: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 98800: loss = 0.04316786676645279\n",
            "step = 98800: Average Return = 0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 99000: loss = 0.04495055973529816\n",
            "step = 99000: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 99200: loss = 0.027780922129750252\n",
            "step = 99200: Average Return = 0.14000000059604645\n",
            "no_improvement_steps:  0\n",
            "step = 99400: loss = 0.09210069477558136\n",
            "step = 99400: Average Return = 0.1599999964237213\n",
            "no_improvement_steps:  0\n",
            "step = 99600: loss = 0.01336006261408329\n",
            "step = 99600: Average Return = -0.019999999552965164\n",
            "no_improvement_steps:  0\n",
            "step = 99800: loss = 0.010027089156210423\n",
            "step = 99800: Average Return = 0.05999999865889549\n",
            "no_improvement_steps:  0\n",
            "step = 100000: loss = 0.08279545605182648\n",
            "step = 100000: Average Return = 0.03999999910593033\n",
            "no_improvement_steps:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  saved_policy = tf.saved_model.load(policy_dir)\n",
        "  avg_return = compute_avg_return(eval_env, saved_policy, 1000)\n",
        "  print(avg_return)\n",
        "  avg_return = compute_avg_return(train_env, saved_policy, 1000)\n",
        "  print(avg_return)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "_VnLdoofJeew"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = range(0, iter_count + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "#plt.ylim(top=250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "WIVpHQGzeTqj",
        "outputId": "3b40de56-b086-4907-a3f5-729fa9909170"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Iterations')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkYAAAGwCAYAAABM/qr1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACjAUlEQVR4nO2dd3wUZf7HPzvb0hMCgVCCVCmCVEFARSUC4llOz8rZDytnwXLyO0U9C+hZUPTOs53lUNSznOd5eEgRUaT3LsXQQoCQnmyd3x+7M/s8s8/Mzm52s5vk+3698oKd+szs7jyf/VaLLMsyCIIgCIIgCEjJHgBBEARBEESqQMKIIAiCIAgiCAkjgiAIgiCIICSMCIIgCIIggpAwIgiCIAiCCELCiCAIgiAIIggJI4IgCIIgiCC2ZA8g1fH7/Th06BCys7NhsViSPRyCIAiCIEwgyzKqq6vRqVMnSJJ5OxAJowgcOnQIRUVFyR4GQRAEQRAxsH//fnTp0sX09iSMIpCdnQ0gcGNzcnKSPBqCIAiCIMxQVVWFoqIidR43CwmjCCjus5ycHBJGBEEQBNHMiDYMhoKvCYIgCIIggpAwIgiCIAiCCELCiCAIgiAIIggJI4IgCIIgiCAkjAiCIAiCIIKQMCIIgiAIgghCwoggCIIgCCIICSOCIAiCIIggJIwIgiAIgiCCkDAiCIIgCIII0uyE0auvvopu3bohLS0NI0eOxMqVK03tN2/ePFgsFlxyySWJHSBBEARBEM2WZiWMPvroI0ybNg2PPvoo1q5di0GDBmHChAkoKysz3G/fvn24//77ceaZZzbRSAmCIAiCaI40K2H0wgsvYMqUKbjxxhvRv39/vPbaa8jIyMDbb7+tu4/P58PkyZPx+OOPo0ePHk04WoIgiNZFg8cHWZaTPQyCaBTNRhi53W6sWbMGxcXF6jJJklBcXIzly5fr7venP/0J7du3x80332zqPC6XC1VVVdwfQRAEYcz+8jr0fWQ+pn64LtlDIYhG0WyE0bFjx+Dz+dChQwdueYcOHVBaWircZ9myZXjrrbfwxhtvmD7PzJkzkZubq/4VFRU1atwEQRCtgfeW7wMA/Gfj4eQOhCAaSbMRRtFSXV2Na6+9Fm+88QbatWtner/p06ejsrJS/du/f38CR0kQBEEQRCphS/YAzNKuXTtYrVYcOXKEW37kyBEUFhaGbb97927s27cPF154obrM7/cDAGw2G3bs2IGePXuG7ed0OuF0OuM8eoIgCIIgmgPNxmLkcDgwbNgwLFy4UF3m9/uxcOFCjBo1Kmz7vn37YtOmTVi/fr36d9FFF+Gcc87B+vXryUVGEESLR5ZlbDxQgeoGTxOcK7HHr6zzYMuhysSehCDQjCxGADBt2jRcf/31GD58OEaMGIHZs2ejtrYWN954IwDguuuuQ+fOnTFz5kykpaVhwIAB3P55eXkAELacIAiiJfLttjJMeW81urfLxOL7z07ouRKdi3bms4tQ1eDFZ3eMxtCubRJ8NqI106yE0ZVXXomjR49ixowZKC0txeDBgzF//nw1ILukpASS1GyMYARBEAnli/UHAQB7j9UmeSSNp6rBCwBYsuMoCSMioTQrYQQAU6dOxdSpU4XrlixZYrjvO++8E/8BEQRBpCpNWFKoqcoXOayWpjkR0Woh8wpBEATRbLBbadoiEgt9wgiCIFoochOajJrqXCSMiERDnzCCIIgWSrBCSZOQSFea3x86uN1G0xaRWOgTRhAE0UJpSotRImnw+tT/U4wRkWhIGBEEQbRQWko/1wZPyPRlo8xjIsHQJ4wgCKKF0kJ0Eeo9IYtRS7kmInUhYUQQBNFCaUqLkZzAk9W7Q8LI15SBUyb523e78frS3errvcdqcf8nG7DnaE0SR2XMP376BX/+ZntC37fmSrOrY0QQBEGYpWVMeg2MxcjrT61rqqzzYOZ/twMArh7RFdlpdvz2zRU4WFGPn/Ycx7I/nJvkEYp5+IvNAIAxPdthdC/zjdZbA2QxIgiCaKE0pYZI5KlYYeRLMWHEuvkUY9bBinoAwIET9ckYUlSs21+R7CGkHCSMCIIgWigtxU3CBl+nmjDyMffYk4JuPhHs52LXkeokjiQ1IWFEEARBNJpEarD6FLYYub0hMeT1pdbY9PAw49x5JHXjoJIFCSOCIIgWSlNO02zNJH8M4uWJr7bi2rdWCIVPvSDG6FiNCxfOWYb3f/olhtHGDxdTY8nj4y1GVik1ay55GcvWrrLqFmNZjBckjAiCIFooyTKu+GOYaN9athff7zqGH3cfC1snijF66dtd2HSwEo8Eg4iThYtx82mFkT1Fi1F6vIz7zydzrkqChBFBEESLJVmWAF8jzsu6phREwqjW5Y35HPHExbrSNErUkaJ93bSxUI15v1oiqfmuEQRBEHEl0SKJPXxjYpBFw2TrGKVaur6RK81hszb1cEyhjYXyNZPYqKaChBFBEEQLhXVpJTpomT16JFdavdunK9RES/mstOD/U8RLxbvStBaj5A+ywRN+r7UCTmsxYoVoa4SEEUEQRCugKd0lRufaeqgKg/70Pzz+763C9SJRxWelNX588YRzpYVZjJI7xe4vr0PfR+Zj6gfruOVaYcQGY7++dDf6zZiPJTvKmmSMqQgJI4IgiBYKqzGaMs3dKCvtHyt+gdvrxzs/7hNuL7Ik8TFGqaWMeFeazImOZAujuStKAAD/2XSYW651R7Kfjae/DlTxvv+TjQkeXepCwoggCKKFwmqMRMfmcDFGBqfqmp+h/r+6wQOAtzCJ9k3lliCsxcjj86OOcUPZkxx8bdHx5GkD3EWimb3nrQ0SRgRBEC0U1i0VS22haJBNxjNlp4VadG4vrQ7bXhh8zbXdSDFhxIk2P+rcoWw5PWHSVOid3shipFBPwoggCIJoaTSlxYidXI2Cr9mMqK2HqkztG8li5PL6sLbkhHCCl2UZ6/dXJCygmLcYyVwZgWRXwpZ0lJk2Fkp0T1OtwnhTQsKIIAiihcIG1SZ6ojMrjNgYnO2lQWHEbC/asz5Cr7RpH23ApX/5EXMW7QpbN2/Vflzy6g+49q0VhuOPFa0rrdYVEmDuJEeK67rSNONKNStcsiFhRBAE0UJhLQEJF0YmXWnsmKobAtaVSMHXLlGvNGYzJbj4b9/tCdv3w5WBAOTVv5wwGn7MsMHXXp+MWnfqWIx0XWmacaVa3FayIWFEEATRQmHr6jSpxcjAUMK6cZQg4EjWJtbKZHQdyajgrG0JUudihVKyLUY6rjRt5WsSRhwkjAiCIFoo7MScaKsAF+htFGPEjENx6bATs7ZIIrsdt79gzk+GS6hBk67PWozcybYY6Wal6VuMUrTvbZNCwohosfj8Mj5YUYKfy2qSPRSCiJlDFfV4f/m+mIKHeVdafK0XP5dV48OVJaqoYcWNkeWGdeOoFqMIbjg2vdzIuhFL89pY2XO0BnNX/MLFFAWy0vjXTc3GAxX4bO0BAIBFx5lmZDFKt4famJhtI/P9rqNYuO1ItENNWWyRNyGI5sm8VSX44+eBztv7Zl2Q5NEQRGxc/OoPOFrtws9lNXj84gFR7cu7oOI7ruIXlgIIGG6uGtFV40ozCL72G7vSRO4n88Io8rjjxbnPfxe2LFDgkY85amoueuUHAEBhTpqu9SesVxorjBxW1AbFXb3HhwyHsUzw+Py49q2VAID1M85DXoYj1qGnDGQxIlos60sqkj0Egmg0R6tdAIDFO45GvS8nOBJkvVi/vyLsXEYChbMYBUUQOzShK03UwT4Fw2I8Pj8nRpOZlba9tNp0Vhr72bAyaqqq3otIsKUUlGD65g4JI6LFolfDgyCaI7G4iZo0+Npk+xFh8LVsLODcnOUrOrHRlPHYXp+fj+tKojDy+WUu+Jp1i2ktRuwtZddVBSuTG8GK1pbyyCVhRLRYJPp0Ey2IWCb4pqxj5I+QWRYakyj4Wr9DPcDXClJXp+Ak7PbJ3PX55eRlfHn9MidU2GEYNZFlRWhVfWRh5DLp5mxO0NRBtFjIYkQkkm+3HsEfP9/E1bFJBnuO1uD+TzZgz9HwJAP21/+0jzfg57LquJ9f+ZrFUvk6FGMUWh85+Do6K0xTPgYCFiN+/Pd/ssGU5SVaGjw+/PHzTVi0XRz07PP7ueBrLyc+9YOv2XVmxq0tcNkSIGFEtFhIGBGJ5HfvrcbcFSV4f/kvTXI+PbFxw99X4Z9rDmDym+GVndmJau+xWlz1emKqPwOarLQ4B197OPdUdDFGTepK88vc9QHA5+sOYs7C8IrcjeXdH/dh7ooS3PTOauF6n58XhXy8mX7wNWuxMxMzxP4w0JYBaK6QMCJaLFSPg2gK9h6rbZLz6AmjkvI6AMDhyoawdVqBcqzGFf+BKecyW8dIFHzNbO/RjNnr83NuoKZMyY8WtzfcYgQAhyrC35vGcqii3nB9wGIUghVDHq/YYuTzy9xnxkyJCG2By5YApesTLRaJlBHRBDR4mmYyiFYPyLLcpK0eTGelCSxGXgOLUXgGVXTX1KSuNL8fshx+Qqet6W0QXr/MWc19PgPxGXytFTZsxpkeLdGVRsKIaLFYyZVGNAENTRRjFK3Gaer+V36zvdKEMUb6la/dOtYNs8HXiTAw6dVp8vpkyAIN5LTHXxjptftQ8MkyZMbfaCQ+fTrCqN6E6OdcaS1EGJErjWixWMliFDdKKxvwqznf46NVJckeSlQcqqjHBS9/j49X7U/YORpiqEgdG9HN8GaLCx6vceHCOcvw3vJ9MYwpwG3vr8HGA5Xqa6MCj15NrIvfL3OiSpuurxVGka5r1n+348q/LTdl7QCAyjoPLn5lGd78fg9++PkYxsxahHOfX2JYMV9PALgFwdcA4LRZBVsnFp9P1g1q1wu+1orSZ+Zvx2/fXGFYdoB3paWumzMaSBgRLZZIv6gI8zz99TZsPliFP3y6KdlDiYpn5m/HlkNVePDTjQk7R6pajLRBwHq8tHAXNh2sxIx/bYlhVMAvx+swf0spt8xorNpJ2e3zGwZuuzTCSBVROud47bvdWLG3HB+vDohh9jEganHx5rI92HCgEk/+Zxv+u/kwDlbUY8/RWsMWFy4dS4rXFx58DSTPlaYnOLUCRs9iBADLfj6GH3Yf1z0P50rzksUoKbz66qvo1q0b0tLSMHLkSKxcuVJ32zfeeANnnnkm2rRpgzZt2qC4uNhwe6JlQQaj+FHnbp4Vbdk+VomiqWKM9IKObTofdJ/JX+9matUYIXLZme2VBgSEkd/IlaYTYxTp6jYFLVjsUERjZSf2enfo/0aB6nolGrx+PYtR00+1vqA1jn2toLXKKfdFa51TMBI8Lq6JLgmjJuejjz7CtGnT8Oijj2Lt2rUYNGgQJkyYgLKyMuH2S5YswdVXX43Fixdj+fLlKCoqwvjx43Hw4MEmHjmRDMiVFk+a573UEw2NhZ1wzLpsGoue1tD7nJu1GDXa/SHY3diVFu4qMwy+NsigMmLnkfCaTSLRwt4/9r1UWrGI0FqxQmOVhZW7HckQRrKsqSjOZqXJYdsCsQkb9l5QjFESeOGFFzBlyhTceOON6N+/P1577TVkZGTg7bffFm4/d+5c3HHHHRg8eDD69u2LN998E36/HwsXLmzikRPJoCW50mLprB5PmuuttFkTM3B2AqhvImEUrcXIbIxRY3uoiSZDs5WvgYDw8Rmk62stWqowihBVvSMojNjPbqg3m4wGjw9ur58TnOx7ebTGBZ9fFlqHjCxGIqEZj2eRLMvccyDSIX0+mXNpcjFGmvfcF7wveiLZ6F67PKzFqGXEGDWbrDS32401a9Zg+vTp6jJJklBcXIzly5ebOkZdXR08Hg/y8/N1t3G5XHC5Qr8UqqqqYh80kVTYrDS/X2626ftPfrUVby7bi8/vGI0hXdskZQzN884lzmLEigG9eJN4ozc32awSgPCJ2rQwauRkJnK/mM1KU/b361iMvtxwCHd9uE54bCOrFBBwcTZ4fLwrLXjsKe+txsLtZbBY+PuqtRj9as4yHCivw6qHi5FmtzLb6ccYSZbwcTX2HgPA9M82Yd6q/Vh8/9no3i5TuI2ssRDx91X8f2VbQN9iZPTDrCWm6zcbi9GxY8fg8/nQoUMHbnmHDh1QWlqqsxfPH/7wB3Tq1AnFxcW628ycORO5ubnqX1FRUaPGTSQPK/PpburU5Xjy5rK9AIA/f7MjaWNorhYjK9MwTxR4GyusGGgqi5He+PXEn1lXWmO/GyLriZHFSDt5egyCr7WiCAhZuPTEF9cdvsETlgUHAAu3B8IvtMOs1wijbYerUO3yYvPBSm47XVeazy8UB9G2MRExL5hZ+frSPQDAtftQ0BbC9OmUUNCOUXm/9K6r1iDGkIRRM2bWrFmYN28ePv/8c6SlpeluN336dFRWVqp/+/cnLs2XSCys+bqx7gKiecKKBr2Hfiywwqi6wRNX0aWHnn7RizFqKlea6L4aF3gMzzrjKl/rjFt5L5W5V08Y+TRWEtYCpRdcrMBaRk7U6Qel67rSdNL14/nDzKg3n19rMTKZlRbJYlRnkMTAtwRpGc/ZZuNKa9euHaxWK44c4VMojxw5gsLCQsN9n3vuOcyaNQvffvstTj31VMNtnU4nnE5no8dLJB92wmjOFqNUQPTrtDnAuk/r3D7OHdIY3F5+kmnw+JHuSGytGlknD8vOmEZZl7FZwdPYuBCRMDJ2pYWn6/NWHfG40x1WVDd4VetLpBgjIDDRiyxGepi1/umJbK9fhlUw/nh2nVfOLbLi8kU2/RGz0hxWKVAuwWcsjAwtRlTHKHk4HA4MGzaMC5xWAqlHjRqlu9+zzz6LJ554AvPnz8fw4cObYqhEisD+kDabukyIaYwr7Wi1C7uYDCG/X8a6khOoavBgzS8nDH/5ry050aju9ewkXOuKX8kBbcBxY7qnVzd4sOlApWp12nWkGmXVDdiwv4IbsxmL0U97j6vHMR9jpC+gDlbUC3vBsROuyEoQdfC1Qbq+QnpQ1J6o8+CX47WmxMaWQ1UoZXrIGV0rYK5pKqAfV7bxQCWO1bjDlnv9Mg6cqEPJ8TpsL63CidrwbXTP5fVhzS8nIp4bAFhNFuh7xo8BCHze1uwLHE+pyB3RYmQQY8R+F9j9XV4f1pbof79TmWZjMQKAadOm4frrr8fw4cMxYsQIzJ49G7W1tbjxxhsBANdddx06d+6MmTNnAgCeeeYZzJgxAx988AG6deumxiJlZWUhKysraddBND1m4y0IMVIjlNFpT30LAPj+wXNQlJ+BuStL8MgXm9X1d43rjWnnnRy230sLd+Hlhbtw6dDOeOGKwTGdm/1lb/RwjxatGKiq96BDjr6L3ogL5yzDvuN1+PsNp6FX+yyc9+JSdd2Azjnq//XcdawwuuaNFfjr5KE4f2BH0xYjIyvKmFmLAABrHi5G26yQJZ211kQbYyQMvmZdPsHJVXu9rEVu7J+XYGjXPN1zKNwxdy1/rgjCqDKYAZdut3LWI+3VGIl1kZBs8PhwxjOL1dc2yYKfn55kOBaFP/xzI75Yf8jUubVtWURtWqa8txrVQcGdZg9Y4ZTt3ME0fptk4T4XRj8q9JrIPvDJRny54RDuKe6Ne4rDv9+pTLOxGAHAlVdeieeeew4zZszA4MGDsX79esyfP18NyC4pKcHhw4fV7f/617/C7XbjN7/5DTp27Kj+Pffcc8m6BKIJ0UtVJWIgDp40pWXE3J9+4Za/vHCXcPs5iwLLP1sbe90xdhIxcgdEi3aCbUz9ln3H6wAA/95wiGurAQCbD4ayYvW0hlaEvPbdbgDm3Rq6KdrMd2bl3nLddWJXmv75FMGmFD0MWIzCj12tmYzTNW5QkWUmEpGsaMq522Y5hMsVoo1Xq9FcSzSufVYUsedmv5KKiDSOMQr8/6c9ofcyTcdilJNu585p9KNCr1falxsC4/7Lkt26+6YqzcpiBABTp07F1KlTheuWLFnCvd63b1/iB0SkLHyaLgmjZKMYnczGKccjnpmzGMWxCna0/bvMEClmRs8Koz23UpjQbIaQXsYUu/+ushqcrzMW0bCMUukVIZbptMHldetWvtYWWMzQxHDFYgH0+v2mfiTlpttx4EQ9Myb+HkUrjOIbfC2OYbJZLQJXGh9zpCUt2MNN2xIkJ82GcsbdZ2gx4lqChF9nc/xR2qwsRgQRDaJfS0RsxCP0WjmGkZsl3rBm/rhajLTCKA6fr0iH0FuvtVYdrdEXRqIENlZY6aV0a6tIR5rsjF1pgeMqQic8QDqwXiuMtNl31THEdelVptaSk8ZbTMKEUZQlGuKZraWcm/VuK/cv3JXGbCMQ70oyglYYZadFYzEyTtcnYUQQKYQsG/9aSgX2HavFP376JeXTXONRuVe1GDX6SOZhzfxm+72ZeU/02lQ0hkgFC1m+2ngIq/cFXCLagGKPT8bM/27D/7aEN0EVnYGNv2NFAzuRRiuMDHulBffNdAQcFi5N5WuvjsXIrZnYYym/oNfLTEtOOu9MUWJvalxevLd8H0rK66I6r9FYS47X4X3N583nl/HBihIuaSE0lvBjHa124Z0f9qKCqRKuLfDoU+97yPKmCBllnXKPtddf6/bC6/Nj7opfsPtojebaIvdKW1dyAl+sC7nEXV4f3v1xH/ZHeR+bimbnSiMIs7DP7lRNIz37uSUAApP2LWf1NNy2CQ0tYcQzWb8pav4osBOS2Yayynvi9vpx0xndhduENTaNQ2E7s5a07aVVmPrBOgDAvlkXCD/bf/tuj3BfWQ5MglwpC01FZGdwVmAF056jtZBlWRXIES1Ghr3SAuuUYGpt5WvlvMc1TVzjcY8DdY2Mx+6wSmFlHZQJ/8mvtqqFFqPByMI07oUl8PhknKh1465xvQEA/1yzH//3+Sbh9orlk/2xcsv7a7DtcBVGMWLY6/MLe6WlO6yoDVqA8jLswXXBliDB70u2U2Mxcvnw/k+/4PF/bwUQ+NyFro2pE6XzHv36Lz8CALq1y8Tgojw8O38H3lq2F+/+uA+L7j9buE8yIYsR0WIRZWSkKqv3nYi8URJhDUbRCBt+grQE9w/frjEp+UbwWWnRudI2aaodsyTClWbmM+rzy9hzlM960v5Kv2mMWMzpbe/RCCPhcr+sTqaAmXgo8XJZDsW9ZDpDwkhblBEAGrzacTZeGHl8/ogZqml2CQ4rPzUq556/RdxloXNeOh6+oB+37M3rhqNzXjoAY4uRcp9/+PmYumyVwfNAVFJg2+FAkP7yPce544qegYqQuXFMN4zu2S64TtmHd3Mq1Lq9WLWPD8BX4F1pxp8LxdL2fjABY48ggy8VIGFEtFg4/3qKCyO7ie7byWzLwZ46GpHJTqDK+EWWkVgyjMzAZaVFGXzdJsOhuy5cGDV+0vb5Zd0ijgoen5+7JlmWwz7bMy7sb9gjLszaxVZE5lxp4SUJFCJdrt5nhJ040+02dTyieEDtPY6H1ddjwmKUZrfCYRMLIz1LmN1qwW9PP4lb1q1dBu44J2AFNuP2Y++ZkfWwzu0zJRI9mqB25b4qY5lyZg+1ybISaqAcV3v9dW6f7nvOudIiXKcjeL5UDx0gYUS0WLiGiinew8ee4g1u+fYqUQgjwbai3bXxJPGCNfObsRixD+w2GXbd7VxhrrR4BF/LEUWn2+fnrqnBI86yslv1H+3ayYttnMpbjPSLWMacQcfMrqzFiAu+Dp43XBjFwZXmF/cyY0l3hAsjJfZG77KtkkUtP6BgkyRVoLpFtZ4071ukTD+W6gZvxJg0t88P9mOpVMJWhLHTJqmNtlUxGtxB+/mpdXl1RXs0vdKsksR9XguyU7PLBAkjosXCpesn0GL06uKf8eb34pgOEWVVDXjgkw3YeKBCXWazSnjz+z34a4rW/GBlW6zCSDmG6AGbMGHExhiZSO8+XhsaR1aaDW8s3YNXF/+sLvvleC2mfbweWw9Vcfux17m/vA7TPl6P7aX8NiK4dGo5slXE4/Vz16Stj5MdDBDSTuws0z7egMp6D95athdzFu7iOsXf+9F6NdBaO5aqei8OnAhcm7apqpYn/7MNd85di1+O864S9rOTEQy+1jaRVcRZeBxX47/Dd89bj7vmrTPcJl1kMQrecz1BaLdKsFgs3H42q0VtYiyyGGm/R2tLKvDUf7Zy7kY9quo9EcXpnqO1+PeGUA0kLyOKAMBpt6qxZtqsNK0wcnn1g9bNxBgpNHh8uPndVerrTkFXY6pBwddEi8UvyHSJN2XVDWrX+2tHnQSnLXK/rK83HcYnaw5wlXU9Pj+e/M82AMAVw7twVYZTghjbq7APf8XqJHqeR9MiIRpYM78Z8cVuU93gxQsLdgIArhhehIJsJ678208orWoI28/DXOcNf1+J3Udrsb6kImJgKfsLW5bliL+4PT6ZS53W1pe55aweAIyF0Xc7j2LWf7fjw5UlYeuW7zmOi1/5AduemBj2namq9+D1pbvx7bYyU0U3/7PpMHq2z+KqmrPHzGCCr9mYHsWdp7UYTR7ZFc8H34/GsK6kwnC9026FUyfGSM8SpggMp01Sx223hixGYmHkh0Njm3jj+724bFiXiIH4VQ2eqLIYgYD4YUWM0yYxjXmDwkgZu80Cu9XCiWO9chfsZzDS53f9/gos2XFUfR3tNTQVZDEiWix8jFFiXGkN7tBxzZ5CeUiWVYUmYfY5mKoZdArRtFcR/fIVBl9rHqjaB2YsmWyyLHMTkhKgagQrjA4zPbaUYnciUQTw5SB2B4OjzQSWsuPz+c0IIz/n0mItRu/eNAK3nx2IadEGD2vZanAvFMEu6ge3v7xetIsu2mwsxU1msTCVr7XZUxqL0a1je2Du70aq15Zo0u1SmMUkFGMk3sfGCCN2mSKYRFlpet/zijpPRGFU5/aZaqLL4vXJ6g8FycKPL5SuH3SzWSX88NC5+OdtoT6kInHX4PFxFcojPbu0mYbxcI8mAhJGRItFFmRkJBKzDyrFilJWLZ5kY80ASyTsMKIKvhYKo/BlWuuAtst5LDVrPD6ZG/eBE/VqLyw9WGFUWlnPLTdMQVdjUELbdMqN3DuNvW6vT44YlOr2+bkgaFYYndW7HWzBCV0b72J0Xj1Ewdcntc2IuB93DM09UyxrdikkPsJcaX4Zshy6F3npDozpFbq2eNExNw09CjLDlouCr5XYGz3BooyN7SnIxRgJBIBe3GNVvSfij6w6t9ew7YoInz/0Q8Fps8JiCbn6tAHvDpuE9tlpGN4tH/ZgwHSDoHmt1gobSehov3+pmhRDwohosbAPsURZYdh4GbMuJmUSOFQREkbsnuzDIlXKDGgnrlj2U/4v2ls7UWvN9vUxtH9g3WhKkGckq5GexehoTQPXIkKLck/YfczET7ATZoPXFzFGI2AxCt0bxY1ht1q4AHkjVxpgrjyC9n2urPci0xld9IV28lde26yheByPNzymxueXuUk6EdisFjX4mEUYY2TSlcbWhwrEGOm70vS+25Um4odqXb6o3VBef8hi5Az2SFOEm18gjBQU66PoM1OmLcIZQXBXaIQRWYwIoolhnxtGAuOX47WY9NL3+Nd6c81K/7PxMCbOXordR2s0Ad7mvuRejdka4F1H7GQSyy8qWZYx5b3VeOjTjVHvq4dPZ3xPfLUVQ59YgKkfrBVagth7okwqoslF+0DV9jXTWpD0eHHBTlz21x9R4/Jyk9GgLnkAgC2HIgijGrEwKqtyYeth/YBj5X1ig7K9fhlvLduLC+cs4/pOsbDX3eDxCXtNsXi8stBipHX9RBRGgl//LK8v3Y3Jb67gllU1eAx7ZonQfn6V1zbJoloiPBpXmrKdOklbE5OxaZOksDYjgFgYKWPR+zoqAoOzGFktajq8SOd4dA5W1eDlMgVF1Lm9UbvSvt91FMUvLAUQsigq16+m8ivp+sznSSklIvrMGFmMRIm2lXUe7vyp2sOShBHRYjErWh76dBO2Hq7C3fPWmzrunR+sxfbSatz/yQbu4WTWuiNqT6JXJC0WYVRW7cKCrUcwb9X+uLni9CxGH6woQXmtG19tPMxZMhS0TS0B8STh9vETgdZiFGmiUHhp4S6s+eUE3l62l3EbSKrL5HCFcYzMibqQ6GDN/kerXZyFT4svOCEcZmKQ6txePPHVVmw6WInXl4qzFllhVO+JXJ/GrRNjpK1bxE5sAzrnhB0nkmvy6a+3hy2rqvdE3bhV+53wMungipjTNpEFApOw8sOBFSn9O4ZfS6zYpJBwYcl02nQLPOoey8oLDSDgLlRcVSKMXGmRBGgsFqMfd4eKPypJImExRmrwdbjFSPQdVH5IKBZZ9tklcn0q36ksZygjMRUhYUS0WMxmpWnNu2apqPPw7RRMPqhE29V7vMx6JiA3hl9UrNspXq44Pfce60oUmdpFFiMzrjTtBGzWYqSw6WClGvDqtEnqgzhSyn6dzoR0tMaFBgP3k3J/2MmOLShZpdPwlLcYRa6x49HEGNXqWIzY1xP6F2LDjPFq+wcAqHFF/5mvavBE3YhX68L2iFxpPn9YvEydxyt06/xr6hi8es3QaIcuxGaVhK609tlOXVea7rGCAoMVqJJkMSy06fGJ0/KrGiIL0FgsRixai5FWGLHCULkXIjGtWIwUtzF7n0T3VnnWZqWRMCKIpGA2+DrWlNFAYTo+q8gMIrHDPgi5InsxZNOxIqIxD08WPl5LfM0iU7swxsiEK037i9msxUhh55HqkMXIblVTwyMVedSb+I9Wu9BgMFkpwoi93oq6yCUIuBgjd/QxRjUNkV1pdpuE3Aw7dw9FgbSRqKr3hrk4I6G1joZcaWzwtRy2Xa3Lx0zSoRIYdquE9jnxKWVhZ2KAWAqynWHB626vcWC86krTHE90fAWv3y88ZlW9N6IArXH5GvWjRxtjpK1jxMUYqcKIr7gOhIRR57w0bn9AXK8s1Mg2IIzIlUYQTQz73DASGLGKB59fjp/FiBVGOtYZs8dnhVG8qhRox1Tv9gXr7rAWI+MidsauND7WRlu4MNqJ/JfjdZwrTQkajtQWRO+X+tFql6HVyuvzo8HDCxvWOqU3qUbrStNajGqC16N1CXHCSHWFNO7DEJPFKMyVxliM2Kw0zYeizu0VutKA8D5esWKTLLAJXF0F2U5hur5enBgQuv9aK4mRxUgvC7GqwRNRgNa5vaabDotQXGmSGmPE145yClxp7Hdd+b9qMcpN5/YHjEWP6kpLUBmVxkIFHokWi9kmsrFajLx+v2biN/clF42FnXBYdww7UZpta8K1ePD7ATR+ImHv0csLd+HbbWV4+4bh3DYiV5qo/5PYlRZYeqLWjRFPfxvmgoklK015aDtsknmLkY4rrazaJRQWShG8o9UuDHj0G13xqq3fosAKKa9fjuhCqW7gg8r1XGl8VlHjgpfzMx0or3WbcvFo0VpHlfc1EHwdjDHyhrvSOIuRRhgp1obGohd8XZDtDEsr/6W8DqfPXKh7LCWWKDqLkQyXL/x+VtVHFqC1jbUY2cQWI5EYFZV+8Pj8cNgkNcZIcaUp+4t6+LGEXGlkMSKIJoUr8GjwBYzVYuT1yzFZdCJZjNiHhcgVFYmGBFiM2HN/u60MAPCHTzdx20RKSVYmP2FWWnDl/7aWCh+WRvE9LOwD/UgwENphldTJNGKMkc76WpdXaDFKtwcE15cbDhm+/0f1hJHmnok6p7NoA8BrmHR9FrZys1HfNDO0zQw0062q9xoGBYvqNmnvyf4Tge7q7bPTuKw07Y8TzmKkGX+GM3qhP75/B3TNz8CZvdupy2wGrjTtOTfsrzA8vl1N1+eXiyxSCl6f2JVWUeeJaN2LpY4RS1iMkayNMQrdY1GGo/I8VdzFXfMD9a0aPAHLaSTBo1hwfcGaVakGCSOixcI1kTVQCLH6ub0+mU+tN13HKHwsdToB094YhBf7UI1XxW9hM1hBJpEWzqKmWIxErrSg8LFA/AvbrMWID34OWVOUyVQvuFq7T9hx/bJwXZo91NbCCLbKOUu4MDIOitb2HgtlpRnEGEUpjP552yhcPaJIfZ0fFEaV9R7DbLav7z4Td43rzS3Tfv6UcgandMpRM588Pjnsx0mtO34Wo6d/PRCvXzccSx88BxcM7Kgut1nFFqO2meHB15EI1TGShMtFeHyyUEDoVVdnqXX7DF1p3dtlqmJFhOJKUz43arVxwT0XfX4UF5ji1i3Kz1DF5LEaV8TnThZTDysVrUYkjIgWi9kmsrH66j0+rSstdosRK4xYvzsXw2TyAcJlpcUrXV9wHO19E7nS2G0UIWUYfK0zj5gJvvb7Zc5KqFiHrJJFnUyNXEGybOzKElXNTlf6fUX4+X681i38fGjLFFTVGwu3fRphVKtjMdIGX0eD02blev61M9m3zy7I8tJesyKM+nfK4WOMtBYjl1eYIQWErHRmYe8Nd190ssYcNilqYRSKMRIvF6EXfB2pOjsQuD9GzxuLxfjcyvUpOk45lksgjET3wuPzQ5ZlNREgL8OupuwfrXZFFDtZjNUvFTPTSBgRLRYuxoj5oh6rcakdxAH+4V3v9mFdyQlT5t1YXWmRBBQf0B299YfLSmPScNf8cgJenx/Ha1yYv7kUu4/WmDpe4NwiYcS/3rC/MqwZrFfgFjQKvtZ7lDd4AuM2qlytHSMrGjKDD2IlduNIVQN+LqvmtndrhK6WijqBMApO0pEmAp9fxvr9J3CIqaPk9vrx0+5ybrvqCGn0vxyv417rFXhkX0cbY+S0S1xcSaYzvOChCLtVCivqx77/fr+s9mjr3ylHHeOJOjfWlZzg9qtlMvS055YkS1QB2HrWD5vVEhYTpBCppYoWxfKitRAZxhiZaAGjh8hixF6n1WJcKkD5oaOMe1dZDSrq3Oo958SkwGLkDTYzVr7TOWl2tOOEkfF1sRXUUzEzjYQR0WLhs9JCL4Y/+S3Gv7gU+4JNPtkHzLVvrcCv//Ij5q3aH/H42qafjbEYcet1jhlLjJGyz8NfbMJlf/0Rzy/Yid+9txq3/WMNJry4VLe+jhZRgLp22Yvf7sQZzyzilpkPvg4KI0HtEyAg9n41ZxnOf+l7bDkkrkCtFY61jJspQ7EYBbN9zv7zEhS/sBQHToSERqRMINEv+bQorBeX/XU5Rs9apIruJ77aio9W85+zSoH4YmGrcQOMK81EVppZnDZeGNmsEnLS7AZ7KOcJFxrse3Kwoh41Li8cVgk9C7LUyfdIlQv7NIKPsxgJREpGFO40TiRqrktPPLAxNmZQBJAUTVaaXw6zGEaiS5tAkHMgxoj/JqUx1yZZxBl3CorbmxVuv3ltuTgrTcdipDw77FYL0uwSCoKWxaM1rjCxo4xbgXWHpmJmGgkjosXC1zEK//JtOFARXBfabvUvgV+uZoSRdl+zFp1IRRtZESdKkY2EKPj649UHAAB/XbIbe4OC0OuXsb+8Lmx/4ZiFFiNRdh3/oBdVBjdypenNI/UenyoKPl8rbt2ivT+saFAexO5gwKtiVVuy4ygzdrEbS/nFLHSlRenWAULuivd/+iVsnah6uBG6WWmC4Osv7hyD8wcURjym02aFk7kuh1VCTrqxEFF6tWktIOyPACV2pmNeGlf5mj93YJlRjBEA1QJoBl1hJIUHX//j5pGBfWzRWdkUkac9nlHjW6/Pb7o58ge/G4lfndoRsy49FYA4Ky2dsaJZLOHuVRbF7c2O9+eyGtPB1x6frLp9c9LssFgsGlda6LouGNgRs68czO2fZpe44PtUg4QR0WIxW/m6MWmvnhgsOpEsRj6dopFmj18flq7PU6VpdWEG0bnNxC9x16L2SgvfTpkg9C6RFXs7jlQLt9GWM2DdTOykwRZeZIOZ9eKLlElYW1sJCDzgI6FkdSlE6lNmhnZZDm5MhgUeg+sGF+Xhr78dFlHMhVmMJEtEixHb3oOF/d4pnzXFsiCacHPSA+epdelnpQHRWoxCkz+bradN13/1mqE4I5i1JjonAFw36iThcquOK82w8rXfvCttdK92eOWaoWpAtaiOEWu9lCwWQ1GmfN+04xMVeNRL11csRsp7JhJG2U4bXp08NKyhstNmDQv8TiVIGBEtFi5dXzDjKm6bxggjHxcDFHtWGotHN8Yo+qw0kVWHPYxpYRThOHpoY0z0jqU8SLUTRXaw3gknjEp1hJFOjJFVCrSfUCY7tiP43mN1YduzBNwE+kIi3USsS6EmjV0RrmZElR6d2wQmSLYuEAvvCuHXRbK2OO0SZzGyWSV18tODrUnEwr4nqjAKTqAii5HyfrPWOaHFKJoYIx2LkbbyNft/vZgqPVFpk8QWI+MYI3HwtRFqdqXbF/Z5Z8dmjdCORBFG2vEpx4zkivX6Q82Mc4LvmfK+llW7QhXOg6JU+x1yksWIIJIDKwreWrYXxzS1ZJRngmiiNmtI54KLTf7yiRxjJLZ0RXLV7TpSjQ9XlnBFDCOdS1RfZ/fRGsxd8Qt+3H0MX286DEAsHs0EqIvqGIkKVbpVYcRbbXKDEzIr9sqqXfD7ZcxbWYLtpaFg7HBhFDiW8gBWJhX2mnccCe0vshjZrZJhIK6ZGCPt/orIy0t3iDYXop3ktDEb2swzo4ktkrXFoblmh9WiTn566Akj5UfAml9O4ONgPFVIGIV/y7KDlqkvNxxSl4nuf4bTeDys6LTrxMvYrLx4MCOM9N5vZddoKl///Yd9KDHpylZgY3PWlVRw65ycxcg4K80VIcuTLxBq0mIUtATuL6/D33/YCyD0udAKSqct5Ep94/u9pmMdmwqqfE20WLTT9h/+uRFvXh+q1qzUzBHpDZ0Y4DBiqTMU6hdkFRYc1Ou/Fkl4nffiUt1z6SGyGI17/jvu9eL7zxa70kxcLxdjJAey+ES7KROq1hWTl2HHgRP1qHf7YJUs6jnfW74Pj/17KwBg36wLAIQLLiVmSDHZZzpsqKjzcNe8v7weDR4f0uxWHYuRxKWua4kkjMaeXBAmFhSLUV6G3VTNGiAw+bAtKcKEkWYCFsUYKeRnOgwnZJtVEHzNWIyy02xhhSiVzLcwi1GwUeplf/1RXaa60gQTrkiAibaLZDFKs1tVMa0fYyTBxxyaFTF6rrQ0uxVDu+ZhbUkFBnTOweaDQWEdfGAMPakNFm4vU7c3shhtOliJTQfFiQQs7FjS7IHMP9F3iA2+tugEX2c5bahxeTGye756PZHOqRt8zcQYAUBBdkDoby+txvagVVe5906bBIsllJHqtFlV4fbhyhIcrXZxz+ZkQ8KIaLFoLRoLt5dxk7mRxcgsehlkhvsEt7u7uDf+t+WIGvAdOqZYbJkVXiyxCCMtB07U6QRfR3d+vyaLj0UVRpqJVbGq1Ht8sFos8AXl7k97+DR3IDz4ulaTsaWkeGuvud4dEEa6FiMDl5eea+Wuc3vBabdi8siuuOej9dw6xWIUyT3FkqsVRpqYDa0Fxchi9KeLT8G/1h/CW8v26p6PFYM2Kx9j1K9jDlbu5e+/YpUJizHyy9h7jC8LYcaVpp5bEqfUR7J6pdmsAEJZUwoOLsbIAi8zBPY8etmR6XYJr107DJ+tPYgLBnbEmc8u5tZPObMHnDYJZ51cEDyHOadMQbYTt4/tibUlJ/DVxoCV9sYx3dAxNw3n9Q8FzFssFrxwxeCwzxTAu3WtkoW77l+d2hH3FPeGw2rFfzcfxuTTA7FS3dtl4p7i3pj97S7uWHq1nxQCwdeKxSjwXojeE+W7J0kW5Gc4cDz4GWYtRgDw7bYjotuSNMiVRrRYtJYgm2ThxIXy7BNN+mZdaXVu40BnEYqYOqltJu48p1f4eq7AY+OqWMdDGPnl6OKwuEw9H28xilYYhVxpPt14K+V82vujBiYrFqOgeNBesxJvoViYtH3GjFxpesKoS34G7jynF/IyHGH7KxYjo6whLawlJdNhRRtNQLfWgsLXMeLPf2qXPDzyq/44o1c76MGKQW1W2uCivLDt9V1pMrYc4mtPqcJIFHytCfLW+9ESKU4qzS6+fr6OkcQJF60bTHxcK9pnp+G2sT3V6wBCzwuHTcLvzuyBkztkB45pYDFiGda1DW46ozvyMkLXX5iThlvO6onu7TK5bSecIs4sTLNpXGnMtd02tid6tc9G17YZuHVsT67y9OSRfEC5wypxwlDcEoRxpQXfM9F3gb3f7P0KxBilrvxI3ZERRCPRBh7nZzo0Vpfom2tqrVBsppL5liChgFnRg9OTAIuRNtZBediZEkb+8JYNRnC1nTSVr/XukWJpcGmEU25woqhz+zgLFVtl+3ht4Bq0x1ZcKcqv1kxBjBF7LKWOETs5223GrjS94GsuE0qzv2Ix8njN31PWulSQ7Qw7pvbXeqTKxUB4w1MWbVYaO5EO6pIXtr0yyWnTzz0+v1rUkR1/YB9RjBF/HXofu4gWI7bcgE6GlVHwtR7s+23jLEzi7Y1ijFiUMbKfPT2Lom5gOJeub+FijLT1lbjjGWQ0itYDmnT94DhFbjn2+jlhZLOG3ZtyTXHYZELCiGg2fLxqP56Zv91000Gtjqh1eTkLjMlnFofWcsLGWkTrSrNKFmGApNcn41iNC/d9vAGr94VcFnKUlht2TNqHXc+CLAC8MPq5rAYPfLIh7Bgub3jLBiP0il76/LJuMTePTw642jRiIZdJ32Zhe48p16AnHJUHsDKZRrIY5TLWkViDr9lf6+HB1+J4KiO0wkib0aa1oPBWEvEH3WjS5l1pEjdpndwhK2x7NcbIF24x2qpnMRK4mcwWzIwUY8QGIuvFGFmAqIVRmibzK3Qs8b6SZDEVr6gKI+Z91iuRYNX5QcV+JiQLf91G16at2aR9Vog+/+v3V6jFSRVrZlQWI1u4xcioqn1TQ8KIaDY8+OlG/HXJbqzVZGPooRVQtW4f14vKaKrXizHQTr6cxSjK4GubJAljELx+P+79aD0+XXsA7y7/JWxdNCgWG+1DqEfQPF/t8qoWjKte/wmfrDkQdgxRlV0j9CxeAVea/nHcPn9YJeC84EShrSF0uDLUWkMVRjpCQ6nnkh20erD7AqG6QkqPuWzWYhQhxkgv5Z6zGGm2Uc7DCsiJOu4RhQKmX1lAGBlbjNiPr16vNCMrAp+VJmFUj7YAAjWZROJF+Xz96tSO3HKPzx/W303pvSayWGmDyvWImJWmE2PFTvp+uXHCSO8ZocWM1UhsMdK/RpEVRyva2OsxGoJRcVDRegB4+4dQfFrXtoFnSZojfDv2h5/WYqQV7FoBnUxIGBHNDrZAnxHKFHzJ4E7qskPMpChqcxEJbYxMDWcxMhljxFiMRA9jr1/G97uOCfeN1WKkfbh1bpOuTp6K1UtbzkCh1h1eZdcI9h6x9zjgStO/Rx5BXRcl5kIrjNgK0Yow0hNdysNZ6eW0v1wjjIKuNMVyxLqNAjFGfOwGi16MkZWzGGlcaV5eGP3+3F647eyewuMoDOmap/6/IMsZZk3QWoy4GBGdWA6jCZsVfDarBSO65+OjW07H/HvOEk6UyrLfDO2CD6ecjn9PPQNA4POnlE2Yc/UQLLhXvL9CfqZT3deISBYjvXRz9v9+WdZN1weAFf83Dp/fMZq7F3rvt5FGMiO4lHGxYsioqKbInaYt8MgKDyO3qZ6b3ehcyvf03L7tcWavUFFM7X1gnwWsuHfaJa4A5Q2ju2GSRlQnExJGRLPD7CStxBid0bsAJ7UNFMRjrQVGcTN6jxFtHEujLEZWi9DNYSweYnSlac6Tm25XBUCkGiKROnlrYcVNVBYjb7gwUlxpRl3vlZghPWua4rIp0OkSrwgiRSCxIkPrSsvSWCp0C/4x91trVQpZjAL3YuzJBWiTYZyhNrgoT53ACrKdYdaEMIsR8389IWI0YWtdaRaLBSN7tEVBtlP4mVWsUpJkwaiebdEmM3A9Xr+sukGHdM1D72BQsh51bi8GdskNCzjWEslixFrDWFeRzWpgMdLM6h1y0jCkaxvO0qErjAzGYiYzzRlFjBEgFivpGmuW2cByi8VimJ5v1EB4XL/2quiyWCxh94f93uYzCQMBV1poTNeM7IrOeeashU0BCSOi2WF2jla2kyyhSZFtwunzy6bjlRS0MTLVrlhijEJVZ0WTk6gnV6RzRLs8J92uPoSrDM4HRG8xcnMxRuz/xVVu7Ux8ijZGJddEEcSQK83YYsROcCyqxSjoUmOL6GmFUbbmV7xeTIzdyGIUdF0qItBuoklrUZsMtdxAQbbAYmTgStMTQMbCiM/MYxG55sK2YYKxFeGpHaMI5YdGpFIGkSxG7KXpCUNZljnBoHc/2meHKpfHUq3clMXIpmROhq7LqKim2JXGCiGYDr4G9Esa6J1Lb51WGNUzwoj9zAZcaaF9M6KoZN4UkDAimgWsgDErZpTtJKbB4eEKXhhFEwALCCxGjLWFtY78d9NhTJy9FLuOVGPGvzbjt2+uUMWFj2njIHpob1KKxgm496P1uO39NWH3QK+1wN5jtTj/pe9xSNOVPSfNpk4+kRqX1rm8UWelfbv1CCbOXspdS53bi4tf/SFsezbVW8+VZkTIlaYTYyRFEEYexWIUnMAZa4TdxrfH0FqM9CZdG5eVxm/z3P92YsRT3+JgRb16DG02lhZJsqjjKsh2IsNh5SbcDK0rzUTGpVmLkdYeIpootfdBdGztGEUoruncCMIoUlYa68rUcxn6ZRlWa2RhxFoa9YSwke4wFWOk3r/QtloRzm1vypXGBGNHmOn1qoPrnUtvnfb+sBYjVuw6bHxJADOiuSlJrdEQhA4+jUvGDIorzWIJTbBKajcQEDJ6rh29B53WcsJ+8dl1t89dCwC48Z1VOHAiMAHuKK1G/045EWOM9hytCVum8N3OQDf4/eX16Bp0DwL6wujJ/2wTLg9YHYKutIgWoyiDr70yfvfeagBQK+ACwKLtZcJxOmwS6oLd1LXp3pEmSCAkjPTGqLhP9C1GvCuNncDtEl/HKFcj1PTaLtgNhBHA92tz2AINP0/plIMth6q4Ct8AMDVY62pg51yU17rRv2MuLJZAm44TdYH3TjuxDCrKBQAU5eu7J1grwsDOudh0sBLXB5uksgHj2rIXRjFGCloxYJMshpYHhQuCcSbTzjsZS3cexVWnFQm369U+C+l2K9cwmaVvYTa+3XYE7bIcukHSfhmGMUYK7bJDVkt9YaQvfqKxGPXvmAOnTUL7HKexIIkQfG2xRFeKwKjuVTTj0HMbA4HPmMMmoVNuGqySBR7mu25GNDclzU4Yvfrqq/jzn/+M0tJSDBo0CHPmzMGIESN0t//kk0/wyCOPYN++fejduzeeeeYZTJo0qQlHTMQD1hpj2pUW/N5JllAAbY3Lx6zX726t94tba5XgOtkLRJYiioDQJMpnpYWfRysORGgnK5dPPwZHRP+OOYzFyFgYads/aNFOUHpWOKdNgqj9q4Nxu2jfDzPVoZUYI0+EdH39GCM++NrIldZR0xBWzxrABV9HSEFXJqXP7xiDX47Xoig/Ay6vH9lOG6oaPKo4/Otvh6HW7VVdEjnpdlUYaV0R2Wl2bHl8guGkxo79nD4F+MfNI9XYJW2QMn9tghgjrTAK689mjZjFtfGx8eq1DS7Kw4YZ43UzswqynVjxx3H4ZnMpHvjnxsCYbZL6+cl02rD1TxMMBYHPL3PiUG9b1gVkpmmwFjMWI+W46Q4r1j5ynmGfM8CMxYhvExPJlWYYY2TkSotgMWKfBekOK9bPOE+9z+w6M6K5KUmt0UTgo48+wrRp0/Doo49i7dq1GDRoECZMmICysjLh9j/++COuvvpq3HzzzVi3bh0uueQSXHLJJdi8eXMTj5xoLP4YXGmsxUj5BVyrCZaOtru1NsC6nrMYGR9LORdfxyi2r6BWoEUblJ2X4WBijIyFTyRhpH046rm06nUCqJX9RTFGaTYpYlxHpHR9ZdLOTbcLA4dVi5FHx5XGuJXaa6xOeu8fOxka1UFix+ewSejdIRtpdity0+2QJAvyMkIWD6vEt+ZgBVymIBg502kzzACTOIuChNwMu3oudp0Za6HIQqQdixF2TdsRANx4ROSk2TnXJhvfYpMsyHDYDItzypqsND0Bk8W4OdMivJcirBFEDsALikyn8bgBsTCyayxE7GczqhijRrjS9ILTFdj3hH32mi190FQ0K2H0wgsvYMqUKbjxxhvRv39/vPbaa8jIyMDbb78t3P6ll17CxIkT8cADD6Bfv3544oknMHToULzyyitNPHKisfAWIxkNHh9kWRZOtl6fHy6vT03XZy1GrDDyy9ELI+2kz1p3alw+Q9Hm8fkDrS2Cx7BJFlO/JkUoFhrl+qO9DiCUGhzJYhRpvVGKLouoYS4Qerh6vP6wfW1WSfdhq9TDqW4I1GJSAne1QoTt19SOsRopdY1cHj/q3T41jZ4N7LVbLZxbSeuO03v/WEESaZKLtTUCO5nFErzKiQKDyVtrMRKhTQfXWl8ijc9s6wwtacxx2c+JUXq6ggzAysXhiPdhY330hLBxjFHk99dsYUsFkYVF0liIEuFKO7VLruE4orGo6T0nUoFmI4zcbjfWrFmD4uJidZkkSSguLsby5cuF+yxfvpzbHgAmTJiguz0AuFwuVFVVcX9E8mE7yx+qaEDfR+bj1Mf/h4GPfYOXF4YaIPr9Mi54eRkmvLhU/eJJltBkqVQ3BpTg6+hcUEZtP97+YS9+/+E63fUHTtSj/4z5qkDQizEyQ4PHj5/2HEe/GfPx0re7ohJGvdsHKhcrLppIMUaRLEbaB7/H5zcVNK1g5EoD9H+Fts10qA/tR/+1BTP+tQVAeIC0XluCvGBK+foDFRj42DfYeKASAJ8KbtPEGGmFkd77ZxR8rSVWN4KNE1+Ny5YysihEsjYEjqUZW5QWI7PNVrWw/cG0lZ8jYZMkPitN5zojlVIAxFW81eOacaVFK4wE77e2RYlk4toU9IpgAnzPyYGdNcIorEq2+euI5cdcU9FshNGxY8fg8/nQoUMHbnmHDh1QWloq3Ke0tDSq7QFg5syZyM3NVf+KisTBf0TTwgZcf7QqUIq+usELr1/GCwt2quv2Ha/FjiPV2He8DieCbQwsltDkVuvig6V143l0niORahUpnbFFv5A/WFHCxUfZrBbDB6oR9R4fHv4i4BJ+8dudph8y7bKceHXyUACh2IlIWWmRhNPIHvkYflIb9bXbK6NNRuQ0ewVl0qxze4XXodt2w2pR44aU9gTs8dTtmHt8yeDOyHRY0bt9Fk47KR8A8J+Nh7n3NYsJBLVKfIHHgiw+xkiv3QaXrh/BFahtyWAWriVFDK4IawQ30k1jumNA5xzdpqXcsTTn1wb/RrIYxfj7gLNQsJ8TI/ffQ+f3Rc+CTNxxTk9O0OkJmPP6d8Cgojz89vSuYet+f24v9C3MxtUjw9cpmBGt0QojkZVR0liI2MuxRJGVph3LyR2yMLRrHn4zrEvYD54wV5rG2vr3G07TPaeZWMpk0eyCrxPN9OnTMW3aNPV1VVUViaMUgH3QNQgyUXx+GVbJwjWsVDLGJItFDYCt0dQdijrGyKT5NzfdHlaUUPuwtkoWU/EHIho8Ps6NaNby9dP0c1VLQ06cLEZOm4R/3j4aV72+HD/tKY9oMbp0SGd8tu6g+jqUHRelMJICZRiUtHeFMGHE3OObzuiOm87oDgB4+mtxxh6fCh7JYiSecaycxSgxrrRY3bAKkVwtMy7sb/pYIjcUm10XKR071lg7rio1Mykbuf9uG9sTt40NVBq3mAi+dtqs+NedY4Tr7hvfB/eN72M4RjOp6NHWRxKJLZvGAhiNxYitQ6XNELNZJXx2R+D6Z3+7k1unHQdr/Vz5f8Vok6n/AymVXWnNRhi1a9cOVqsVR44c4ZYfOXIEhYXiXzSFhYVRbQ8ATqcTTqc4e4VIHuwvelGK7t5jtejVPovrtxMSRqEvsLYqs17Qsl67ELPVrUUPLm1xSL2sNDM0eHycW9DMry+LhX/4q4IkQgyRXkq0gnINygTv8fkNH8RpGusBmx0nymjTi1uwWSWumq6CtvifnvDQ+yXPuuIkCz95hgVf68UYRRF8HetnIFY3rGj/Rh9L8H7bJQuU5j2RKlXHen7WusFOymbnXPasjb0HephJRY86xkjwmWLfA4vFXJFPBfY7YiTkwvuq6Y870jVFW0OuKWk2rjSHw4Fhw4Zh4cKF6jK/34+FCxdi1KhRwn1GjRrFbQ8ACxYs0N2eSF1YoaJkD7Gs3leOtSUnsIUTRgHhIFkswsnJKF1fTwCZ/ZUjOq42PslqEHwd6UFW7/ZxFqkVe8ojjkmyWLhJ3qzFKBLKr30HI4yMBKTWVM9W4BYJPL1f04rFSEu4K03PEiA+Luv2kSwWzsqoraukF7TMxf8YWAMsltgzchptMTJhLTF9LB2LkUKkStWRLBp6sJ8l9nxm+xZyY0iQMDJjMYq2DIDIhcu50jSuzEi3l6tCbSDktJ85IzdwpB8EHnKlxYdp06bh+uuvx/DhwzFixAjMnj0btbW1uPHGGwEA1113HTp37oyZM2cCAO6++26MHTsWzz//PC644ALMmzcPq1evxuuvv57MyyBigJ1oRb80HvpsU9gydReL2J3h9cu61hC9vltGwdcsojFqxYLNIPi6Q7YTR2tcuhatBo+PE18vMQHoemhdeYogqajzRN2clkVrMXL7ZN37B4QLnVB2nNiVphd/YbdKwtpEWZoq0voWI/Fx2UnKYtEE9UbIvlKwmXSlRdmRhuOktpkAjsa8P3stjRVZIlca3/LBeKrp2d64N5oeTp24oliMEY29B3qYyRiM1mIkcuE2xpUWu8XIXDkIEd0LMrHZoMp/MmlWwujKK6/E0aNHMWPGDJSWlmLw4MGYP3++GmBdUlICifnAjB49Gh988AEefvhh/N///R969+6NL774AgMGDEjWJRAxEssvQIVAjJHAYiTL2K1TZVpPAJl1pSkTfP+OOch0WrFq34mw+CSrZNG1FhRkO1HV4IXHJ47vqRdYzaKla9sMWCzA8Vo311w3WpTgZjuTdm8kIPUsRpX1IVfamF5tce3pgSrMenVUbFaLsJWGtseUbuyIjiWHzXSSLBZMPKUQVwzvghHd24Ztqxc8z/dKC/2/b2E2rhvVDf/3ebiQj5Z7zzsZx2vduHRI55j2N1P12Syi3TmLkY4V4tPbR+HtH/bh4Qv6xXRe9rPE9eiLQXGayb6LBVFG3qSBhfh6UygJKNrga5Ee4dL1Jf49ifT+OhjLj5GQ01q2jGocReLVa4bi2fk71HivVKJZCSMAmDp1KqZOnSpct2TJkrBll19+OS6//PIEj4pINGYFiQjJIrYa+PwyF5Nk5nxGlhAW5SH9t2uH4cn/bAUQ7oYz+oVakO3EgRP1qHGJ10eK+zFDltOGbm0zsfdYLdbvr4i4vd1qEVqwFOuIYt73+MLrEbFofx0rLr0TdW7VgvLqNUORF8xs0/tVapMswl/a6XYbbJJFfQ/13F16pn6tYLJZJTz7m0Fh21ks+sX72ImIPc+ffzMIA7vkxkUY5abbMefqITHvn+gYI/bzrWcxGnZSPoYFswNjgXUpsdZGvRhBLWaCrxuLSGi8cvVQjNy3UC1OGm3wtai8AZ+uz//oiuSu5SxGBvFg2mtpjDA6qW2mmiGbajSbGCOiddMYVw9b4FF7zC2HKqM6n2lXWvAh7bRJ6kOHbUcCGD+IC7Kd3ITajemLBgCuOAgjIGDRAoB1JRURt9Wb3JTrMBtjFCaMghajYzVs/zD9uioKNkkSCiO71cI9wKN1pbFCzMiKYIFRgUdGGNl511yqEE9hJHKb2LjJNjG9sNgJnxVGsVmM4jKkMERCQ5IsnHhLi6L+DyB+v7Sus2gsYGY73WvdbKnWyiNetMyrIlocjRFGFovYOlDt8mLPsVoA4cGhepYhM8HXHp9fjW+yWyX14VHj4oOcjX7FFWQ5uQm1V7Aoo0I8LEYA0L+TIoxOCNf3LcxW/x8pA4uLMYrGlRaMMTpW7VaXGfVuUsdjtQhdEDarhZuMog2+5lspCDcBEHj/2F/u7DjZ9zaW4otNAWvlibXAouhYoWNGthjFE9aVZtZixJKothR6QoMtKWCmUjeL6DPNWkYlS3RCz7TFiBG4DquUcq084kVqfmMJQkM0rjTtrynJYhGaqn8uq4EsA/mZDnTQNAdtTIwRmy3mYCxGDRHigrKZB5LWYtSzgBdGta74Woz0giAvHNRJ/b/eQ1AJBLVzFqMoXGkai5Fk4cWJXkqw3Soh3SFyKUjmLEYm3BdGz32txWhwUZ5wO3YbUXmBZMFbjGLfF9CvY6QQKSstHrAWo5M7ZBts2bToBTM35seeMAtQ4zqLRmw5TLaXYa9F9INF2zKkudLsYoyI1onZX4AXnNoRaTYrPl17QF1mgdhtcjw4ERdkOSGDP75ujJEJixHbj81hk0xXNs5Jt6M6uG9BtpN78GiF0VG94KMoUSxGoiy6P118Cq4e0RV//mYHAN7VxaLGGAWvM2LwtUbMKDFGyj3XCid9V5pF6IKwayxGRoX7ImHoSrMEBMHr1w6Dy+vH0p1HsXJveNkEiyVQAbiqwYNOeekRz9lU8MIoOmW0cNpY/G9rKZ7+envYsRRYcdtOUFYh3ri9fvx76hlYta8cl8QYkJ4IWCvLxYM74bKhXQAAjdBFnNj+v0l9Ma4f3+HBKln0ivcLMZuVxn6vRN/LySO7wuPzY3TPdlGcPfUgixHRLDBrMbpmRNewiVeSxHWMTtQFXFs56bawiUFPAJmzGAXEjcUSeICZrWzMZlgVZDs5U7s2nbmsqsHUMSPRPtuJtgIrRv+OObhuVDdTY1ce0myMkWHwtU1rMeIfxFr3mEO3VpAlrFgkEJjk2Ye7XtsOMwGvRq4CZeoZf0ohZ1kTcU7f9rh4cOpM1kDj0vW7tcvELWeFsokiudJEZRXijdvrx8AuubjpjO6mY6aawhHEfhYvH1aEs04uANBYi1Hos3vLWT3RsyBLk64fXZYdK2KN6hixsWKi75XNKuF3Z/ZQf3A1V0gYEc0Csw8Rq8CKIOnUMTpRF4hpyUmzh7kS9C1GkcehBFkrPnizAYps8cCCrDSukGVRPh98XVZtbDEye06LxSJ8iLFXqY1v0qINvg7UMTIIvtaIGbZ7ORCNxUgSxhhJFv4Brhc/Y8ZiZDi3aNY1t3ALbiJNQPC1yxty94oKccYbV4pWUmZdU1wRykYUsRKm61ss3P+jCahnf4QZWYzYWLHobFLNCxJGRNJYV3ICD/5zg66LhsWsxcgmWcJqbVh06hgpz4KcdHuYxUhPiGnbeoioC7rDlAndrMUohxFG7bIdXBXotpn8xFJe64YRZtoQKChxRnr0KTSO11ALPNpCFqNoKl87bLzA0VpyjIKvRVlpHp+fe4DrpeubwTD4Oux185oo+ODrxqbrhy9TLLKAcUBvvIilW3tTiFk9t24sAeKh4whi65g3IZCub/547L0zqqnEijyzpUuaIySMiKTx67/8iI9XH8DDn2+OuK3Zh4gkqG0jRbDa5KTZwiYG3eBrnxIHo388JfjaaVIYKce6YngRHFYJnfPSkeGwcb+4o02nNtOGQEFoMWJ+QV4zItA5vFNuGq4Y3iVsW8UMzwVfR6hjdP2oQPHGC07tCIB3I4YVkdO5f3ar2GLk9vo1Jn/x/h1yQgH32oBT5fNwZu8C3etobhYiLfFM1z+te3gtohMRxHsqcGqXvISfQ89iZNToNhIiIcu3eInu2ljXt5H1kP0u6VXlbwlQ8DWRdH7WqT7NEo3FKFwYBb7sDqskDDIOWIw0wki3JUhgebrdqptlpjR3VR4ikYKvlz54DkqO12F4t3x8dsdoNUtL2zfsh4fOxdcbD+Mpna7wLFlR/EKPZDEa06sd/nnbKHRtm4GcNDvOH9ARN76zSl0fijEK/Ovy+A0DS9PtVvzxgv4Yf0ohhp3URh2v4h4MtyjpNJGVxOn6Lp8f6fbIwdcF2U589fszkJtuxwUvf8+tWz59HErKaw2LDzY3C5EW9r7EajH6afo4HK6sxymdwrORGlOUtano1T4LX04dg3YJjIHStRg14vZE6k1ngQXd22Xiy6lj0NbEtcXS6d5MIkpzhYQRkXTMPJLNtgSxCiZLxffutOkIozR7WPCoXw5YqbS/npSHfbrdihMQN19VUukVF1CkeJ/22Wlonx2wXgzoHJpgtM1yO+elY4Tgl7kIbb8wI7q3y4TTJnFCTPtjdni30HnHnsxbUbQWo7oINZbS7VY4bBLG9AplrrCuv2iy0kQuUpfHz1md9Np2AKH7bdO8RwXZzohxMVqLUXOzILETaawxRoW5aSjUlLpobiTaasRajBoTcM0icg+LxJLZa4vF+uNpBsI3VsiVRiQdMxOK2R8ngWrI0dWtyUm3CR80ol+8akq5Qa0PJSvNYTXnStODdaUpmG02GU1Mh80qcYUcAYSVL2CRJD5+QdtEtt4t7u+mIHof2Jgg08LIKgmzDd0+PydGzcQYxeJKCosxiuIQiWo/EQ3xsBiZoaVWRzYL+9mOV8X6SBYjo++vCNEPxki0ZItR6/7EEimBGZeE2UA/qxTuiglZjMSiIpCVFj4G0a87D+NK0+PJ/wRcXdEGX2sR/SAz22wy2oJ62jijSOEPogakSvD1qn3iKtqAfhVydrxhwkivV5pV3ITX7fVzYsqMMIpFGDSm6m+qCaNEjqdNpj3yRi0Y9t5q3eMxH1Pw2WuMm84Tw7hasMGIhBGRfMxZjMym60th1hzFk6LXmiEnPdyVBogz0MwII4WQMIpt0nn2N6cCANd5PFK1ZqdNQqbDyhWE7FEQqIH0wIQ+uvud3ac99zrS3WYfwsr1ndwhPK3/jF58obcMu1UoKDIYC1d6WFaa+P7puchuGN2Ne6+NXGkKj190CgDg9rMjd/q+e1xvAMBTvx7ALb9+dDcAQLGm2J6IZy8LvLf3nXdyxG0TBR+sG39h9PtzewEAnrxkYNyPzTLz0sDx/zipX4Qtk0ePdoHv4NBgTB0APHNZYNzs99ssSkFHNj6QTdePNq5b+dxfmkKFMZMJxRgRScfML2+zwkhUDVl5YOi5ZHQtRgK/uxL3YyaGR7F0xNqB+orhRRjfv4PaZR4wtmx8evtoDOqSi3qPD/9cE6r8fdVpRbhyeFfkZuj/cp9wSiHWPXIehjyxAACflSYiUBsocC+U1OG+hTl46Py+mPXfUDXk924agbUlJ/Cb15YD4Gs1sbAWo/ACjzrB1xrBaZUsWPvwecjNsGMt0/vNTPzM+FMKsX7GebrjY7n3vJNx05juYfezb2EONjw6nmvtosclQzrjnD7tDd+TRJNoV9q0807GzWd05z6/ieDqEV1x/oDChJ+nMXxz71lwef1cUsSVp3XFhFNiG3dBthObHhvPfVdsjXClDenaBhtmjFf7FrZ2YroLu3btwuLFi1FWVga/5lf1jBkz4jIwovVgLvjafLq+Nt1beV44daw80cQYKaZwMzE8jXWlAQh7aBo1+2yX5YDNKiHbKnEPSafNamoCbhNFHy+9SbUz0/LCJgX6NeUx587RER5mY4xskkV9X7STuSzL6nXG0rg1mglK736aEVaRjtFUNKYliBksFkuTiZVUFkVA4Bkgeg40Ztzawqjcj7sY3FzJ/jymElELozfeeAO333472rVrh8LCQu7XvsViIWFERI0ZV1p06fr8A8jCZKWJyEmzC8vnK3FNDR4fHFYJkmRRA6LNWAUaG3wtwihehj2PxAmj6M8f6W6zooQdEytqlPGw9zYnTcdiZDIrzWmT4A3WidJmksk6+xBiOGHU3FLqiDDYubgFh/80CVELoyeffBJPPfUU/vCHPyRiPEQrJL4xRsbp+iKyBQUegUAxx8o6D0576lsMPSkP824ZFZXFSBFzscYYiTASRuw6zmJkoieYlkgxVHqBu5xpPzgedr2eqd7QYsQIIKfdilpFGIVZjNjxk0sgEtx7GMfPKJF8IrnCCWOifmKeOHECl19+eSLGQrRSzGSlRRNjpM0+U45emBNeb8Vpk2CzSkJXgs8v49ttR+D2+fHTnkDXdCXGiBVGHXPTMKgoL2z/yvpAnSNtVlWbDLsa7BstRoHE7DrWSmOmJ5jC7CsH46S2GXj+ikGG27GihLUCsQ18lW1MWYy4GCP9liBcULWBJe7cvu0xuCgPvz29q+42rZ2mStcnmp5E6qIPp5yOrvkZeO+mEYk7SZKJ+mfV5Zdfjv/973+47bbbEjEeopXA1sCIp8VIkixhFhplYu7fKQdYw2+vxCMJLUZ+f5hJWuRKy0234193jsG1b63A97uOqcurgsLIrrFU/fDQuZyFJBokyQLJIk6VZc9jjdGVdsmQzrjERGYKa7ZnCyGyIkyJh+ItRjoxRmxWmiZGzKkjjIysZw6bhC/uHKO7nuAFayqUDyDiRyJT6Uf1bIulD56TuBOkAFE/nXv16oVHHnkEP/30EwYOHAi7nX/Q3XXXXXEbHNFycXPCyEwdI/MWI9i0MUaBf0WtLxTXjyhzyWzwtfJ/rbhSLEZay0ZjC97ZJHEFb1FtISA6i5FZqhpCVb/zmaBtVtQIXWk62XyZJoOveeFFk3ljYG8fxRi1LKLNSiN4ohZGr7/+OrKysvDdd9/hu+++49ZZLBYSRoQp2HYX7CP557Jq5KTZ0T7o9mrw+LDlUKXpKqtWyRIWSK2Inn6CZqnKJKwXY6T11YeEUWiCVkr+awVQSBjxLgtt0HC02KwWuAUFdNnzWxsZYxSJ6oZQdWv2vOmRgq91LUb6wdd2nSrWRhl6RGTYTzbFGLUsKMSocUQljGRZxpIlS9C+fXukp6dH3oEgdGArwCrZX0eqGlD8wlIAwL5ZFwAAHvliMz5Zc0DX0qDFarHAohE5yktRfIsyCYtcCVqLkSzLIVdaWriFQyuMTmqbAUATPByHbCk9S4lezEg8zmkWVtQoeogdl15zW9ZiFN5ENjR+rhVJcDLPcFhR5/ahW/B+E9FD1jeCCBHVE1OWZfTu3RsHDhyIvDFBGNDA9AxyB0XS7rIadZlSYfqTYKHCKsZCMZypHqvFKlmCVqPQMtZi8co1Q9CRaXqpBPqKW4LwMUZevxwq8OgMiSxF+LDWjIJsJ/527TAAvGA6qW2m7tjNYib9P9bg68bCihpvsEAm66bRy+bLMNkShL0u5T58evtoXDCwI9664bRGjLz1wVoVROUqiOaLn0xGjSIqYSRJEnr37o3jx48najxEK4G1GCnCiK0mXV7rFu5345hu+Ofto3H+gMKwdVYp1DuLFQ/sM/9Xp3bCS1cNUV8rk7DoB7PHJ3P+Bq9PFrrSRIUc37p+OHq1DzRmZYOitT3JYiHapqiJtBjlaYrCsedSAuZZj5eexSjLqW8xYl2R7JUr19ivYw5enTyUa4NCRAdZjAgiRNRPzFmzZuGBBx7A5s2bEzEeopXAdo5XhBGbeXa02gUAYcUalQe46Acua5lgrQzacgCiujuHKxvUZUpvMW0mnMfvZ7LSwi1G7ATOx9qElosCwKPFTGxNomOMFPI1lXvZIHbF6seORddixAhN7Xi54HzOYkSTebygrLSWBRmMGkfUwdfXXXcd6urqMGjQIDgcjrBYo/Ly8rgNjmgZLN99HJX1Hri8PhTmpGHr4Sos2l6mrj9U2YB/rjmArvmhGBFFGOWlO1DqCYkWo75XXIwNM2lqd2G3UyxGO49Uq8sUUfP9rmPcco/Xr1qM2Owre7DJKStYWHcQK9L6FGbrjt8sZixGliZypRm1EVEtRpwrTTwWNsbI6OrYdRR8HT/MZIYSzQdypTWOqIXR7NmzEzAMoiVz9Rs/Rdzm/k82YOo5vdTXijCqcXm57RSLkeh7z7epEMelaLdTBMyQojbYeaQGnfPSVZfYa9/t5varc/vU87JWDaXJqV5rDPb/feMhjEz8uvcxPQwT6Uo7q3eB7jrFYsSOt4OgyCbAW9iM+o2Jgq+J2CjIckbeiGiWkCxqHFELo+uvvz4R4yBaKGbT7AHgZyb4+miNC16fP0wYGTW7tOq0xAhL32deK5Wap0/qi27tMnHJkE54+HOxm7jWHRoLV4E5aDHyM663dE0w8V8mD4VksaBtHCYjM8HXHl9oLIkQRv+79yws2VGGG0Z3191GbfZqlfD6tcPg8clop3P9kmTB2zcMR43Lp5ZqEMG+k/HsQdca6do2A7OvHBwWJ0Y0f8hg1DiiFkYlJSWG67t2pRL8RIgGr3lhxKZlH612hYkiwHyMEesus2jmT86VFnQz5WU4cPvZPQHwlZxZapjMOC4FP/h/Nr0/TSNGJg3sKDxmLJixlLDxUY2tmyTi5A7ZOLmDsfXLy4iz8aeEB8trObdvh4jbWKhac1wxU+WcaI6QMmoMUQujbt26GfqjfT5B5Tmi1VIvqkSoA2sBOFrtQlV9uDAymgw5N4uBxYgLvnaEx7zoCqOgUHPaJO47oAi6RIsR9dgmYms8UVjqEoXHH/8xcBYjijEiCCEJ+Oq1KqIWRuvWreNeezwerFu3Di+88AKeeuqpuA2MSD5vLduLBo8PdzKxP9HC1iuKhk0HK3HL+6vDlpu1EnAWI4N12po5gL4wmvpB4LOvdU0pgo51XyUSMzFG3iYaixGJMOezIpdijAiCSARRC6NBg8K7bg8fPhydOnXCn//8Z1x66aVxGRiRXLw+P574aisA4NKhndExN7ZK59EIowYmhb+kvE64jVHwNYs9yuBrFr2gVNVipNlndM92APiA50RiRhAMKsptgpE0HXkZdlTUeXBe/w5YuS+Q+UrCiCB4lO/JuH7tkz2UZk1sLb4F9OnTB6tWrYrX4Ygkw8bL1ApifcxSH40wMuF2M1uhl7MYaXaRBHWMWPQsRgqKxWjF/43DwYp6DOwSECFmG902FjNBx73aZ+Nfd46JeC3NhUX3nY2fy2rQNsuBp77eBoBcaQShZdF9Z2PXkWqM6J6f7KE0a6IWRlVVVdxrWZZx+PBhPPbYY+jdu3fcBkYkF7YORmNcRA0e81YU1mKkh2IliKSPjGKMeItR+ORqVhh1yEnj0s+1BSEThdkqxYOK8hI7kCYkP9OBEd3zsftoKHORGp8SBE9+pgMje7RN9jCaPVELo7y8vLDga1mWUVRUhHnz5sVtYERyYSf5xsSrRGUxMiGijGKMWPealRNG/HZcur7AYqSXUh5pDE0WY9SK09TZ95gsRgRBJIKohdHixYu515IkoaCgAL169YLNFjfPHJFk2HAZbXbRZ2sP4PWle/C3a4dFbIoaTYyRmQw2q0lXGpu5ZWQxErXL0GtboVDrEo+zyWKMWnWaOpv515rvA0EQiSLqn1wWiwVjxozB2LFjMXbsWJx55pno27cvAGDp0qVxHyCRHHyyvsVo2scbsL20GtM+3hDxOHrCqFBQxE/PlXbRoE7q/82m65uNMdKL1xndU98cXVXvES6fem4ge++yoV10940HqW4xevKSAQCAGb/qH/djF+VnwCpZkJNma+UCkSCIRBH1E/acc84R9kOrrKzEOeecE5dBiSgvL8fkyZORk5ODvLw83HzzzaipqTHc/ve//z369OmD9PR0dO3aFXfddRcqKysTNsaWBOtK06uJs2F/RcTj6FmBbj+7J9bPOI9b5hK40m45qweuG3WS+lqxEkTKSmOtCVrXr9WEMHrvphEY11ec2VGtE4w+7KR8bJgxHs9dfqrx4BqJPcUFwW9PPwkbZozHTWfoV8WOFafNis2PTcCqh4upvxdBEAkhamEky7LwgXT8+HFkZhq7VRrD5MmTsWXLFixYsABfffUVli5diltuuUV3+0OHDuHQoUN47rnnsHnzZrzzzjuYP38+br755oSNsSXBBl+7dCw5ZrKw9CxGDpuEPE1ndlE8UvtsJzKYBqNGLUFYjKwJ7DqHTrsMm1WKGGskIjfDnvAJuzm4kHIT2GYi3WFNaGNcgiBaN6aDgpT6RBaLBTfccAOcztCk4fP5sHHjRowePTr+IwSwbds2zJ8/H6tWrcLw4cMBAHPmzMGkSZPw3HPPoVOnTmH7DBgwAJ9++qn6umfPnnjqqafw29/+Fl6vl+KhIsBajLSWnEyHFbVBS1CtyyuMyal3+5DusKJeJ6DaIbDUiESUwyZxHdmVGKNI2sNIQLExR6JxKIjij1KBVHelEQRBNGdMP2Fzc3ORm5sLWZaRnZ2tvs7NzUVhYSFuueUW/OMf/0jIIJcvX468vDxVFAFAcXExJEnCihUrTB+nsrISOTk5hqLI5XKhqqqK+2uNcMJI0++MbaOxvbQ6bN8PV5ag/6Pz8d9Nh3Wz0kSWGqEwskqcxUgx9kSyGDTWYhQ4R2oKkFR3pREEQTRnTJtN/v73vwMI9Eq7//77E+o201JaWor27fl4D5vNhvz8fJSWlpo6xrFjx/DEE08Yut8AYObMmXj88cdjHmtLwciVxmZlVdS5w/ad/tkmAMDtc9fi1rN6CI+vCJKHL+iHJ/+zLXhO8XasxcgT3OjBiX2w4UAFrj39JDz+761h+xnVuJEkC351akdU1nvQqyBLdzs98fXab4fp7tMUsNawxy7sj7d/2IfHLop/oDNBEERrJOqfxI8++iicTie+/fZb/O1vf0N1dcBicOjQIcNgaBEPPfQQLBaL4d/27dujHWIYVVVVuOCCC9C/f3889thjhttOnz4dlZWV6t/+/fsbff7miJ7FyOeXOStQpGalkSxGvzuzB64cXqS7v8MmIY0RKIpVqWNuOhbddzZuHCMO8I2UsfTKNUPx/s0juQw1LSKL0ewrB2PigMid4hOJnRF9I3u0xdIHzzHVmZ4gCIKITNSBNr/88gsmTpyIkpISuFwunHfeecjOzsYzzzwDl8uF1157zfSx7rvvPtxwww2G2/To0QOFhYUoKyvjlnu9XpSXl6Ow0HiSqq6uxsSJE5GdnY3PP/8cdrtxUKjT6eTip1orrMWIdXFphY5bUNSQjUHSS213MnEyRtYdh1XixIvWraeH2WazRohijFLBvcYGX1PKOkEQRHyJWhjdfffdGD58ODZs2IC2bUO1Xn79619jypQpUR2roKAABQUFEbcbNWoUKioqsGbNGgwbFnBjLFq0CH6/HyNHjtTdr6qqChMmTIDT6cSXX36JtLTw2jmEGNYQxAZf12lS1T0CoRIIDg4Io00HxeUR7IzAMJrc7Roh4jIoGMmm8MdDMIhcaakQkM0Wr6RAbIIgiPgS9VP1+++/x8MPPwyHg0+17tatGw4ePBi3gbH069cPEydOxJQpU7By5Ur88MMPmDp1Kq666io1I+3gwYPo27cvVq5cCSAgisaPH4/a2lq89dZbqKqqQmlpKUpLS+Hzma/G3FrRc6XVauoSaV1pLq8PlYyVaPfRWuHx2WwwI+uOUzPx56SZSwOPh2AQWYdSIU3cThYjgiCIhBG1xcjv9wuFxYEDB5CdnR2XQYmYO3cupk6dinHjxkGSJFx22WV4+eWX1fUejwc7duxAXV0dAGDt2rVqxlqvXr24Y+3duxfdunVL2FhbAnrB17Vai5FGGB2vCQ/GFuEwaTFStnvpqsFYuvMYLhnSWXdbNoU/LhajFHWlWTmLEQkjgiCIeBK1MBo/fjxmz56N119/HUCgrlFNTQ0effRRTJo0Ke4DVMjPz8cHH3ygu75bt26Qmcn87LPP5l4T0aFnMapzG8cYHa12mTo+K4yMag4p2108uDMuHqwvirTEJcZI5EpLMYtRPK6TIAiCCBG1MHr++ecxYcIE9O/fHw0NDbjmmmuwa9cutGvXDh9++GEixkgkAbZXGhtjVOs2thiZFkbW6CxG0RKfGCOBxSgFYozYApU26jBPEAQRV6IWRl26dMGGDRvw0UcfYcOGDaipqcHNN9+MyZMnIz09PRFjJBrJiVo3vtxwCBcN6oQ2maHYsP9sPIx2WQ6M7BHeMNXvF7vS6jSd5bXB10drzAkjp81cjJFRZWojzLYOMUJsMUq+EOFchuRKIwiCiCsx9cWw2WyYPHkyJk+erC47fPgwHnjgAbzyyitxGxwRH+6YuxbL9xzHt9uO4P2bA1l8+8vrcOcHawEA+2ZdELaPfvC1scXoWAyutJS1GAljjJLvSmOvjIKvCYIg4ktUs86WLVvwyiuv4PXXX0dFRQWAQEXpe++9Fz169MDixYsTMUaikSzfcxwA8P2uY+oyNnNMG1ANaFxpXv10fW2MUaVO3SItXIyRUR2jGIVRPGJvRNaqVLAYsVCMEUEQRHwx/ZT/8ssvMWTIENx111247bbbMHz4cCxevBj9+vXDtm3b8Pnnn2PLli2JHCsRR9gJXhQX5GfrGLFZaRHS9asaAsIoUmd6u0GMEfsyVleaPQ4uJpHoSIUYIxY7xRgRBEHEFdNP1SeffBJ33nknqqqq8MILL2DPnj2466678PXXX2P+/PmYOHFiIsdJxBkPY+kRxQXpBV/XRXClVdUH1nfIMRZGrBjSxgOxNYhitxg1XjCwQc4KsQq1eGJhxmXU0oQgCIKIHtNP+R07duDOO+9EVlYWfv/730OSJLz44os47bTTEjk+Io6wYoSNISqrElmMdGKMtMHXPj/8fhl/+vdWfL7ugGox6pBjXGXcYtEvUsh2j49GiMS78rVIW1GlaYIgiJaN6eDr6upq5OTkAACsVivS09PRo4e4czqRmrDuKy/jKzta3RC2rU8nK60+6ErLTrOhusELt1fGjiPVePuHvchOs6FrfgYAoH22+X5zWpeV3SYBwfPEKkROF2TaRUvH3NTMshQYsgiCIIg4EVVW2jfffIPc3FwAgQrYCxcuxObNm7ltLrrooviNjogrrFvK64/ClcZYjNxB11m2MyCMPD6/6l6rbvBiV1kNAKB9BIsRS5jFKEYxxAqGgV1y8fkdo9EpL3Zxk5tuxzf3nIWXFu7E15tKYz4OQRAE0XyIShhdf/313Otbb72Ve22xWKgPWQrDWYzYGCNh8LU4xsgdFEmZzsBHx+PzC9c3ymIUp7iZIV3bNPoYfQqzURS0ghEEQRAtH9PCyO8P76JONC+cNj1XWiSLkY/5v0AYecM/G2yMkVWycK45LdoihfYUS4n3G4w9GZAnjSAIInGk1gxEJJSoXGk6wdeKKy0rKIzcPpkTTgpsVlqG3bgoYlhWWoplWqWYLiIIgiASCAmjVgRb28fHuNKqG8ILPPoZi1G9x6daTdxBEZThCIgdjzfcYmS3WtAmI9R6pE9hNnfusScXcNtrhVC0/b+mntMLAPDYRadEtZ9Zrh7RFQBwdp+CCFs2DRNOKQQAnNolN8kjIQiCaHnE1BKEaJ44dFxprAhSYMsTyTJQ4/YiJ82u1j/K0okxAoCcNDvSGCtRXoYdGx+dAEkCGjx+ZDv5j114Vlp0FqP7J/TBlDN7IDfDHtV+ZunVPgsbHh0fNu5k0T4nDZseG48MR2qMhyAIoiVBT9YWjsyIHodV7EoThY9p42qq6j3ISbOLg681rrTcdDvSmArRVsmC9KCFSdRrrLEWIwAJE0Xq8dMTe/xoyU5LrfEQBEG0FMiV1sJh3VxsVhobQ6RYjOqZdh8+WdwDTSuMAjFGvLLKTuctRpH6eWnXp0J1aYIgCKJ1EtMMVFFRgTfffBPTp09HeXk5AGDt2rU4ePBgXAdHNJ4GT0jssK40tiWIX5bx1cZD6DdjPuau+AUAwrLIlFYfoeDrYIyRICstJ83GiTBLhIqEWgtRdhoZMgmCIIjkEPUMtHHjRhQXFyM3Nxf79u3DlClTkJ+fj88++wwlJSV47733EjFOIkYamPgfVqD4GP+Zzw9M/WAdAOCPn2/G5JEnhcUdKa0+wixGXj9cQfF1Rq92KCmvwwUDO3L7WiMII63F6LGLTsHeY7W4cUy3iNdHEARBEPEkaovRtGnTcMMNN2DXrl1ISwvVqpk0aRKWLl0a18ERjaeesRixcUNsjJEsy2Huq3CLUUAYGdUx6t8pB0sfPAdXBbO4FCJl37N1jDIcVhTlZ2DR/Wfj2lHdjHckCIIgiDgTtTBatWpVWMVrAOjcuTNKS6ltQqrBxQ2xwohxpflkGRlOPig6TBgFU/qVdP1MR7gwcuoUZhR1qWdhLUaUaUUQBEEkk6iFkdPpRFVVVdjynTt3oqAgNeq8ECEavOKAaj4rTVaFjrpM60oLWoyU2KTMoJA6VuPG9tLA50FXGEUwGbFZaZlO42KQBEEQBJFIohZGF110Ef70pz/B4wlMlBaLBSUlJfjDH/6Ayy67LO4DJBpHg1vsSmNjjGQ5VLAxtJ4/jhpjpKl8DQA/7QkE4ItS8QGgU65xQ1myGBEEQRCpQtTC6Pnnn0dNTQ3at2+P+vp6jB07Fr169UJ2djaeeuqpRIyRaAR6FiNPmCstksXIC59fVl1smYJih047/3H627XDcPHgTrjt7J6GY2Sz0jIdZDEiCIIgkkfUP89zc3OxYMECLFu2DBs3bkRNTQ2GDh2K4uLiRIyPaCT1bqbCNWcx4tP1WUEiy7IgxsijZqQBvMVIQetKm3BKodq+wgjOYpQi1aUJgiCI1knMs9AZZ5yBM844I55jIRIAm5XGxRj5WMHEu7AaPH5VGOVl2FFR50FVPS+MhBYjHVdaJLgYI7IYEQRBEEkkamH08ssvC5dbLBakpaWhV69eOOuss2C10gTXVByqqMe3247gN8O6hMXosAUe2bghr8ZixLrBqho8qiutTYYjIIwavHD5AseyWIB0e/j7qxd8HQmKMSIIgiBShahnoRdffBFHjx5FXV0d2rRpAwA4ceIEMjIykJWVhbKyMvTo0QOLFy9GUVFR3AdMhHPxqz/gaLULe4/V4tEL+Q7z9brB17wwYtdV1Xs4i5GyTLEYOawSV0VbQRtjZBa2jhFlpREEQRDJJOqZ7Omnn8Zpp52GXbt24fjx4zh+/Dh27tyJkSNH4qWXXkJJSQkKCwtx7733JmK8hICj1S4AwHc7joatqw5mkwH6wdd+mX9d1eBRt80JNiutc3vVbRxWSdj/LFZXGlmMCIIgiFQh6lno4YcfxqeffoqePUOZRr169cJzzz2Hyy67DHv27MGzzz5LqftJQFRHUSnMCOin6wOAl3ldVe9Vt1X6ltW6fSGLkY7LLFZXGpuVJnLREQRBEERTEfVMdvjwYXi93rDlXq9XrXzdqVMnVFdXN350RFSIKkwrhRkB/QKPALjA6qoGjxqPlB20GLm9ftS5A++7vjBqvMUoLUZ3HEEQBEHEg6hnoXPOOQe33nor1q1bpy5bt24dbr/9dpx77rkAgE2bNqF79+7xGyVhCrHFiBFGOi1BgEBrD3Wf+lDwdQ7T6b6iLnAsXWEUa4wRI4xitToRBEEQRDyIehZ66623kJ+fj2HDhsHpdMLpdGL48OHIz8/HW2+9BQDIysrC888/H/fBEsaILUZiV1qYxYiLMfKqIsppt8IeDI4+UecGgLCGswrxyEpzkiuNIAiCSCJRxxgVFhZiwYIF2L59O3bu3AkA6NOnD/r06aNuc84558RvhESjqNIJvtbGGLGutAaPT93WarEgw2FDZb0HJzQWox8fOhfvLt+Hv323B0B86hiRxYggCIJIJjGnAPXt2xd9+/aN51iIRhIxxojRQh6/viutweNTrUtWKVB0sbLegwrFYhQUL53y0jG6ZztGGMXBYhSjuCIIgiCIeBCTMDpw4AC+/PJLlJSUwO12c+teeOGFuAyMiB5JoEu4rDTWYuTTD76u9/hUV5okWdQ2HWqMEeNKS2PEUKwxRhYLWYwIgiCI1CBqYbRw4UJcdNFF6NGjB7Zv344BAwZg3759kGUZQ4cOTcQYCQNkRuxYwFuMvD4/alwhYVRe68b5L32Pt64fzqXnA1qLkV8VUVaLRW3TcUJjMQL4mCC92KNoiFVcEQRBEEQ8iHoWmj59Ou6//35s2rQJaWlp+PTTT7F//36MHTsWl19+eSLGSBjgYiw92pqLrChS2Ha4Cn/4dGNY8DUrjOo5V5pFLbooshj1LcyGwyqhU24abI0QRie1zQAADC7Ki/kYBEEQBNFYop7Jtm3bhuuuuw4AYLPZUF9fj6ysLPzpT3/CM888E/cBKpSXl2Py5MnIyclBXl4ebr75ZtTU1JjaV5ZlnH/++bBYLPjiiy8SNsZkwPZC0+brsxlpLOtKKrjUfYAXWA1uHxRPm2SxqG06RBajNLsV6x89D4sfODvWSwAALLh3LLY8PkGtm0QQBEEQySBqYZSZmanGFXXs2BG7d+9W1x07dix+I9MwefJkbNmyBQsWLMBXX32FpUuX4pZbbjG17+zZs7k4lpZEPdcklnePsRlpLDUuL2chAjSuNG8Ei5EmDijDYWt00LTDJiHTSe1ACIIgiOQS9Ux0+umnY9myZejXrx8mTZqE++67D5s2bcJnn32G008/PRFjxLZt2zB//nysWrUKw4cPBwDMmTMHkyZNwnPPPYdOnTrp7rt+/Xo8//zzWL16NTp27JiQ8SUTtkmsy6MRRsGMtOw0G6obeOtRvUc/Xb/e7UO2MxR8HWYxikMsEUEQBEGkIlELoxdeeEF1YT3++OOoqanBRx99hN69eycsI2358uXIy8tTRREAFBcXQ5IkrFixAr/+9a+F+9XV1eGaa67Bq6++isLCQlPncrlccLlc6uuqqqrGDT7BNDACh3WHASGLUZsMR5gw2n6Yvy7Ws1bv8YfVMQKAuqAI06t8TRAEQRDNnaiEkc/nw4EDB3DqqacCCLjVXnvttYQMjKW0tBTt27fnltlsNuTn56v92UTce++9GD16NC6++GLT55o5cyYef/zxmMfa1LCuNJfXJ1yXnRb+NmtFFLdOUMeIJSed4oAIgiCIlklUP/2tVivGjx+PEydOxOXkDz30ECwWi+Hf9u3bYzr2l19+iUWLFmH27NlR7Td9+nRUVlaqf/v374/p/E2FixNGYvdYWpRtNuqZyteSJVTHSKEgyxnLUAmCIAgi5YnalTZgwADs2bMnLk1i77vvPtxwww2G2/To0QOFhYUoKyvjlnu9XpSXl+u6yBYtWoTdu3cjLy+PW37ZZZfhzDPPxJIlS4T7Kf3fmgucxUgnbig9SmHUwBR4tEqWMItRQXbzuT8EQRAEEQ1RC6Mnn3wS999/P5544gkMGzYMmZmZ3PqcnBzTxyooKEBBQUHE7UaNGoWKigqsWbMGw4YNAxAQPn6/HyNHjhTu89BDD+F3v/sdt2zgwIF48cUXceGFF5oeY6qjdaXJsqxm4LkaYTFSCzwyWWkKJIwIgiCIlkrUwmjSpEkAgIsuuohLgVcmZJ/Pp7drzPTr1w8TJ07ElClT8Nprr8Hj8WDq1Km46qqr1Iy0gwcPYty4cXjvvfcwYsQIFBYWCq1JXbt2jYu1K1Vgg6/9MuD1y7BbA++LO5iCn+6I1mLkh9cXcqVlOnmPKwkjgiAIoqUStTBavHhxIsYRkblz52Lq1KkYN24cJEnCZZddhpdfflld7/F4sGPHDtTV1SVlfMmCtRgBASuRPZhOH3KlRZ9FphzXKllwUlveKkjCiCAIgmipRC2Mxo4dm4hxRCQ/Px8ffPCB7vpu3bpxfcNERFrfHHFphZHHh6xgsHSsMUYAUBtsJyJZLOhZkMWty6ZCjARBEEQLJaaCNN9//z1++9vfYvTo0Th48CAA4P3338eyZcviOjgiMmyBR4DPTFOqWUcbYwQAta6QxUhbt6ilVhEnCIIgiKiF0aeffooJEyYgPT0da9euVYshVlZW4umnn477AAljRK40BcVi5IyiIKPSiFaxGClFrnOpdhFBEATRCohaGD355JN47bXX8MYbb8BuD02WY8aMwdq1a+M6OCIydWEWo9BrJfg6mkrVSr+yWnfIlQYAvx7SGQDQOS899sESBEEQRIoTdbDIjh07cNZZZ4Utz83NRUVFRTzGRETBsRoX99olaBFiN+htZrda4PGFYq+ynYG+akqLEGvQhPSHiX3RIScNEweYa61CEARBEM2RqC1GhYWF+Pnnn8OWL1u2DD169IjLoAjzHK3WCCOBK83IYqRtCKvtcG8NWozSHVbcfnZPdG/HZ6gRBEEQREsiamE0ZcoU3H333VixYgUsFgsOHTqEuXPn4v7778ftt9+eiDESBhzVWoxYV5oZYcSskyxAhqbmkSRRoDVBEATReojalfbQQw/B7/dj3LhxqKurw1lnnQWn04n7778fv//97xMxRsIAxWKUnRZwgbGuNDXGyMCVxgojmySFZbBZSRgRBEEQrYiohZHFYsEf//hHPPDAA/j5559RU1OD/v37IysrK/LORFxp8PhQ3RAIku7SJgPbDldF7UpjaxxZJUtYlWyJUvMJgiCIVkTUrrR//OMfqKurg8PhQP/+/TFixAgSRUlCsRY5bBLaZTkA8K40pY6RXrp+ltOGdKYPmk2yIFPTF40sRgRBEERrImphdO+996J9+/a45ppr8PXXXyekNxphDiW+qCDLqbrAorEYFWQ7weoeq9USFmNkJYsRQRAE0YqIWhgdPnwY8+bNg8ViwRVXXIGOHTvizjvvxI8//piI8REGKBajgmynahViW4QoIslhFVe+LshychYhm2QJy0qTYqqNThAEQRDNk6inPZvNhl/96leYO3cuysrK8OKLL2Lfvn0455xz0LNnz0SMkdCBF0YCi5FPqWMktvoUZDu59h5WSWAxIlcaQRAE0YpoVDfQjIwMTJgwASdOnMAvv/yCbdu2xWtchAmqGjwAAu06FHdZNK60dlkOHK6sV1/bJCnMYmSU0UYQBEEQLY2YZr26ujrMnTsXkyZNQufOnTF79mz8+te/xpYtW+I9PsIAjzdQntpulUKutCjqGOVlOLisM5HFiHqkEQRBEK2JqC1GV111Fb766itkZGTgiiuuwCOPPIJRo0YlYmxEBLx+JYbIEnKlCeoY6WWlZafZOGEkykrLTiNhRBAEQbQeohZGVqsVH3/8MSZMmACrJqh38+bNGDBgQNwGRxij9DizcRYjP9xePywWxmKkE3ydnWbjgqutkgUZztC26XZrVA1oCYIgCKK5E7Uwmjt3Lve6uroaH374Id58802sWbOG0vebEKVOkc1qgdMeEDB1bh/OeW4JZFlGQzBDTU/c5KaHu9JYi1FOeqNC0AiCIAii2RGzOWDp0qW4/vrr0bFjRzz33HM499xz8dNPP8VzbEQEvEzLD8WVVlbdgIMV9ThU2QB/wKAUJoyuGdkVg4vycE7fAj5dX1PHKIfcaARBEEQrIyqTQGlpKd555x289dZbqKqqwhVXXAGXy4UvvvgC/fv3T9QYCR08QeVjk0KuNDeTlaagFUZP/3qg+n8+XZ/PSsuhwGuCIAiilWHaYnThhReiT58+2LhxI2bPno1Dhw5hzpw5iRwbEQGPl3GlBcVPvSfclalXxwgAV/naJmktRuRKIwiCIFoXpme+//73v7jrrrtw++23o3fv3okcE2ESb9Bi5LBKcAZbgtS6vGHbsbWItAUbrdoYI8ZiRBlpBEEQRGvDtMVo2bJlqK6uxrBhwzBy5Ei88sorOHbsWCLHRkSAC762hYKvWRxWiXOXaQtZWzTp+qzFKM1OGWkEQRBE68L0zHf66afjjTfewOHDh3Hrrbdi3rx56NSpE/x+PxYsWIDq6upEjpMQEBJGkr4w0sQXWTRNYa3adH0mK81KjdIIgiCIVkbUM19mZiZuuukmLFu2DJs2bcJ9992HWbNmoX379rjooosSMUZCB69PcaWFCjxqXWlaYaS1GGkLPGqbyhIEQRBEa6JRJoE+ffrg2WefxYEDB/Dhhx/Ga0yESbistKDbS4k7UtD2OpM0FiNJ4rPSWKiBLEEQBNHaiIuvxGq14pJLLsGXX34Zj8MRJhFlpWnRWoysWmFk0bcQkcWIIAiCaG1QEEkzJtQrLVTgUUt4jBG/ntU+Vk1a/4DOuY0fJEEQBEE0I6hQTTNG1CtNS9tMB/daMkjXdwbdbl9OHYPV+07gokGd4jlcgiAIgkh5SBg1Y5SsNDvTK01LQbaTe611pbFZaop16dQueTi1S14cR0oQBEEQzQNypTVjlKw0u4ErTSuMjNL17Vb6OBAEQRCtG7IYNUPcXj/+9NUW7DgSqB1lk/SDr7XCyChdXxuPRBAEQRCtDZoJmyEfrd6Pf/xUor622/RjjAqyAsJodM+2AIDJI0/i1otcaQRBEATRWiGLUTPkSGUD99ouBdp+OGwS3MEUfgXFYvTGdcOxYX8FRnTP59az3jNtzSOCIAiCaG3QTNgM0brDbME0e5HVSBFGmU4bRvdqB5tBwUeyGBEEQRCtHZoJmyHalHslaFoUgK2NMQo7FpuuT8KIIAiCaOXQTJji/HvDIZz/0vfYc7RGXaZNubcbWIzaZpoXRmQxIgiCIFo7NBOmOL//cB22Ha7CjH9tUZdpLUY21WLEv52922dF7HfGrqYYI4IgCKK102xmwvLyckyePBk5OTnIy8vDzTffjJqamoj7LV++HOeeey4yMzORk5ODs846C/X19U0w4vjS4PGp/9c2glUsRjampcegLrn4+u4zIx6XFU5Ux4ggCIJo7TSbmXDy5MnYsmULFixYgK+++gpLly7FLbfcYrjP8uXLMXHiRIwfPx4rV67EqlWrMHXqVEhSs7lslXRHKH5I2+/MHrweVtjkpNtNCR1K1ycIgiCIEM0iXX/btm2YP38+Vq1aheHDhwMA5syZg0mTJuG5555Dp07inl733nsv7rrrLjz00EPqsj59+jTJmONNmt0Kn1+G1++HR5OSb1MtRiFhY4vgQlPg0vVJGBEEQRCtnGYxEy5fvhx5eXmqKAKA4uJiSJKEFStWCPcpKyvDihUr0L59e4wePRodOnTA2LFjsWzZMsNzuVwuVFVVcX+pQLrdiiv+thyjZy5CZb2HW6dYhuyMGLKatIpR8DVBEARBhGgWM2FpaSnat2/PLbPZbMjPz0dpaalwnz179gAAHnvsMUyZMgXz58/H0KFDMW7cOOzatUv3XDNnzkRubq76V1RUFL8LiRKlSSwApNklrPnlBI7XurF8z3FuO1UYMeYfs+FCrCvNSTFGBEEQRCsnqTPhQw89BIvFYvi3ffv2mI7t9wdExa233oobb7wRQ4YMwYsvvog+ffrg7bff1t1v+vTpqKysVP/2798f0/njQXWDV/0/6yY7VuPitlMCqNnga5tJi5GVLEYEQRAEoZLUGKP77rsPN9xwg+E2PXr0QGFhIcrKyrjlXq8X5eXlKCwsFO7XsWNHAED//v255f369UNJSYloFwCA0+mE02lc+6epqGJcZg3uUFbasRq3cHveYmQuxohL1ydhRBAEQbRykiqMCgoKUFBQEHG7UaNGoaKiAmvWrMGwYcMAAIsWLYLf78fIkSOF+3Tr1g2dOnXCjh07uOU7d+7E+eef3/jBNwFVDSFhVO0KWY98flm4vd3KxhiZFEaUrk8QBEEQKs1iJuzXrx8mTpyIKVOmYOXKlfjhhx8wdepUXHXVVWpG2sGDB9G3b1+sXLkSQCB25oEHHsDLL7+Mf/7zn/j555/xyCOPYPv27bj55puTeTmmqaoPiaEaxq2mhy0mixG50giCIAhCoVmk6wPA3LlzMXXqVIwbNw6SJOGyyy7Dyy+/rK73eDzYsWMH6urq1GX33HMPGhoacO+996K8vByDBg3CggUL0LNnz2RcQtSwFqMaV2Rh5IghXZ8qXxMEQRBEiGYjjPLz8/HBBx/oru/WrRtkOdzF9NBDD3F1jJoTbIyRGWHEiiFt2xA9WMsSNZElCIIgWjs0E6YwXIxRlK40sxYjVkuSK40gCIJo7dBMmGL4/TLmrSzBjtJqPsbI5THYK4AjhuBrjz9UK4mEEUEQBNHaaTautNbCvzcewkOfbQIAXD/qJHV5g8evt4tKLBYjny9kMqIYI4IgCKK1QzNhirH5YKX6/zqmdpEZ2AKPZmOMPEzqv42EEUEQBNHKoZkwxWBrCTV4I1uJWGLJSvP6ojsHQRAEQbRkSBilGKzVpj5ai5HE1jEy99bqFYskCIIgiNYIxRilCO8t34fyWjfsjKXH5Y3dlcb2QDPCS8KIIAiCIFRIGKUAsixjxr+2AACuOq1IXW4mxqhX+yz1/5wrzWoy+JqEEUEQBEGokDBKATxMZliDJySGqhuMU/RnXToQ5w/sqL62xZKuTzFGBEEQBKFCMUYpAOsyY2ODTtQZC6MzTy5AbrpdfR1Lur7XRxYjgiAIglAgYZQCuJjsM7atyYlat+F+2rpDbIFHiWKMCIIgCCJqSBilAKwwqnWHql1HEi3aStVsVprZGCOvn1xpBEEQBKFAwigFcDFxRWaaxSpoLUaxxBj97oweAIBJAwtNn5cgCIIgWioUfJ0CsBajmgjNYnPT7aisD8QeaS1GrFAym64/sEsuNswYj5x0+igQBEEQBFmMUgBOGEWwGOVlhIKttVYhNvjarMUIAHIz7LCYFFIEQRAE0ZIhYZQCRONKY7PQtLCuNLMxRgRBEARBhCBhlAJwwdcuvqij1pDTNtOhexzWlWY2K40gCIIgiBAUWJICGLnSMh021Ht8aoXqC07thPI6DwZ1yQ07Dlu7yGayVxpBEARBECFIGKUARj3R0uxWeP1+VRhlp9nwrzvHCLeNNcaIIAiCIIgAZFZIAVwe/VpCaXYJdkbwaDPRWBwxVL4mCIIgCCIECaMUgHWlaUm3WznB4zQQRrHUMSIIgiAIIgQJoxTAyJWW7rByFiOnzaq7rZ1caQRBEATRKEgYpQANRq40mxUeX2j9SW0zdLe1s+n6JIwIgiAIImpIGKUAhsHXDiuOM81k22U5dbdlg68lEkYEQRAEETUkjFIAoxijNJu5wGuALEYEQRAE0VhIGCWZnUeq8c4P+3TXpztCMUW9CrIMj2VnahdRfUeCIAiCiB6qY5Rkxr+41HB9mwwHLBZAloELB3Uy3JbNSguWPSIIgiAIIgpIGKUoHXPTcMmQzrj29JNw+fAu+PHn47jpjO6G+7BZaX5SRgRBEAQRNSSMUpS7xvXG1SO6AgA65aXjlE7hLUC0sMKIZBFBEARBRA/FGKUosdQhYveRSRkRBEEQRNSQMEpRGptVJpMyIgiCIIioIWGUojS2cnWPCBlsBEEQBEGEQzFGKYpNik2zfv/gOais96AwNy3OIyIIgiCIlg8JoyTi9ekXdozVYlSUn4GiWAdEEARBEK0ccqUlEbeBMKLK1QRBEATR9JAwSiIug+axVisJI4IgCIJoakgYJRG2R5q2hQdZjAiCIAii6Wk2wqi8vByTJ09GTk4O8vLycPPNN6OmpsZwn9LSUlx77bUoLCxEZmYmhg4dik8//bSJRhwZl9cHAHDaJGx4dDwKc0IB043NSiMIgiAIInqajTCaPHkytmzZggULFuCrr77C0qVLccsttxjuc91112HHjh348ssvsWnTJlx66aW44oorsG7duiYatTGKxSjTaUNOmh1Oe+jtiDUrjSAIgiCI2GkWs++2bdswf/58vPnmmxg5ciTOOOMMzJkzB/PmzcOhQ4d09/vxxx/x+9//HiNGjECPHj3w8MMPIy8vD2vWrGnC0eujxBg5bRL3L0AWI4IgCIJIBs1CGC1fvhx5eXkYPny4uqy4uBiSJGHFihW6+40ePRofffQRysvL4ff7MW/ePDQ0NODss8/W3cflcqGqqor7SxSsKw0AHDbWYkTCiCAIgiCammYhjEpLS9G+fXtumc1mQ35+PkpLS3X3+/jjj+HxeNC2bVs4nU7ceuut+Pzzz9GrVy/dfWbOnInc3Fz1r6gocVWBFFea02bl/gXIYkQQBEEQySCpwuihhx6CxWIx/Nu+fXvMx3/kkUdQUVGBb7/9FqtXr8a0adNwxRVXYNOmTbr7TJ8+HZWVlerf/v37Yz5/JFSLkT3clWajdH2CIAiCaHKSWvn6vvvuww033GC4TY8ePVBYWIiysjJuudfrRXl5OQoLC4X77d69G6+88go2b96MU045BQAwaNAgfP/993j11Vfx2muvCfdzOp1wOp3RX0wMGMUYkSuNIAiCIJqepAqjgoICFBQURNxu1KhRqKiowJo1azBs2DAAwKJFi+D3+zFy5EjhPnV1dQAASZPdZbVa4ffrF1ZsSoxdac3Cy0kQBEEQLYpmMfv269cPEydOxJQpU7By5Ur88MMPmDp1Kq666ip06tQJAHDw4EH07dsXK1euBAD07dsXvXr1wq233oqVK1di9+7deP7557FgwQJccsklSbyaENrgaz5dnyxGBEEQBNHUNAthBABz585F3759MW7cOEyaNAlnnHEGXn/9dXW9x+PBjh07VEuR3W7H119/jYKCAlx44YU49dRT8d577+Hdd9/FpEmTknUZHKrFSBBjRMHXBEEQBNH0JNWVFg35+fn44IMPdNd369YNsixzy3r37p1Sla61KDFGaQJXGlmMCIIgCKLpaTYWo5aINivNbiWLEUEQBEEkExJGSUQbfG1nUvSpJQhBEARBND00+yaRkDAKvA2slchKdYwIgiAIoskhYZREXB4+K42NK6IYI4IgCIJoekgYJZFQVlrAlcbWLqIYI4IgCIJoekgYJRGtK41tA2K1kDAiCIIgiKaGhFES0RZ4ZK1EElmMCIIgCKLJIWGURJQ6Rg5BjBFBEARBEE0PCaMk4vbxwojiigiCIAgiuZAwSiJKjJHDGgi+JosRQRAEQSQXEkZJxO3VWozo7SAIgiCIZEIzcRLRCiOyGBEEQRBEciFhlESUGCOlFcg5fdsDAPp3zEnamAiCIAiiNWNL9gBaM25NHaOCbCc2PTYe6cGCjwRBEARBNC0kjJKIx8cHXwNAdpo9WcMhCIIgiFYPudKSiDbGiCAIgiCI5EIzchIhYUQQBEEQqQXNyEnE5SNhRBAEQRCpBM3ISUKW5ZDFyEpvA0EQBEGkAjQjJwmPT1b/TxYjgiAIgkgNaEZOEkoNI4AsRgRBEASRKtCMnCQUNxpAFiOCIAiCSBVoRk4SSg0jq2SBlVqBEARBEERKQMIoSVDgNUEQBEGkHjQrJwkX1TAiCIIgiJSDZuUkQcUdCYIgCCL1oFk5Sbh95EojCIIgiFSDZuUkQRYjgiAIgkg9aFZOEhR8TRAEQRCpB83KScJDfdIIgiAIIuWgWTlJUFYaQRAEQaQeNCsnCQq+JgiCIIjUg2blJEHB1wRBEASRetCsnCRIGBEEQRBE6kGzcpJwe30ASBgRBEEQRCpBs3KSoBgjgiAIgkg9aFZOElTHiCAIgiBSj2YzKz/11FMYPXo0MjIykJeXZ2ofWZYxY8YMdOzYEenp6SguLsauXbsSO1CTuH0yAHKlEQRBEEQq0WxmZbfbjcsvvxy333676X2effZZvPzyy3jttdewYsUKZGZmYsKECWhoaEjgSM1BwdcEQRAEkXrYkj0Aszz++OMAgHfeecfU9rIsY/bs2Xj44Ydx8cUXAwDee+89dOjQAV988QWuuuqqRA3VFCSMCIIgCCL1aLGz8t69e1FaWori4mJ1WW5uLkaOHInly5fr7udyuVBVVcX9JQK3L5iVRjFGBEEQBJEytNhZubS0FADQoUMHbnmHDh3UdSJmzpyJ3Nxc9a+oqCgh45MsFjhtEpz2FvsWEARBEESzI6mz8kMPPQSLxWL4t3379iYd0/Tp01FZWan+7d+/PyHn+dPFA7DjyfNxx9m9EnJ8giAIgiCiJ6kxRvfddx9uuOEGw2169OgR07ELCwsBAEeOHEHHjh3V5UeOHMHgwYN193M6nXA6nTGdkyAIgiCI5k1ShVFBQQEKCgoScuzu3bujsLAQCxcuVIVQVVUVVqxYEVVmG0EQBEEQrYdmE+BSUlKC9evXo6SkBD6fD+vXr8f69etRU1OjbtO3b198/vnnAACLxYJ77rkHTz75JL788kts2rQJ1113HTp16oRLLrkkSVdBEARBEEQq02zS9WfMmIF3331XfT1kyBAAwOLFi3H22WcDAHbs2IHKykp1mwcffBC1tbW45ZZbUFFRgTPOOAPz589HWlpak46dIAiCIIjmgUWWZTnZg0hlqqqqkJubi8rKSuTk5CR7OARBEARBmCDW+bvZuNIIgiAIgiASDQkjgiAIgiCIICSMCIIgCIIggpAwIgiCIAiCCELCiCAIgiAIIggJI4IgCIIgiCAkjAiCIAiCIIKQMCIIgiAIgghCwoggCIIgCCJIs2kJkiyUwuBVVVVJHglBEARBEGZR5u1oG3yQMIpAdXU1AKCoqCjJIyEIgiAIIlqqq6uRm5trenvqlRYBv9+PQ4cOITs7GxaLJW7HraqqQlFREfbv30892BIM3eumge5z00D3uWmg+9w0JPI+y7KM6upqdOrUCZJkPnKILEYRkCQJXbp0Sdjxc3Jy6EvXRNC9bhroPjcNdJ+bBrrPTUOi7nM0liIFCr4mCIIgCIIIQsKIIAiCIAgiCAmjJOF0OvHoo4/C6XQmeygtHrrXTQPd56aB7nPTQPe5aUjF+0zB1wRBEARBEEHIYkQQBEEQBBGEhBFBEARBEEQQEkYEQRAEQRBBSBgRBEEQBEEEIWGUJF599VV069YNaWlpGDlyJFauXJnsIaUMM2fOxGmnnYbs7Gy0b98el1xyCXbs2MFt09DQgDvvvBNt27ZFVlYWLrvsMhw5coTbpqSkBBdccAEyMjLQvn17PPDAA/B6vdw2S5YswdChQ+F0OtGrVy+88847YeNpLe/VrFmzYLFYcM8996jL6D7Hh4MHD+K3v/0t2rZti/T0dAwcOBCrV69W18uyjBkzZqBjx45IT09HcXExdu3axR2jvLwckydPRk5ODvLy8nDzzTejpqaG22bjxo0488wzkZaWhqKiIjz77LNhY/nkk0/Qt29fpKWlYeDAgfj6668Tc9FNjM/nwyOPPILu3bsjPT0dPXv2xBNPPMH1yaL7HBtLly7FhRdeiE6dOsFiseCLL77g1qfSfTUzlojIRJMzb9482eFwyG+//ba8ZcsWecqUKXJeXp585MiRZA8tJZgwYYL897//Xd68ebO8fv16edKkSXLXrl3lmpoadZvbbrtNLioqkhcuXCivXr1aPv300+XRo0er671erzxgwAC5uLhYXrdunfz111/L7dq1k6dPn65us2fPHjkjI0OeNm2avHXrVnnOnDmy1WqV58+fr27TWt6rlStXyt26dZNPPfVU+e6771aX031uPOXl5fJJJ50k33DDDfKKFSvkPXv2yN988438888/q9vMmjVLzs3Nlb/44gt5w4YN8kUXXSR3795drq+vV7eZOHGiPGjQIPmnn36Sv//+e7lXr17y1Vdfra6vrKyUO3ToIE+ePFnevHmz/OGHH8rp6eny3/72N3WbH374QbZarfKzzz4rb926VX744Ydlu90ub9q0qWluRgJ56qmn5LZt28pfffWVvHfvXvmTTz6Rs7Ky5Jdeekndhu5zbHz99dfyH//4R/mzzz6TAciff/45tz6V7quZsUSChFESGDFihHznnXeqr30+n9ypUyd55syZSRxV6lJWViYDkL/77jtZlmW5oqJCttvt8ieffKJus23bNhmAvHz5clmWA19kSZLk0tJSdZu//vWvck5OjuxyuWRZluUHH3xQPuWUU7hzXXnllfKECRPU163hvaqurpZ79+4tL1iwQB47dqwqjOg+x4c//OEP8hlnnKG73u/3y4WFhfKf//xndVlFRYXsdDrlDz/8UJZlWd66dasMQF61apW6zX//+1/ZYrHIBw8elGVZlv/yl7/Ibdq0Ue+7cu4+ffqor6+44gr5ggsu4M4/cuRI+dZbb23cRaYAF1xwgXzTTTdxyy699FJ58uTJsizTfY4XWmGUSvfVzFjMQK60JsbtdmPNmjUoLi5Wl0mShOLiYixfvjyJI0tdKisrAQD5+fkAgDVr1sDj8XD3sG/fvujatat6D5cvX46BAweiQ4cO6jYTJkxAVVUVtmzZom7DHkPZRjlGa3mv7rzzTlxwwQVh94Luc3z48ssvMXz4cFx++eVo3749hgwZgjfeeENdv3fvXpSWlnLXn5ubi5EjR3L3OS8vD8OHD1e3KS4uhiRJWLFihbrNWWedBYfDoW4zYcIE7NixAydOnFC3MXovmjOjR4/GwoULsXPnTgDAhg0bsGzZMpx//vkA6D4nilS6r2bGYgYSRk3MsWPH4PP5uIkEADp06IDS0tIkjSp18fv9uOeeezBmzBgMGDAAAFBaWgqHw4G8vDxuW/YelpaWCu+xss5om6qqKtTX17eK92revHlYu3YtZs6cGbaO7nN82LNnD/7617+id+/e+Oabb3D77bfjrrvuwrvvvgsgdJ+Mrr+0tBTt27fn1ttsNuTn58flvWgJ9/mhhx7CVVddhb59+8Jut2PIkCG45557MHnyZAB0nxNFKt1XM2Mxg830lgSRBO68805s3rwZy5YtS/ZQWhz79+/H3XffjQULFiAtLS3Zw2mx+P1+DB8+HE8//TQAYMiQIdi8eTNee+01XH/99UkeXcvh448/xty5c/HBBx/glFNOwfr163HPPfegU6dOdJ+JqCCLURPTrl07WK3WsMyeI0eOoLCwMEmjSk2mTp2Kr776CosXL0aXLl3U5YWFhXC73aioqOC2Z+9hYWGh8B4r64y2ycnJQXp6eot/r9asWYOysjIMHToUNpsNNpsN3333HV5++WXYbDZ06NCB7nMc6NixI/r3788t69evH0pKSgCE7pPR9RcWFqKsrIxb7/V6UV5eHpf3oiXc5wceeEC1Gg0cOBDXXnst7r33XtUaSvc5MaTSfTUzFjOQMGpiHA4Hhg0bhoULF6rL/H4/Fi5ciFGjRiVxZKmDLMuYOnUqPv/8cyxatAjdu3fn1g8bNgx2u527hzt27EBJSYl6D0eNGoVNmzZxX8YFCxYgJydHnaRGjRrFHUPZRjlGS3+vxo0bh02bNmH9+vXq3/DhwzF58mT1/3SfG8+YMWPCyk3s3LkTJ510EgCge/fuKCws5K6/qqoKK1as4O5zRUUF1qxZo26zaNEi+P1+jBw5Ut1m6dKl8Hg86jYLFixAnz590KZNG3Ubo/eiOVNXVwdJ4qc0q9UKv98PgO5zokil+2pmLKYwHaZNxI158+bJTqdTfuedd+StW7fKt9xyi5yXl8dl9rRmbr/9djk3N1desmSJfPjwYfWvrq5O3ea2226Tu3btKi9atEhevXq1PGrUKHnUqFHqeiWNfPz48fL69evl+fPnywUFBcI08gceeEDetm2b/OqrrwrTyFvTe8Vmpcky3ed4sHLlStlms8lPPfWUvGvXLnnu3LlyRkaG/I9//EPdZtasWXJeXp78r3/9S964caN88cUXC9OdhwwZIq9YsUJetmyZ3Lt3by7duaKiQu7QoYN87bXXyps3b5bnzZsnZ2RkhKU722w2+bnnnpO3bdsmP/roo806jZzl+uuvlzt37qym63/22Wdyu3bt5AcffFDdhu5zbFRXV8vr1q2T161bJwOQX3jhBXndunXyL7/8Istyat1XM2OJBAmjJDFnzhy5a9eussPhkEeMGCH/9NNPyR5SygBA+Pf3v/9d3aa+vl6+44475DZt2sgZGRnyr3/9a/nw4cPccfbt2yeff/75cnp6utyuXTv5vvvukz0eD7fN4sWL5cGDB8sOh0Pu0aMHdw6F1vReaYUR3ef48O9//1seMGCA7HQ65b59+8qvv/46t97v98uPPPKI3KFDB9npdMrjxo2Td+zYwW1z/Phx+eqrr5azsrLknJwc+cYbb5Srq6u5bTZs2CCfccYZstPplDt37izPmjUrbCwff/yxfPLJJ8sOh0M+5ZRT5P/85z/xv+AkUFVVJd99991y165d5bS0NLlHjx7yH//4Ry79m+5zbCxevFj4TL7++utlWU6t+2pmLJGwyDJTFpQgCIIgCKIVQzFGBEEQBEEQQUgYEQRBEARBBCFhRBAEQRAEEYSEEUEQBEEQRBASRgRBEARBEEFIGBEEQRAEQQQhYUQQBEEQBBGEhBFBEARBEEQQEkYEQRAaunXrhtmzZyd7GARBJAESRgRBJJUbbrgBl1xyCQDg7LPPxj333NNk537nnXeQl5cXtnzVqlW45ZZbmmwcBEGkDrZkD4AgCCLeuN1uOByOmPcvKCiI42gIgmhOkMWIIIiU4IYbbsB3332Hl156CRaLBRaLBfv27QMAbN68Geeffz6ysrLQoUMHXHvttTh27Ji679lnn42pU6finnvuQbt27TBhwgQAwAsvvICBAwciMzMTRUVFuOOOO1BTUwMAWLJkCW688UZUVlaq53vssccAhLvSSkpKcPHFFyMrKws5OTm44oorcOTIEXX9Y489hsGDB+P9999Ht27dkJubi6uuugrV1dXqNv/85z8xcOBApKeno23btiguLkZtbW2C7iZBELFCwoggiJTgpZdewqhRozBlyhQcPnwYhw8fRlFRESoqKnDuuediyJAhWL16NebPn48jR47giiuu4PZ/99134XA48MMPP+C1114DAEiShJdffhlbtmzBu+++i0WLFuHBBx8EAIwePRqzZ89GTk6Oer77778/bFx+vx8XX3wxysvL8d1332HBggXYs2cPrrzySm673bt344svvsBXX32Fr776Ct999x1mzZoFADh8+DCuvvpq3HTTTdi2bRuWLFmCSy+9FNTDmyBSD3KlEQSREuTm5sLhcCAjIwOFhYXq8ldeeQVDhgzB008/rS57++23UVRUhJ07d+Lkk08GAPTu3RvPPvssd0w2Xqlbt2548skncdttt+Evf/kLHA4HcnNzYbFYuPNpWbhwITZt2oS9e/eiqKgIAPDee+/hlFNOwapVq3DaaacBCAiod955B9nZ2QCAa6+9FgsXLsRTTz2Fw4cPw+v14tJLL8VJJ50EABg4cGAj7hZBEImCLEYEQaQ0GzZswOLFi5GVlaX+9e3bF0DASqMwbNiwsH2//fZbjBs3Dp07d0Z2djauvfZaHD9+HHV1dabPv23bNhQVFamiCAD69++PvLw8bNu2TV3WrVs3VRQBQMeOHVFWVgYAGDRoEMaNG4eBAwfi8ssvxxtvvIETJ06YvwkEQTQZJIwIgkhpampqcOGFF2L9+vXc365du3DWWWep22VmZnL77du3D7/61a9w6qmn4tNPP8WaNWvw6quvAggEZ8cbu93OvbZYLPD7/QAAq9WKBQsW4L///S/69++POXPmoE+fPti7d2/cx0EQROMgYUQQRMrgcDjg8/m4ZUOHDsWWLVvQrVs39OrVi/vTiiGWNWvWwO/34/nnn8fpp5+Ok08+GYcOHYp4Pi39+vXD/v37sX//fnXZ1q1bUVFRgf79+5u+NovFgjFjxuDxxx/HunXr4HA48Pnnn5venyCIpoGEEUEQKUO3bt2wYsUK7Nu3D8eOHYPf78edd96J8vJyXH311Vi1ahV2796Nb775BjfeeKOhqOnVqxc8Hg/mzJmDPXv24P3331eDstnz1dTUYOHChTh27JjQxVZcXIyBAwdi8uTJWLt2LVauXInrrrsOY8eOxfDhw01d14oVK/D0009j9erVKCkpwWeffYajR4+iX79+0d0ggiASDgkjgiBShvvvvx9WqxX9+/dHQUEBSkpK0KlTJ/zwww/w+XwYP348Bg4ciHvuuQd5eXmQJP1H2KBBg/DCCy/gmWeewYABAzB37lzMnDmT22b06NG47bbbcOWVV6KgoCAseBsIWHr+9a9/oU2bNjjrrLNQXFyMHj164KOPPjJ9XTk5OVi6dCkmTZqEk08+GQ8//DCef/55nH/++eZvDkEQTYJFpnxRgiAIgiAIAGQxIgiCIAiCUCFhRBAEQRAEEYSEEUEQBEEQRBASRgRBEARBEEFIGBEEQRAEQQQhYUQQBEEQBBGEhBFBEARBEEQQEkYEQRAEQRBBSBgRBEEQBEEEIWFEEARBEAQRhIQRQRAEQRBEkP8Hr7aB6FU2tNwAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import csv\n",
        "\n",
        "with open('7b_returns.csv', 'w', newline='') as file:\n",
        "  writer = csv.writer(file)\n",
        "  writer.writerow(list(iterations))\n",
        "  writer.writerow(returns)\n",
        "\n",
        "files.download('7b_returns.csv')"
      ],
      "metadata": {
        "id": "KNIuFszNF-PS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "494351bd-db5d-48bb-a595-3ae26cb7657d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5fe170b9-9f97-49a3-83ae-c0e2de9c828c\", \"7b_returns.csv\", 5571)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}